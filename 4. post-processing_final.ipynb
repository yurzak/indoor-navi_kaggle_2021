{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d403211",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from post_multi import motion_multi, fs_multi\n",
    "\n",
    "import matplotlib.path as mpltPath\n",
    "from scipy.spatial import distance\n",
    "from scipy.optimize import minimize\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from PIL import Image\n",
    "#%matplotlib inline\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from xyz10.io_f_mod import read_data_file\n",
    "from xyz10.visualize_f_mod import visualize_trajectory, save_figure_to_image\n",
    "from xyz10.compute_f_mod import compute_step_positions, split_ts_seq, correct_positions\n",
    "from xyz10.compute_f_mod import compute_steps, compute_headings, compute_stride_length, compute_step_heading, compute_rel_positions\n",
    "from xyz10.compute_f_mod import correct_positions_mod, correct_positions_mod2, compute_step_positions_mod, compute_step_positions_mod2, split_ts_seq_mod\n",
    "\n",
    "from xyz10.io_f_mod import read_data_file\n",
    "from xyz10.visualize_f_mod import visualize_trajectory, save_figure_to_image\n",
    "\n",
    "FLOOR_NUM_to_ID = {'5a0546857ecc773753327266': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5c3c44b80379370013e0fd2b': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5d27075f03f801723c2e360f': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'},\n",
    "                 '5d27096c03f801723c31e5e0': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6'},\n",
    "                 '5d27097f03f801723c320d97': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5d27099f03f801723c32511d': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5d2709a003f801723c3251bf': {0: '1F', 1: '2F', 2: '3F', 3: '4F'},\n",
    "                 '5d2709b303f801723c327472': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F', 3: '4F'},\n",
    "                 '5d2709bb03f801723c32852c': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5d2709c303f801723c3299ee': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F', 3: '4F', 4: '5F', 5: '6F', 6: '7F', 7: '8F', 8: '9F'},\n",
    "                 '5d2709d403f801723c32bd39': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F'},\n",
    "                 '5d2709e003f801723c32d896': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5da138274db8ce0c98bbd3d2': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                 '5da1382d4db8ce0c98bbe92e': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5da138314db8ce0c98bbf3a0': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                 '5da138364db8ce0c98bc00f1': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                 '5da1383b4db8ce0c98bc11ab': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                 '5da138754db8ce0c98bca82f': {0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5da138764db8ce0c98bcaa46': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5da1389e4db8ce0c98bd0547': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5da138b74db8ce0c98bd4774': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5da958dd46f8266d0737457b': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'},\n",
    "                 '5dbc1d84c1eb61796cf7c010': {-1: 'B1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7', 7: 'F8'},\n",
    "                 '5dc8cea7659e181adb076a3f': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f6bcc",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "036ffc92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "paths = glob.glob('./img_out/predictions/blend_post_x12_1705/*/*median.png')\n",
    "_ = [os.remove(path) for path in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae795c",
   "metadata": {},
   "source": [
    "Function: Blend, Plot_Traces, Snap2Grid, Make_Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b849e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_predictions(blend_folder, reference_file, is_mean=False):\n",
    "\n",
    "    blend_paths = glob.glob(blend_folder + \"*\")\n",
    "    blend_data = []\n",
    "    for b_path in blend_paths:\n",
    "        blend_data.append(pickle.load(open(b_path, \"rb\")))\n",
    "        #break\n",
    "    reference_data = pickle.load(open(blend_folder+reference_file, \"rb\"))\n",
    "\n",
    "    compound= {}\n",
    "    for site_id in reference_data.keys():\n",
    "\n",
    "        compound[site_id] = {}\n",
    "        for trace_id in reference_data[site_id].keys():\n",
    "\n",
    "            _timestamps = reference_data[site_id][trace_id].to_numpy()[:, 3]\n",
    "            _floor = int(np.median(reference_data[site_id][trace_id].to_numpy()[:, 2]))\n",
    "            _x = []\n",
    "            _y = []\n",
    "            for data in blend_data:\n",
    "\n",
    "                predicted_record = data[site_id][trace_id].to_numpy()\n",
    "                _x.append(predicted_record[:, 0])\n",
    "                _y.append(predicted_record[:, 1])\n",
    "            \n",
    "            if is_mean:\n",
    "                compound[site_id][trace_id] = pd.DataFrame({\"x\": np.mean(_x, axis=0), \"y\": np.mean(_y, axis=0), \"floor\": _floor, \"time\": _timestamps})\n",
    "            else:\n",
    "                compound[site_id][trace_id] = pd.DataFrame({\"x\": np.median(_x, axis=0), \"y\": np.median(_y, axis=0), \"floor\": _floor, \"time\": _timestamps})\n",
    "\n",
    "            #break \n",
    "    return compound\n",
    "\n",
    "def blend_predictions_x(blend_folder, reference_file, grid_data, is_mean=False):\n",
    "    snap_range = 0.2\n",
    "\n",
    "    blend_paths = glob.glob(blend_folder + \"*\")\n",
    "    blend_data = []\n",
    "    for b_path in blend_paths:\n",
    "        blend_data.append(pickle.load(open(b_path, \"rb\")))\n",
    "        #break\n",
    "    reference_data = pickle.load(open(blend_folder+reference_file, \"rb\"))\n",
    "\n",
    "    compound= {}\n",
    "    for site_id in tqdm(reference_data.keys()):\n",
    "\n",
    "        compound[site_id] = {}\n",
    "        for trace_id in reference_data[site_id].keys():\n",
    "\n",
    "            _timestamps = reference_data[site_id][trace_id].to_numpy()[:, 3]\n",
    "            _floor = int(np.median(reference_data[site_id][trace_id].to_numpy()[:, 2]))\n",
    "            _x = []\n",
    "            _y = []\n",
    "            _loss = []\n",
    "            for data in blend_data:\n",
    "                predicted_record = data[site_id][trace_id].to_numpy()\n",
    "                grid_record = grid_data[site_id][_floor]\n",
    "                _xs = predicted_record[:, 0]\n",
    "                _ys = predicted_record[:, 1]\n",
    "                \n",
    "                _x.append(_xs)\n",
    "                _y.append(_ys)\n",
    "                \n",
    "                #########################  SNAP POINTS + LOSS   ##################################\n",
    "                trajectory_extent = ((_xs.max() - _xs.min()) ** 2 + (_ys.max() - _ys.min()) ** 2) ** 0.5\n",
    "                snap_distance = snap_range * trajectory_extent\n",
    "                \n",
    "                try:\n",
    "                    snap_points = []\n",
    "                    for i_step, _ in enumerate(_xs):\n",
    "                        _distances = ((grid_record[:, 0] - _xs[i_step]) ** 2 + (grid_record[:, 1] - _ys[i_step]) ** 2) ** 0.5\n",
    "                        _snap_points = grid_record[_distances <= snap_distance]\n",
    "\n",
    "                        if len(_snap_points) > 0:\n",
    "                            snap_points.append(_snap_points)\n",
    "                except:\n",
    "                    print(_ys)\n",
    "\n",
    "                snap_points = np.concatenate(snap_points)\n",
    "                snap_points = np.array(list(set(map(tuple, snap_points))))\n",
    "\n",
    "                distances2 = (snap_points[:, 0].reshape(-1, 1) - _xs.reshape(1, -1)) ** 2 + (snap_points[:, 1].reshape(-1, 1) - _ys.reshape(1, -1)) ** 2\n",
    "                \n",
    "                _loss.append(np.sum(np.amin(distances2, axis=0)))\n",
    "                \n",
    "            ###########################################################\n",
    "            loss_arr = np.array(_loss)\n",
    "            best_inds = list(np.where(loss_arr <= np.median(loss_arr))[0])\n",
    "            _xb = [_x[el] for el in best_inds]\n",
    "            _yb = [_y[el] for el in best_inds]\n",
    "            ############################################################\n",
    "            \n",
    "            if len(best_inds) > 1:\n",
    "                if is_mean:\n",
    "                    compound[site_id][trace_id] = pd.DataFrame({\"x\": np.mean(_xb, axis=0), \"y\": np.mean(_yb, axis=0), \"floor\": _floor, \"time\": _timestamps})\n",
    "                else:\n",
    "                    compound[site_id][trace_id] = pd.DataFrame({\"x\": np.median(_xb, axis=0), \"y\": np.median(_yb, axis=0), \"floor\": _floor, \"time\": _timestamps})\n",
    "            else:\n",
    "                compound[site_id][trace_id] = pd.DataFrame({\"x\": np.array(_xb), \"y\": np.array(_yb), \"floor\": _floor, \"time\": _timestamps})\n",
    "\n",
    "            #break \n",
    "    return compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e7805a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_submission(model_name, data, sufix=\"coarse\"):\n",
    "    \n",
    "    sample_submit = pd.read_csv(\"./submit/sample_submission.csv\")\n",
    "    splits = sample_submit.site_path_timestamp.str.split(pat=\"_\", expand=True)\n",
    "    sub_data = sample_submit.copy(deep=True).join(splits)\n",
    "    sub_data.rename(columns={0:\"site\", 1:\"path\", 2:\"timestamp\"}, inplace=True)\n",
    "\n",
    "    gr = sub_data.groupby(\"path\")\n",
    "    for trace_id in gr.groups:\n",
    "        timestamps = sub_data.loc[gr.groups[trace_id]].timestamp.to_list()\n",
    "\n",
    "        site_id = sub_data.loc[gr.groups[trace_id]].site.to_list()[0]\n",
    "        predicted_record = data[site_id][trace_id].to_numpy()\n",
    "\n",
    "        func_x = interp1d(predicted_record[:, 3], predicted_record[:, 0], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "        func_y = interp1d(predicted_record[:, 3], predicted_record[:, 1], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "\n",
    "        sub_data.loc[gr.groups[trace_id], \"x\"] = func_x(timestamps)\n",
    "        sub_data.loc[gr.groups[trace_id], \"y\"] = func_y(timestamps)\n",
    "        sub_data.loc[gr.groups[trace_id], \"floor\"] = predicted_record[0, 2]\n",
    "        #break\n",
    "\n",
    "    _ = [sub_data.pop(col) for col in [\"site\", \"path\", \"timestamp\"]]\n",
    "\n",
    "    sub_data.to_csv(f\"./submit/{model_name}_{sufix}.csv\", index=False)\n",
    "    \n",
    "def plot_predictions_multi(model_name, data, sufix=\"coarse\", delay_suffix=False):\n",
    "    \n",
    "    def swap_trace_floor(predicted_data):\n",
    "        swap = {}\n",
    "\n",
    "        for site_id in predicted_data.keys():\n",
    "\n",
    "            swap[site_id] = {}\n",
    "            for trace_id in predicted_data[site_id].keys():\n",
    "\n",
    "                floor_id = predicted_data[site_id][trace_id].floor[0]\n",
    "                if floor_id not in swap[site_id].keys():\n",
    "                    swap[site_id][floor_id] = {}\n",
    "                swap[site_id][floor_id][trace_id] = predicted_data[site_id][trace_id]\n",
    "\n",
    "        return swap\n",
    "\n",
    "    data = swap_trace_floor(data)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f\"./img_out/predictions/{model_name}/\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    n_s = 0\n",
    "    for site_id in tqdm(data.keys()):  # over sites \n",
    "        n_s += 1\n",
    "        #print(f\"Processing Trajectories #{n_s}: Site-{site_id} with {len(data[site_id])} traces\")\n",
    "\n",
    "        try:\n",
    "            os.makedirs(f\"./img_out/predictions/{model_name}/{site_id}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for floor_id in data[site_id]:  # over traces\n",
    "            site_path = \"./data_in/metadata/\" + site_id + \"/\"\n",
    "            \n",
    "            positions = []\n",
    "            legends = []\n",
    "            for trace_id in data[site_id][floor_id].keys():\n",
    "                positions.append(data[site_id][floor_id][trace_id].to_numpy()[:, :2])\n",
    "                \n",
    "                if delay_suffix:\n",
    "                    legends.append(f\"{trace_id}_s{int(0.001*data[site_id][floor_id][trace_id].to_numpy()[0, 4])}_e{int(0.001*data[site_id][floor_id][trace_id].to_numpy()[1, 4])}\")\n",
    "                else:\n",
    "                    legends.append(trace_id)\n",
    "\n",
    "            try:\n",
    "                floor = FLOOR_NUM_to_ID[site_id][floor_id]\n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                with open(info_path) as info_file:\n",
    "                    info_data = json.load(info_file)             \n",
    "\n",
    "                map_width = info_data[\"map_info\"][\"width\"]\n",
    "                map_height = info_data[\"map_info\"][\"height\"]\n",
    "\n",
    "                fig_steps = visualize_trajectory(trajectory=positions, is_multi = True,\n",
    "                                                 floor_plan_filename=map_path, mode=\"lines + markers\", title=f\"{site_id}_{floor}_{sufix}\", legends=legends, \n",
    "                                                 width_meter=map_width,  height_meter=map_height)\n",
    "                save_figure_to_image(fig_steps, f\"./img_out/predictions/{model_name}/{site_id}/{floor}_{sufix}.png\")\n",
    "            except:\n",
    "                print(f\"Exception: wrong floor-{floor} site-{site_id}\")\n",
    "\n",
    "        #break  # only first site_id\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44ca815b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "       \n",
    "def snap2grid(predicted_data, grid_siteid_floorid, aux_grid_siteid_floorid, timestamps_traceid, snap_range=5, aux_snap_range=30):\n",
    "\n",
    "    def closest_point(path, point, snap_range):\n",
    "        #print(path)\n",
    "        distance = (path[:, 0] - point[0])**2 + (path[:, 1] - point[1])**2\n",
    "        \n",
    "        if distance.min() < snap_range**2:\n",
    "            idx = distance.argmin()\n",
    "            return [path[idx, 0], path[idx, 1]], True\n",
    "        else:\n",
    "            return [point[0], point[1]], False\n",
    "\n",
    "    snap2grid_data = {}\n",
    "\n",
    "    n_s= 0\n",
    "    for site_id in tqdm(predicted_data.keys()):  # over sites\n",
    "        n_s += 1\n",
    "\n",
    "        snap2grid_data[site_id] = {}\n",
    "        for trace_id in predicted_data[site_id].keys():  # over traces\n",
    "\n",
    "            trace = []  # list of points [x,y]\n",
    "            predicted_record = predicted_data[site_id][trace_id].to_numpy()\n",
    "            \n",
    "            floor_id = int(np.median(predicted_record[:, 2]))\n",
    "            grid = grid_siteid_floorid[site_id][floor_id]\n",
    "            aux_grid = aux_grid_siteid_floorid[site_id][floor_id]\n",
    "            \n",
    "            func_x = interp1d(predicted_record[:, 3], predicted_record[:, 0], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "            func_y = interp1d(predicted_record[:, 3], predicted_record[:, 1], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "\n",
    "            _x = func_x(timestamps_traceid[trace_id])\n",
    "            _y = func_y(timestamps_traceid[trace_id])\n",
    "\n",
    "            for i, _ in enumerate(_x):  # over points \n",
    "                point = [_x[i], _y[i]]\n",
    "                _closest_point, _isOk = closest_point(grid, point, snap_range)\n",
    "                if _isOk:\n",
    "                    trace.append(_closest_point)  # grid = closest step/waypoint point(slow) vs path_a0= closest contour point (fast)\n",
    "                else:\n",
    "                    _aus_close_point, _ = closest_point(aux_grid, point, aux_snap_range)\n",
    "                    trace.append(_aus_close_point)\n",
    "                \n",
    "\n",
    "            snap2grid_data[site_id][trace_id] = pd.DataFrame(trace, columns=[\"x\", \"y\"])\n",
    "            snap2grid_data[site_id][trace_id][\"floor\"] = floor_id\n",
    "            snap2grid_data[site_id][trace_id][\"time\"] = timestamps_traceid[trace_id]\n",
    "            \n",
    "    return snap2grid_data\n",
    "\n",
    "def snap2grid_full(predicted_data, grid_siteid_floorid, snap_range=5):\n",
    "    \n",
    "    \n",
    "    def sma(ar, length=1):\n",
    "\n",
    "        if not isinstance(ar, np.ndarray): ar = ar.to_numpy()\n",
    "        _length = min(length if length > 0 else 1, ar.size)  # length check\n",
    "        if _length == 1:\n",
    "            return ar\n",
    "        else:\n",
    "            _sum = np.cumsum(ar)\n",
    "            _sum[_length:] = _sum[_length:] - _sum[:-_length]\n",
    "            _sma = sma(ar[:_length - 1], _length-1)\n",
    "            return np.concatenate((_sma, _sum[_length - 1:] / _length))\n",
    "\n",
    "    def closest_point(path, point, snap_range):\n",
    "        #print(path)\n",
    "        distance = (path[:, 0] - point[0])**2 + (path[:, 1] - point[1])**2\n",
    "        \n",
    "        if distance.min() < snap_range**2:\n",
    "            idx = distance.argmin()\n",
    "            return [path[idx, 0], path[idx, 1]], True\n",
    "        else:\n",
    "            return [point[0], point[1]], False\n",
    "\n",
    "    snap2grid_data = {}\n",
    "\n",
    "    n_s= 0\n",
    "    for site_id in tqdm(predicted_data.keys()):  # over sites\n",
    "        n_s += 1\n",
    "\n",
    "        snap2grid_data[site_id] = {}\n",
    "        for trace_id in predicted_data[site_id].keys():  # over traces\n",
    "\n",
    "            trace = []  # list of points [x,y]\n",
    "            predicted_record = predicted_data[site_id][trace_id].to_numpy()\n",
    "            \n",
    "            floor_id = int(np.median(predicted_record[:, 2]))\n",
    "            grid = grid_siteid_floorid[site_id][floor_id]\n",
    "\n",
    "            for i, _ in enumerate(predicted_record):  # over points \n",
    "                point = [predicted_record[i, 0], predicted_record[i, 1]]\n",
    "                _closest_point, _isOk = closest_point(grid, point, snap_range)\n",
    "                if _isOk:\n",
    "                    trace.append(_closest_point)  # grid = closest step/waypoint point(slow) vs path_a0= closest contour point (fast)\n",
    "                else:\n",
    "                    trace.append(point)\n",
    "            trace = np.array(trace)\n",
    "            trace[:, 0] = sma(trace[:, 0], 4)\n",
    "            trace[:, 1] = sma(trace[:, 1], 4)\n",
    "\n",
    "            snap2grid_data[site_id][trace_id] = pd.DataFrame(trace, columns=[\"x\", \"y\"])\n",
    "            snap2grid_data[site_id][trace_id][\"floor\"] = floor_id\n",
    "            snap2grid_data[site_id][trace_id][\"time\"] = predicted_record[:, 3]\n",
    "            \n",
    "    return snap2grid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bbb101",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005d1863",
   "metadata": {},
   "source": [
    "## Combine/Plot/Submit blended models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd041cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb849ce1784544e5ba613c2cb975151a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e75b47fb124011951b88021c0343b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_blend = 12\n",
    "\n",
    "blend_suffix = f\"_post_x{num_blend}_1705\"\n",
    "model_name = f\"blend{blend_suffix}\"\n",
    "grid_data = pickle.load(open(\"./data_out/freespace_05m_siteid_floorid.pkl\", \"rb\"))\n",
    "\n",
    "blend_data_median = blend_predictions_x(f\"./submit/fit_data/post/blend_x{num_blend}_1705/\", \"blend_x7_0305_snapped2motion4s-fsV2nlB-02-04-02-02.pkl\", grid_data, is_mean=False)\n",
    "blend_data_mean = blend_predictions_x(f\"./submit/fit_data/post/blend_x{num_blend}_1705/\", \"blend_x7_0305_snapped2motion4s-fsV2nlB-02-04-02-02.pkl\", grid_data, is_mean=True)\n",
    "\n",
    "with open(f\"./submit/fit_data/post/{model_name}_median.pkl\", \"wb\") as f:\n",
    "    pickle.dump(blend_data_median, f)\n",
    "    \n",
    "with open(f\"./submit/fit_data/post/{model_name}_mean.pkl\", \"wb\") as f:\n",
    "    pickle.dump(blend_data_mean, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08ffb0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0268fdd07f03488b808a100d0c97537b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7301225565483690fdb734b4ab3de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions_multi(f\"{model_name}\", blend_data_median, sufix=f\"median\")\n",
    "plot_predictions_multi(f\"{model_name}\", blend_data_mean, sufix=f\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dd11e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bafe83676743b09fa64f06ff0dfc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277096b4e78f4d4f990308b2a9ac9577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a10c47e6af2465da6ff67803c528bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5e21f2698e4faba552cd3c3f57611d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_data_post_median = pickle.load(open(f\"./submit/fit_data/post/{model_name}_median.pkl\", \"rb\")) #\n",
    "predicted_data_post_mean = pickle.load(open(f\"./submit/fit_data/post/{model_name}_mean.pkl\", \"rb\")) #\n",
    "\n",
    "grid_data = pickle.load(open(\"./data_out/waypoints_siteid_floorid.pkl\", \"rb\"))\n",
    "timestamps = pickle.load(open(f\"./data_out/submission_timestamps_traceid.pkl\", \"rb\"))\n",
    "\n",
    "aux_grid_data = pickle.load(open(\"./data_out/freespace_1m_siteid_floorid.pkl\", \"rb\"))\n",
    "\n",
    "snapped2grid_median = snap2grid(predicted_data_post_median, grid_data, aux_grid_data, timestamps, snap_range=3, aux_snap_range=50)\n",
    "snapped2grid_mean = snap2grid(predicted_data_post_mean, grid_data, aux_grid_data, timestamps, snap_range=3, aux_snap_range=50)\n",
    "make_submission(f\"{model_name}_median\", snapped2grid_median, sufix=f\"gridWPs3-gridFS1m50\")\n",
    "make_submission(f\"{model_name}_mean\", snapped2grid_mean, sufix=f\"gridWPs3-gridFS1m50\")\n",
    "plot_predictions_multi(f\"{model_name}\", snapped2grid_median, sufix=f\"median_gridWPs3-gridFS1m50\")\n",
    "plot_predictions_multi(f\"{model_name}\", snapped2grid_mean, sufix=f\"mean_gridWPs3-gridFS1m50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36647021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d3ce8add614879946f94c9813cc824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd58bbe9856432ab80a2af40266ef59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d430551da3854e8c9e1c9bdc18831127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5270bbe78f94821920c293f68111a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aux_grid_data = pickle.load(open(\"./data_out/freespace_2m_siteid_floorid.pkl\", \"rb\"))\n",
    "\n",
    "snapped2grid_median = snap2grid(predicted_data_post_median, grid_data, aux_grid_data, timestamps, snap_range=3, aux_snap_range=50)\n",
    "snapped2grid_mean = snap2grid(predicted_data_post_mean, grid_data, aux_grid_data, timestamps, snap_range=3, aux_snap_range=50)\n",
    "make_submission(f\"{model_name}_median\", snapped2grid_median, sufix=f\"gridWPs3-gridFS2m50\")\n",
    "make_submission(f\"{model_name}_mean\", snapped2grid_mean, sufix=f\"gridWPs3-gridFS2m50\")\n",
    "plot_predictions_multi(f\"{model_name}\", snapped2grid_median, sufix=f\"median_gridWPs3-gridFS2m50\")\n",
    "plot_predictions_multi(f\"{model_name}\", snapped2grid_mean, sufix=f\"mean_gridWPs3-gridFS2m50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0df42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
