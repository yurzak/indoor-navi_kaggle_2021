{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ef65a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from PIL import Image\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "sns.set_style('darkgrid')\n",
    "#%matplotlib inline\n",
    "\n",
    "from xyz10.io_f_mod import read_data_file\n",
    "from xyz10.visualize_f_mod import visualize_trajectory, save_figure_to_image\n",
    "from scipy.ndimage import median_filter\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample, shuffle\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"  # \"0\" = GPU_on, \"-1\" = GPU_off\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "#%load_ext tensorboard  # extension for notebook\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f242166",
   "metadata": {},
   "source": [
    "Supporting Functions (PLOT PREDICTIONS / MAKE SUBMISSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "867e9b5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_seq(data, mode, seq_len):  # returns sequenced data (3d-> sequence*timestep*features)\n",
    "    # data: 2d -> number records*features\n",
    "    # modes: \"moving\"/\"stacked\"/\"full\"\n",
    "    if mode == \"full\":\n",
    "         return data.reshape(1, data.shape[0], data.shape[1])\n",
    "    elif mode == \"stacked\":\n",
    "        seq_num = data.shape[0]//seq_len\n",
    "        _data = data[:seq_num*seq_len]\n",
    "        return _data.reshape(seq_num, seq_len, _data.shape[1])\n",
    "    elif mode == \"moving\":\n",
    "        seq_num = data.shape[0]-seq_len+1\n",
    "        \n",
    "        seq = []\n",
    "        for i_seq in range(seq_num):\n",
    "            seq.append(data[i_seq:i_seq+seq_len])\n",
    "        return np.stack(seq)\n",
    "    \n",
    "def make_seq_inv(data, mode, seq_len):  # returns zero sequence data (2d-> timestep*1(x or y))\n",
    "    # data: 3d -> number of sequences*number records-sequence length*(x or y)\n",
    "    # modes: \"moving\"/\"stacked\"/\"full\"\n",
    "    if mode == \"full\" or mode == \"stacked\":\n",
    "        return data.reshape(data.shape[0]*data.shape[1], 1)\n",
    "    elif mode == \"moving\":  # combine moving sequences via median/mean filter\n",
    "        seq0_len = data.shape[0] + seq_len - 1\n",
    "        _data = np.zeros((seq0_len, 1))\n",
    "        \n",
    "        for i_row in range(seq0_len):\n",
    "            row_el = []\n",
    "            j_start = max(0, i_row-seq_len+1)\n",
    "            j_end = min(i_row, seq0_len-seq_len)\n",
    "            \n",
    "            for i_seq in range(j_start, j_end+1):\n",
    "                row_el.append(data[i_seq, i_row-i_seq])\n",
    "                \n",
    "            _data[i_row] = np.median(row_el)\n",
    "\n",
    "        return _data  \n",
    "\n",
    "def make_submission(model_name, data, sufix=\"coarse\"):\n",
    "\n",
    "    sample_submit = pd.read_csv(\"./submit/sample_submission.csv\")\n",
    "    splits = sample_submit.site_path_timestamp.str.split(pat=\"_\", expand=True)\n",
    "    sub_data = sample_submit.copy(deep=True).join(splits)\n",
    "    sub_data.rename(columns={0:\"site\", 1:\"path\", 2:\"timestamp\"}, inplace=True)\n",
    "\n",
    "    for i in tqdm(list(sub_data.index)):\n",
    "        site_id = sub_data.site[i]\n",
    "        trace_id = sub_data.path[i]\n",
    "        timestamp = sub_data.timestamp[i]\n",
    "\n",
    "        predicted_record = data[site_id][trace_id].to_numpy()\n",
    "\n",
    "        func_x = interp1d(predicted_record[:, 3], predicted_record[:, 0], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "        func_y = interp1d(predicted_record[:, 3], predicted_record[:, 1], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "\n",
    "        sub_data.loc[i, \"x\"] = func_x(timestamp)\n",
    "        sub_data.loc[i, \"y\"] = func_y(timestamp)\n",
    "        sub_data.loc[i, \"floor\"] = int(np.median(predicted_record[:, 2]))\n",
    "        #break\n",
    "\n",
    "    _ = [sub_data.pop(col) for col in [\"site\", \"path\", \"timestamp\"]]\n",
    "\n",
    "    sub_data.to_csv(f\"./submit/{model_name}_{sufix}.csv\", index=False)\n",
    "\n",
    "def plot_predictions_multi(model_name, data, sufix=\"coarse\"):\n",
    "    \n",
    "    def swap_trace_floor(predicted_data):\n",
    "        swap = {}\n",
    "\n",
    "        for site_id in predicted_data.keys():\n",
    "\n",
    "            swap[site_id] = {}\n",
    "            for trace_id in predicted_data[site_id].keys():\n",
    "\n",
    "                floor_id = predicted_data[site_id][trace_id].floor[0]\n",
    "                if floor_id not in swap[site_id].keys():\n",
    "                    swap[site_id][floor_id] = {}\n",
    "                swap[site_id][floor_id][trace_id] = predicted_data[site_id][trace_id]\n",
    "\n",
    "        return swap\n",
    "\n",
    "    data = swap_trace_floor(data)\n",
    "    \n",
    "    floor_convert = {'5a0546857ecc773753327266': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5c3c44b80379370013e0fd2b': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5d27075f03f801723c2e360f': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'},\n",
    "                     '5d27096c03f801723c31e5e0': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6'},\n",
    "                     '5d27097f03f801723c320d97': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5d27099f03f801723c32511d': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5d2709a003f801723c3251bf': {0: '1F', 1: '2F', 2: '3F', 3: '4F'},\n",
    "                     '5d2709b303f801723c327472': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F', 3: '4F'},\n",
    "                     '5d2709bb03f801723c32852c': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5d2709c303f801723c3299ee': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F', 3: '4F', 4: '5F', 5: '6F', 6: '7F', 7: '8F', 8: '9F'},\n",
    "                     '5d2709d403f801723c32bd39': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F'},\n",
    "                     '5d2709e003f801723c32d896': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da138274db8ce0c98bbd3d2': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da1382d4db8ce0c98bbe92e': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da138314db8ce0c98bbf3a0': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da138364db8ce0c98bc00f1': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da1383b4db8ce0c98bc11ab': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da138754db8ce0c98bca82f': {0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5da138764db8ce0c98bcaa46': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da1389e4db8ce0c98bd0547': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5da138b74db8ce0c98bd4774': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da958dd46f8266d0737457b': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'},\n",
    "                     '5dbc1d84c1eb61796cf7c010': {-1: 'B1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7', 7: 'F8'},\n",
    "                     '5dc8cea7659e181adb076a3f': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'}}\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f\"./img_out/predictions/{model_name}/\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    n_s = 0\n",
    "    for site_id in tqdm(data.keys()):  # over sites \n",
    "        n_s += 1\n",
    "        #print(f\"Processing Trajectories #{n_s}: Site-{site_id} with {len(data[site_id])} traces\")\n",
    "\n",
    "        try:\n",
    "            os.makedirs(f\"./img_out/predictions/{model_name}/{site_id}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for floor_id in data[site_id]:  # over traces\n",
    "            site_path = \"./data_in/metadata/\" + site_id + \"/\"\n",
    "            \n",
    "            positions = []\n",
    "            legends = []\n",
    "            for trace_id in data[site_id][floor_id].keys():\n",
    "                positions.append(data[site_id][floor_id][trace_id].to_numpy()[:, :2])\n",
    "                legends.append(trace_id)\n",
    "\n",
    "            try:\n",
    "                floor = floor_convert[site_id][floor_id]\n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                with open(info_path) as info_file:\n",
    "                    info_data = json.load(info_file)             \n",
    "\n",
    "                map_width = info_data[\"map_info\"][\"width\"]\n",
    "                map_height = info_data[\"map_info\"][\"height\"]\n",
    "\n",
    "                fig_steps = visualize_trajectory(trajectory=positions, is_multi = True,\n",
    "                                                 floor_plan_filename=map_path, mode=\"lines + markers\", title=f\"{site_id}_{floor}_{sufix}\", legends=legends, \n",
    "                                                 width_meter=map_width,  height_meter=map_height)\n",
    "                save_figure_to_image(fig_steps, f\"./img_out/predictions/{model_name}/{site_id}/{floor}_{sufix}.png\")\n",
    "            except:\n",
    "                print(f\"Exception: wrong floor-{floor} site-{site_id}\")\n",
    "\n",
    "        #break  # only first site_id\n",
    "\n",
    "        \n",
    "\n",
    "def save_models(models24, models=\"models24_bssid10k_SP_count_mix\"):\n",
    "    print(\"Saving Models...\")\n",
    "\n",
    "    idx = int(time.time())\n",
    "    model_name = f\"{models}_{idx}/\"\n",
    "    model_path = \"./saved_models/\"+ model_name\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    for site_id in tqdm(models24.keys()):\n",
    "\n",
    "        models24[site_id][3].save(model_path + site_id + \"_s1\")\n",
    "        with open(model_path + site_id + f\"_s1/features_list.pkl\", \"wb\") as f:\n",
    "            pickle.dump(models24[site_id][0], f)\n",
    "        with open(model_path + site_id + f\"_s1/scaler.pkl\", \"wb\") as f:\n",
    "            pickle.dump(models24[site_id][1], f)\n",
    "        with open(model_path + site_id + f\"_s1/f_binarizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(models24[site_id][2], f)\n",
    "        \n",
    "        models24[site_id][5].save(model_path + site_id + \"_s2\")\n",
    "        with open(model_path + site_id + f\"_s2/scaler.pkl\", \"wb\") as f:\n",
    "            pickle.dump(models24[site_id][4], f)\n",
    "            \n",
    "    return model_name\n",
    "            \n",
    "def calculate_global_metrics(comparison):\n",
    "\n",
    "    train_mae = 0\n",
    "    val_mae = 0\n",
    "\n",
    "    for comparison_train, comparison_val in comparison:\n",
    "\n",
    "        train_mae += comparison_train.mean_abs_error[comparison_train.index[-1]]\n",
    "        val_mae += comparison_val.mean_abs_error[comparison_val.index[-1]]\n",
    "\n",
    "    comp_length = len(comparison)\n",
    "    \n",
    "    train_mae /= comp_length\n",
    "    val_mae /= comp_length\n",
    "\n",
    "    print(f\"Global Train/Validation MAE: {train_mae}/{val_mae}\")\n",
    "            \n",
    "def xy_loss_metric(y_true, y_pred):\n",
    "    e_xy = tf.sqrt(tf.square(y_true[:, 0] - y_pred[:, 0]) +  tf.square(y_true[:, 1] - y_pred[:, 1])) \n",
    "    return tf.reduce_mean(e_xy, axis=-1)\n",
    "\n",
    "def xy_loss_metric_mse(y_true, y_pred):\n",
    "    e_xy = tf.square(y_true[:, 0] - y_pred[:, 0]) +  tf.square(y_true[:, 1] - y_pred[:, 1]) \n",
    "    return tf.sqrt(tf.reduce_mean(e_xy, axis=-1))\n",
    "\n",
    "def load_data(site_id, fraction_bssid, seq_len=10, data_file=\"10k_mix-counts\"):\n",
    "    print(\"Loading Data...\")\n",
    "    path = f\"./data_out/full24/seq{seq_len}/\"  # full24/\n",
    "    file_name = f\"{site_id}_{data_file}.pkl\"\n",
    "    \n",
    "    f_list = features_list(site_id, fraction_bssid)\n",
    "\n",
    "    return f_list, pickle.load(open(path+file_name, \"rb\"))[f_list]\n",
    "\n",
    "def features_list(site_id, fraction_bssid, bssid_mode=\"count\"):\n",
    "    \n",
    "    train_bssid = pickle.load(open(\"./data_out/train_24IDs_standardF_bssid_ranks.pkl\", \"rb\"))\n",
    "    test_bssid = pickle.load(open(\"./data_out/test_bssid_ranks.pkl\", \"rb\"))\n",
    "\n",
    "    if fraction_bssid <= 1:\n",
    "        _train_bssid = train_bssid[bssid_mode][site_id].bssid.tolist()\n",
    "        _test_bssid = test_bssid[bssid_mode][site_id].bssid.tolist()\n",
    "        _mix = list(set(_train_bssid) & set(_test_bssid))\n",
    "        _list = _mix[:int(fraction_bssid*len(_mix))]\n",
    "    else:\n",
    "        _train_bssid = train_bssid[bssid_mode][site_id].bssid.tolist()\n",
    "        _test_bssid = test_bssid[bssid_mode][site_id].bssid.tolist()\n",
    "        _mix = list(set(_train_bssid) & set(_test_bssid))\n",
    "        _list = _mix[:int(fraction_bssid)]\n",
    "\n",
    "    _list_d = []\n",
    "    for record in _list:\n",
    "        _list_d.append(record)\n",
    "        _list_d.append(record+\"_D\")\n",
    "        \n",
    "    _list_d += [\"x\", \"y\", \"f\", \"m\", \"r\", \"rx\", \"ry\", \"rx_cum\", \"ry_cum\", \"trace\"]  # 2+7 = 9+1 = 10\n",
    "        \n",
    "    return _list_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18cd5a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(featured_data, seq_len=10, train_fraction=0.8, random_state=123):\n",
    "    print(f\"Processing Data of shape {featured_data[1].shape}...\")\n",
    "\n",
    "    # shuffle sequences\n",
    "    data_shape = featured_data[1].shape\n",
    "    columns = featured_data[1].columns.tolist()\n",
    "    #display(featured_data[1].describe())\n",
    "\n",
    "    # define train/test traces\n",
    "    t_list = featured_data[1][\"trace\"].unique()\n",
    "    t_list_s = t_list[shuffle(list(range(len(t_list))))]\n",
    "    train_traces = list(t_list_s[:int((train_fraction)*len(t_list_s))])\n",
    "    test_traces = list(t_list_s[int((train_fraction)*len(t_list_s)):])\n",
    "\n",
    "    # define corresponding train/test indices\n",
    "    gr = featured_data[1].groupby(\"trace\")\n",
    "    for i, trace in enumerate(train_traces):\n",
    "        if i == 0:\n",
    "            inds_train = gr.groups[trace]\n",
    "        else:\n",
    "            inds_train = inds_train.append(gr.groups[trace])\n",
    "\n",
    "    for i, trace in enumerate(test_traces):\n",
    "        if i == 0:\n",
    "            inds_test = gr.groups[trace]\n",
    "        else:\n",
    "            inds_test = inds_test.append(gr.groups[trace])\n",
    "\n",
    "    _ = featured_data[1].pop(\"trace\")\n",
    "    columns.remove(\"trace\")\n",
    "    \n",
    "    train_data = featured_data[1].loc[inds_train, columns].copy(deep=True)\n",
    "    test_data = featured_data[1].loc[inds_test, columns].copy(deep=True)\n",
    "\n",
    "    train_data_shape = train_data.shape\n",
    "    train_data_s = train_data.to_numpy().reshape(len(train_data)//seq_len, seq_len, train_data.shape[1])\n",
    "    np.random.shuffle(train_data_s)\n",
    "    train_data_s = pd.DataFrame(train_data_s.reshape(train_data_shape), columns=columns)\n",
    "    #train_data_s[numeric_cols] = train_data_s[numeric_cols].apply(pd.to_numeric)\n",
    "\n",
    "    test_data_shape = test_data.shape\n",
    "    test_data_s = test_data.to_numpy().reshape(len(test_data)//seq_len, seq_len, test_data.shape[1])\n",
    "    np.random.shuffle(test_data_s)\n",
    "    test_data_s = pd.DataFrame(test_data_s.reshape(test_data_shape), columns=columns)\n",
    "    #test_data_s[numeric_cols] = test_data_s[numeric_cols].apply(pd.to_numeric)\n",
    "    \n",
    "    shuffled_data = pd.concat([train_data_s, test_data_s], axis=0)\n",
    "    \n",
    "    # split-combine features and targets    \n",
    "    y_x = shuffled_data.pop(\"x\")\n",
    "    y_y = shuffled_data.pop(\"y\")\n",
    "    \n",
    "    x_m = shuffled_data.pop(\"m\").to_numpy().reshape(-1,1)\n",
    "    x_r = shuffled_data.pop(\"r\").to_numpy().reshape(-1,1)\n",
    "    x_rx = shuffled_data.pop(\"rx\").to_numpy().reshape(-1,1)\n",
    "    x_ry = shuffled_data.pop(\"ry\").to_numpy().reshape(-1,1)\n",
    "    x_rx_cum = shuffled_data.pop(\"rx_cum\").to_numpy().reshape(-1,1)\n",
    "    x_ry_cum = shuffled_data.pop(\"ry_cum\").to_numpy().reshape(-1,1)\n",
    "    x_f = shuffled_data.pop(\"f\").to_numpy().astype(int)\n",
    "    \n",
    "    x = shuffled_data\n",
    "\n",
    "    encoder = LabelBinarizer()#OneHotEncoder(sparse=False)\n",
    "    x_f = encoder.fit_transform(x_f)\n",
    "    \n",
    "    #x = np.concatenate((x, x_f), axis=1)\n",
    "    x = np.concatenate((x, x_m, x_r, x_rx, x_ry, x_rx_cum, x_ry_cum, x_f), axis=1)\n",
    "    y = pd.concat([y_x, y_y], axis=1).to_numpy()\n",
    "    \n",
    "    # split into train/validation\n",
    "    train_x, val_x = x[:train_data_shape[0]], x[train_data_shape[0]:]\n",
    "    train_y, val_y = y[:train_data_shape[0]], y[train_data_shape[0]:]   \n",
    "            \n",
    "    # scale data\n",
    "    scaler = StandardScaler()  # RobustScaler()  /StandardScaler()/ MinMaxScaler\n",
    "    train_x = scaler.fit_transform(train_x)\n",
    "    val_x = scaler.transform(val_x)\n",
    "        \n",
    "    # final shaping\n",
    "    train_y = train_y.reshape(train_x.shape[0]//seq_len, seq_len, 2) \n",
    "    val_y = val_y.reshape(val_x.shape[0]//seq_len, seq_len, 2)\n",
    "    train_x = train_x.reshape(train_x.shape[0]//seq_len, seq_len, train_x.shape[1]) \n",
    "    val_x = val_x.reshape(val_x.shape[0]//seq_len, seq_len, val_x.shape[1])\n",
    "            \n",
    "    return featured_data[0][:-10], scaler, encoder, train_x, val_x, train_y, val_y  # features list is only bssid related\n",
    "\n",
    "def LSTM(site_id, seq_len, train_x, val_x, train_y, val_y, w_f = 10, drop=0.1, learning_rate=0.005, epochs=200, batch=6): \n",
    "    print(f\"Fitting Model with {train_x.shape}/{val_x.shape} train/validation shapes => {round(100*train_x.shape[0]/(train_x.shape[0]+val_x.shape[0]), 1)}%...\")\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    epoch_iterations = 40\n",
    "    \n",
    "    record_shape = train_x.shape[1:]\n",
    "    features_count = train_x.shape[2]\n",
    "    targets_count = train_y.shape[2]\n",
    "    records_num = train_x.shape[0]\n",
    "        \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False)  # \"adam\"\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_delta=1e-4)\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True)\n",
    "        \n",
    "    if batch==0:\n",
    "        batch = int(np.log2(records_num/epoch_iterations))\n",
    "        \n",
    "    METRICS = [\n",
    "        #tf.keras.metrics.MeanAbsoluteError(name=\"mae\", dtype=None)\n",
    "        xy_loss_metric,\n",
    "        #tf.keras.metrics.RootMeanSquaredError(name=\"rmse\", dtype=None)\n",
    "    ]       \n",
    "    \n",
    "    MODEL_NAME = f\"Site-{site_id}_stage1_f{features_count}_L1bi+1x{w_f}_LR{learning_rate}_bch{batch}_{int(time.time())}\"\n",
    "    tensorboard = TensorBoard(log_dir=f\"log/Neurals/{MODEL_NAME}\", histogram_freq=1)\n",
    "    \n",
    "    nodes_max = 1000\n",
    "            \n",
    "    model = keras.Sequential([\n",
    "        #layers.BatchNormalization(input_shape=(None, features_count)),\n",
    "        layers.Bidirectional(layers.LSTM(min(int(features_count*w_f if w_f < 1 else w_f), nodes_max), return_sequences=True), input_shape=(None, features_count), merge_mode=\"concat\"),  #merge_mode=\"concat\" \"sum\", \"mul\", \"concat\", \"ave\"\n",
    "        #layers.LSTM(int(features_count*w_f if w_f < 1 else w_f), return_sequences=True, input_shape=(None, features_count)),\n",
    "        layers.Dropout(drop),\n",
    "        #layers.BatchNormalization(),\n",
    "\n",
    "        #layers.TimeDistributed(layers.Dense(10*int((features_count*w_f if w_f <= 1 else w_f)), activation=\"relu\")),\n",
    "        #layers.Dense(10*int((features_count*w_f if w_f <= 1 else w_f)), activation=\"relu\"),\n",
    "        #ayers.Dropout(drop),\n",
    "        #layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Bidirectional(layers.LSTM(3*int(features_count*w_f if w_f < 1 else w_f), return_sequences=True), merge_mode=\"concat\"),\n",
    "        #layers.LSTM(int(features_count*w_f if w_f < 1 else w_f), return_sequences=True),\n",
    "        layers.Dropout(drop),\n",
    "        #layers.BatchNormalization(),\n",
    "        \n",
    "        #layers.TimeDistributed(layers.Dense(10*int((features_count*w_f if w_f <= 1 else w_f)), activation=\"relu\")),\n",
    "        layers.Dense(3*int((features_count*w_f if w_f <= 1 else w_f)), activation=\"relu\"),\n",
    "        layers.Dropout(drop),\n",
    "        #layers.BatchNormalization(),\n",
    "\n",
    "        #layers.TimeDistributed(layers.Dense(10*int((features_count*w_f if w_f <= 1 else w_f)), activation=\"relu\")),\n",
    "        layers.Dense(3*int((features_count*w_f if w_f <= 1 else w_f)), activation=\"relu\"),\n",
    "        #layers.Dropout(drop),\n",
    "        #layers.BatchNormalization(),\n",
    "        \n",
    "        #layers.TimeDistributed(layers.Dense(int(1*(features_count*w_f if w_f <= 1 else w_f)), activation=\"relu\")),\n",
    "           \n",
    "        #layers.LSTM(2, return_sequences=True, activation=\"linear\"),\n",
    "        layers.TimeDistributed(layers.Dense(2)),\n",
    "        #layers.Dense(seq_len*2, activation=\"linear\"),\n",
    "        #layers.Reshape((seq_len, 2))\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  #loss=\"mae\",\n",
    "                  loss=\"mse\",\n",
    "                  #loss=xy_loss_metric_mse,\n",
    "                  metrics=METRICS)\n",
    "    \n",
    "    display(model.summary())  \n",
    "    \n",
    "    fit = model.fit(train_x, train_y,\n",
    "                    validation_data=(val_x,  val_y),\n",
    "                    batch_size=2**batch,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    callbacks=[tensorboard,  reduce_lr, earlystop]\n",
    "                   )\n",
    "        \n",
    "    #fit_progress = pd.DataFrame(fit.history)\n",
    "    #fit_progress.loc[:, [\"accuracy\", \"val_accuracy\"]].plot()\n",
    "    #fit_progress.loc[:, [\"loss\", \"val_loss\"]].plot()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c711a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data_2(num_col_drop,\n",
    "                      train_x, val_x, \n",
    "                      train_predict, val_predict):\n",
    "    print(f\"Processing Data Stage #2 of shape {train_x.shape}/{val_x.shape} <-> {train_predict.shape}/{val_predict.shape} + droping {num_col_drop}\")\n",
    "\n",
    "    train_shape = train_x.shape\n",
    "    val_shape = val_x.shape\n",
    "    \n",
    "    # unfold sequences\n",
    "    train_x = train_x.reshape(train_shape[0]*train_shape[1], train_shape[2])\n",
    "    val_x = val_x.reshape(val_shape[0]*val_shape[1], val_shape[2])\n",
    "    \n",
    "    train_predict = train_predict.reshape(train_shape[0]*train_shape[1], 2)\n",
    "    val_predict = val_predict.reshape(val_shape[0]*val_shape[1], 2)\n",
    "    \n",
    "    # combine together (without bssid features)\n",
    "    train_x_2 = np.concatenate((train_x[:, num_col_drop:], train_predict), axis=1)\n",
    "    val_x_2 = np.concatenate((val_x[:, num_col_drop:], val_predict), axis=1)\n",
    "\n",
    "    # scale data\n",
    "    scaler = StandardScaler()  # RobustScaler()  /StandardScaler()/ MinMaxScaler\n",
    "    train_x_2 = scaler.fit_transform(train_x_2)\n",
    "    val_x_2 = scaler.transform(val_x_2)\n",
    "        \n",
    "    # final shaping => number of sequnces / sequence length / number of features\n",
    "    train_x_2 = train_x_2.reshape(train_shape[0], train_shape[1], train_shape[2]-num_col_drop+2) \n",
    "    val_x_2 = val_x_2.reshape(val_shape[0], val_shape[1], val_shape[2]-num_col_drop+2)\n",
    "            \n",
    "    return scaler, train_x_2, val_x_2\n",
    "\n",
    "\n",
    "def LSTM_2(site_id, seq_len, train_x, val_x, train_y, val_y, w_f = 10, drop=0.1, learning_rate=0.005, epochs=200, batch=6): \n",
    "    print(f\"Fitting Model with {train_x.shape}/{val_x.shape} train/validation shapes => {round(100*train_x.shape[0]/(train_x.shape[0]+val_x.shape[0]), 1)}%...\")\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    epoch_iterations = 40\n",
    "    \n",
    "    record_shape = train_x.shape[1:]\n",
    "    features_count = train_x.shape[2]\n",
    "    targets_count = train_y.shape[2]\n",
    "    records_num = train_x.shape[0]\n",
    "        \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False)  # \"adam\"\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_delta=1e-4)\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True)\n",
    "        \n",
    "    if batch==0:\n",
    "        batch = int(np.log2(records_num/epoch_iterations))\n",
    "        \n",
    "    METRICS = [\n",
    "        #tf.keras.metrics.MeanAbsoluteError(name=\"mae\", dtype=None)\n",
    "        xy_loss_metric,\n",
    "        #tf.keras.metrics.RootMeanSquaredError(name=\"rmse\", dtype=None)\n",
    "    ]       \n",
    "    \n",
    "    MODEL_NAME = f\"Site-{site_id}_stage2_f{features_count}_L1bi+1x{w_f}_LR{learning_rate}_bch{batch}_{int(time.time())}\"\n",
    "    tensorboard = TensorBoard(log_dir=f\"log/Neurals/{MODEL_NAME}\", histogram_freq=1)\n",
    "    \n",
    "    nodes_max = 1000\n",
    "            \n",
    "    model = keras.Sequential([\n",
    "        #layers.BatchNormalization(input_shape=(None, features_count)),\n",
    "        layers.Bidirectional(layers.LSTM(min(int(features_count*w_f if w_f < 1 else w_f), nodes_max), return_sequences=True), input_shape=(None, features_count), merge_mode=\"concat\"),  #merge_mode=\"concat\" \"sum\", \"mul\", \"concat\", \"ave\"\n",
    "        #layers.LSTM(int(features_count*w_f if w_f < 1 else w_f), return_sequences=True, input_shape=(None, features_count)),\n",
    "        layers.Dropout(drop),\n",
    "        #layers.BatchNormalization(),\n",
    "\n",
    "        #layers.TimeDistributed(layers.Dense(10*int((features_count*w_f if w_f <= 1 else w_f)), activation=\"relu\")),\n",
    "        #layers.Dense(10*int((features_count*w_f if w_f <= 1 else w_f)), activation=\"relu\"),\n",
    "        #ayers.Dropout(drop),\n",
    "        #layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Bidirectional(layers.LSTM(3*int(features_count*w_f if w_f < 1 else w_f), return_sequences=True), merge_mode=\"concat\"),\n",
    "        #layers.LSTM(int(features_count*w_f if w_f < 1 else w_f), return_sequences=True),\n",
    "        layers.Dropout(drop),\n",
    "        #layers.BatchNormalization(),\n",
    "        \n",
    "        #layers.TimeDistributed(layers.Dense(10*int((features_count*w_f if w_f <= 1 else w_f)), activation=\"relu\")),\n",
    "        layers.Dense(3*int((features_count*w_f if w_f <= 1 else w_f)), activation=\"relu\"),\n",
    "        layers.Dropout(drop),\n",
    "        #layers.BatchNormalization(),\n",
    "\n",
    "        #layers.TimeDistributed(layers.Dense(10*int((features_count*w_f if w_f <= 1 else w_f)), activation=\"relu\")),\n",
    "        layers.Dense(3*int((features_count*w_f if w_f <= 1 else w_f)), activation=\"relu\"),\n",
    "        #layers.Dropout(drop),\n",
    "        #layers.BatchNormalization(),\n",
    "        \n",
    "        #layers.TimeDistributed(layers.Dense(int(1*(features_count*w_f if w_f <= 1 else w_f)), activation=\"relu\")),\n",
    "           \n",
    "        #layers.LSTM(2, return_sequences=True, activation=\"linear\"),\n",
    "        layers.TimeDistributed(layers.Dense(2)),\n",
    "        #layers.Dense(seq_len*2, activation=\"linear\"),\n",
    "        #layers.Reshape((seq_len, 2))\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  #loss=\"mae\",\n",
    "                  loss=\"mse\",\n",
    "                  #loss=xy_loss_metric_mse,\n",
    "                  metrics=METRICS)\n",
    "    \n",
    "    display(model.summary())  \n",
    "    \n",
    "    fit = model.fit(train_x, train_y,\n",
    "                    validation_data=(val_x,  val_y),\n",
    "                    batch_size=2**batch,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    callbacks=[tensorboard,  reduce_lr, earlystop]\n",
    "                   )\n",
    "        \n",
    "    #fit_progress = pd.DataFrame(fit.history)\n",
    "    #fit_progress.loc[:, [\"accuracy\", \"val_accuracy\"]].plot()\n",
    "    #fit_progress.loc[:, [\"loss\", \"val_loss\"]].plot()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb9ff3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on #1: Site-5da1382d4db8ce0c98bbe92e\n",
      "Loading Data...\n",
      "Processing Data of shape (27480, 3572)...\n",
      "Fitting Model with (2688, 10, 3574)/(60, 10, 3574) train/validation shapes => 97.8%...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, None, 714)         11229792  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 714)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 2142)        15302448  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 2142)        0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 1071)        2295153   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 1071)        0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 1071)        1148112   \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 2)           2144      \n",
      "=================================================================\n",
      "Total params: 29,977,649\n",
      "Trainable params: 29,977,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0269s vs `on_train_batch_end` time: 3.0930s). Check your callbacks.\n",
      "Processing Data Stage #2 of shape (2688, 10, 3574)/(60, 10, 3574) <-> (2688, 10, 2)/(60, 10, 2) + droping 3562\n",
      "Fitting Model with (2688, 10, 14)/(60, 10, 14) train/validation shapes => 97.8%...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_2 (Bidirection (None, None, 400)         344000    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, None, 400)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, None, 1200)        4804800   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, None, 1200)        0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, None, 600)         720600    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, None, 600)         0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, None, 600)         360600    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 2)           1202      \n",
      "=================================================================\n",
      "Total params: 6,231,202\n",
      "Trainable params: 6,231,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0124s vs `on_train_batch_end` time: 8.6869s). Check your callbacks.\n",
      "Predictions (train/validation) #1: Site-5da1382d4db8ce0c98bbe92e\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_X</th>\n",
       "      <th>real_Y</th>\n",
       "      <th>predict_X</th>\n",
       "      <th>predict_Y</th>\n",
       "      <th>abs_error_X</th>\n",
       "      <th>abs_error_Y</th>\n",
       "      <th>mean_abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>111.680770</td>\n",
       "      <td>105.675013</td>\n",
       "      <td>111.932991</td>\n",
       "      <td>104.710663</td>\n",
       "      <td>4.493221</td>\n",
       "      <td>5.231294</td>\n",
       "      <td>3.854094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>35.086631</td>\n",
       "      <td>40.743186</td>\n",
       "      <td>35.659561</td>\n",
       "      <td>39.221386</td>\n",
       "      <td>3.493267</td>\n",
       "      <td>3.288079</td>\n",
       "      <td>2.088673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.703297</td>\n",
       "      <td>25.989432</td>\n",
       "      <td>13.895754</td>\n",
       "      <td>25.882315</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.006811</td>\n",
       "      <td>0.021989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>88.703687</td>\n",
       "      <td>79.885337</td>\n",
       "      <td>88.188885</td>\n",
       "      <td>78.234737</td>\n",
       "      <td>1.982887</td>\n",
       "      <td>2.839549</td>\n",
       "      <td>2.016865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>107.715892</td>\n",
       "      <td>118.423605</td>\n",
       "      <td>106.460575</td>\n",
       "      <td>117.041687</td>\n",
       "      <td>3.671229</td>\n",
       "      <td>4.840261</td>\n",
       "      <td>3.805332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>142.352731</td>\n",
       "      <td>139.683514</td>\n",
       "      <td>143.969704</td>\n",
       "      <td>134.840153</td>\n",
       "      <td>6.086119</td>\n",
       "      <td>6.949471</td>\n",
       "      <td>5.727409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>166.038300</td>\n",
       "      <td>156.881424</td>\n",
       "      <td>169.393112</td>\n",
       "      <td>152.318375</td>\n",
       "      <td>21.255671</td>\n",
       "      <td>14.787565</td>\n",
       "      <td>7.397575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           real_X      real_Y   predict_X   predict_Y  abs_error_X  \\\n",
       "count  600.000000  600.000000  600.000000  600.000000   600.000000   \n",
       "mean   111.680770  105.675013  111.932991  104.710663     4.493221   \n",
       "std     35.086631   40.743186   35.659561   39.221386     3.493267   \n",
       "min      9.703297   25.989432   13.895754   25.882315     0.007548   \n",
       "25%     88.703687   79.885337   88.188885   78.234737     1.982887   \n",
       "50%    107.715892  118.423605  106.460575  117.041687     3.671229   \n",
       "75%    142.352731  139.683514  143.969704  134.840153     6.086119   \n",
       "max    166.038300  156.881424  169.393112  152.318375    21.255671   \n",
       "\n",
       "       abs_error_Y  mean_abs_error  \n",
       "count   600.000000      600.000000  \n",
       "mean      5.231294        3.854094  \n",
       "std       3.288079        2.088673  \n",
       "min       0.006811        0.021989  \n",
       "25%       2.839549        2.016865  \n",
       "50%       4.840261        3.805332  \n",
       "75%       6.949471        5.727409  \n",
       "max      14.787565        7.397575  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_X</th>\n",
       "      <th>real_Y</th>\n",
       "      <th>predict_X</th>\n",
       "      <th>predict_Y</th>\n",
       "      <th>abs_error_X</th>\n",
       "      <th>abs_error_Y</th>\n",
       "      <th>mean_abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>146.344747</td>\n",
       "      <td>26.271971</td>\n",
       "      <td>167.600418</td>\n",
       "      <td>38.992283</td>\n",
       "      <td>21.255671</td>\n",
       "      <td>12.720312</td>\n",
       "      <td>0.968886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        real_X     real_Y   predict_X  predict_Y  abs_error_X  abs_error_Y  \\\n",
       "59  146.344747  26.271971  167.600418  38.992283    21.255671    12.720312   \n",
       "\n",
       "    mean_abs_error  \n",
       "59        0.968886  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fitting\n",
      "Global Train/Validation MAE: 4.487535021385855/7.397574656182357\n",
      "Saving Models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f84c6cd29e43b0bf581d536f823a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/models24_v8_LSTMx2_s10_1619435241/5da1382d4db8ce0c98bbe92e_s1\\assets\n",
      "INFO:tensorflow:Assets written to: ./saved_models/models24_v8_LSTMx2_s10_1619435241/5da1382d4db8ce0c98bbe92e_s2\\assets\n",
      "Finished Models Saving\n"
     ]
    }
   ],
   "source": [
    "seq_len = 10\n",
    "\n",
    "models24_xy = {}\n",
    "train_val_comparison_xy = []\n",
    "num_s = 0\n",
    "\n",
    "site_ids_shapes = {\"5d2709c303f801723c3299ee\": (33368, 11667), \"5dbc1d84c1eb61796cf7c010\": (61727, 9043), \"5d27075f03f801723c2e360f\": (73141, 14063), \"5dc8cea7659e181adb076a3f\": (57849, 9733), \n",
    "                   \"5d27096c03f801723c31e5e0\": (19337, 9933), \"5da138b74db8ce0c98bd4774\": (56668, 7075), \"5da958dd46f8266d0737457b\": (47796, 7003), \"5d2709bb03f801723c32852c\": (44009, 4909), \n",
    "                   \"5a0546857ecc773753327266\": (26532, 6799), \"5c3c44b80379370013e0fd2b\": (29359, 6131), \"5d27097f03f801723c320d97\": (35121, 4985), \"5da1382d4db8ce0c98bbe92e\": (28975, 5729),\n",
    "                   \"5d2709b303f801723c327472\": (32449, 3831), \"5d2709d403f801723c32bd39\": (23545, 4283), \"5da138764db8ce0c98bcaa46\": (27771, 3781), \"5da1383b4db8ce0c98bc11ab\": (34396, 3055),\n",
    "                   \"5d2709e003f801723c32d896\": (29752, 2623), \"5da138754db8ce0c98bca82f\": (13795, 3259), \"5da1389e4db8ce0c98bd0547\": (17795, 2047), \"5da138314db8ce0c98bbf3a0\": (13122, 2429),\n",
    "                   \"5d2709a003f801723c3251bf\": (9345, 2509), \"5d27099f03f801723c32511d\": (9700, 1855), \"5da138364db8ce0c98bc00f1\": (5555, 1649), \"5da138274db8ce0c98bbd3d2\": (6338, 985)}\n",
    "\n",
    "site_ids = [\"5da1382d4db8ce0c98bbe92e\"]\n",
    "\n",
    "for site_id in site_ids:#site_ids_shapes.keys():#site_ids_shapes.keys(): #site_ids:#site_ids_num_bddsid.keys(): #site_ids:\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    num_s += 1\n",
    "    print(f\"Working on #{num_s}: Site-{site_id}\")\n",
    "    #  Stage #1 ########################################################################       \n",
    "    f_list, scaler_xy, encoder_xy, train_x, val_x, train_y, val_y = preprocess_data(load_data(site_id, 10000, seq_len=seq_len),\n",
    "                                                                                    seq_len=seq_len, train_fraction=0.95, random_state=123)\n",
    "    \n",
    "    model_xy = LSTM(site_id=site_id,\n",
    "                    seq_len=seq_len,\n",
    "                    train_x=train_x, val_x=val_x,\n",
    "                    train_y=train_y, val_y=val_y,\n",
    "                    w_f=0.1, drop=0.1, learning_rate=round(100/site_ids_shapes[site_id][0], 4), epochs=100, batch=7)\n",
    "    ####################################################################################\n",
    "    # Stage #2  ########################################################################\n",
    "    scaler_xy_2, train_x_2, val_x_2 = preprocess_data_2(len(f_list),\n",
    "                                                        train_x, val_x,\n",
    "                                                        model_xy.predict(train_x), model_xy.predict(val_x))\n",
    "    \n",
    "    model_xy_2 = LSTM_2(site_id=site_id,\n",
    "                        seq_len=seq_len,\n",
    "                        train_x=train_x_2, val_x=val_x_2,\n",
    "                        train_y=train_y, val_y=val_y,\n",
    "                        w_f=200, drop=0.01, learning_rate=round(100/site_ids_shapes[site_id][0], 4), epochs=100, batch=7)\n",
    "\n",
    "    models24_xy[site_id] = [f_list, scaler_xy, encoder_xy, model_xy, scaler_xy_2, model_xy_2]\n",
    "    #===================================================================================\n",
    "    # sanity check section\n",
    "    predict_columns = [\"predict_X\", \"predict_Y\"]\n",
    "    real_columns = [\"real_X\", \"real_Y\"]\n",
    "    col_dic = {0: \"_X\", 1: \"_Y\"}\n",
    "\n",
    "    predictions_train_xy = pd.DataFrame(model_xy_2.predict(train_x_2).reshape(train_x_2.shape[0]*train_x_2.shape[1], 2),\n",
    "                                        columns=predict_columns)\n",
    "    comparison_train_xy = pd.concat([pd.DataFrame(train_y.reshape(train_x_2.shape[0]*train_x_2.shape[1], 2), \n",
    "                                                  columns=real_columns),\n",
    "                                     predictions_train_xy], axis=1)\n",
    "    predictions_val_xy = pd.DataFrame(model_xy_2.predict(val_x_2).reshape(val_x_2.shape[0]*val_x_2.shape[1], 2), \n",
    "                                      columns=predict_columns)\n",
    "    comparison_val_xy = pd.concat([pd.DataFrame(val_y.reshape(val_x_2.shape[0]*val_x_2.shape[1], 2), \n",
    "                                                columns=real_columns),\n",
    "                                   predictions_val_xy], axis=1)\n",
    "    \n",
    "    for col_i, col in enumerate(predict_columns):\n",
    "        for train_valid in [comparison_train_xy, comparison_val_xy]:\n",
    "            train_valid[\"abs_error\"+col_dic[col_i]] = np.abs(train_valid[real_columns[col_i]] - train_valid[col])\n",
    "\n",
    "    for train_valid in [comparison_train_xy, comparison_val_xy]:\n",
    "        mean_error = np.sqrt(np.power(train_valid[\"abs_error_X\"], 2) + np.power(train_valid[\"abs_error_Y\"], 2))\n",
    "        train_valid[\"mean_abs_error\"] = mean_error.cumsum()/train_valid.shape[0]\n",
    "\n",
    "    print(f\"Predictions (train/validation) #{num_s}: Site-{site_id}\")\n",
    "    #display(comparison_train_xy[[\"mean_abs_error\"]].tail(1))\n",
    "    #display(comparison_val_xy[[\"mean_abs_error\"]].tail(1))\n",
    "\n",
    "    train_val_comparison_xy.append([comparison_train_xy, comparison_val_xy])\n",
    "    #========================================================================================\n",
    "    display(comparison_val_xy.describe())\n",
    "    display(comparison_val_xy[(comparison_val_xy.abs_error_X > 20) | (comparison_val_xy.abs_error_Y > 20)])\n",
    "\n",
    "    #comparison_val_xy[[\"abs_error_X\", \"abs_error_Y\"]].plot.hist(cumulative=True, bins=100, logy=True, alpha=0.3, title=site_id)\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    #break\n",
    "print(\"Finished fitting\")\n",
    "calculate_global_metrics(train_val_comparison_xy)\n",
    "model_name = save_models(models24_xy, f\"models24_v8_LSTMx2_s{seq_len}\")\n",
    "print(\"Finished Models Saving\")\n",
    "#%tensorboard --logdir log/Neurals  # commandline to start tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a084a9",
   "metadata": {},
   "source": [
    "Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d313947",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_test_data = pickle.load(open(\"./data_out/full24/test-10k_mix-counts_t550.pkl\", \"rb\"))   # counts_t550 vs counts\n",
    "floor100_siteid_traceid = pickle.load(open(\"./data_out/floor100_siteid_traceid.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d31e0f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Predictions #1: Site-5da1382d4db8ce0c98bbe92e with 11 traces\n"
     ]
    }
   ],
   "source": [
    "#model_name = \"models24_v7_LSTM_1619043731\"\n",
    "model_path = \"./saved_models/\" + model_name\n",
    "\n",
    "#models24_xy = {}\n",
    "#seq_len = 10\n",
    "seq_mode = \"moving\" # [\"full\", \"moving\", \"stacked\"]\n",
    "#site_ids = [\"5da1382d4db8ce0c98bbe92e\"]  # parsed_test_data.keys()\n",
    "\n",
    "predicted_data = {}\n",
    "n_s = 0\n",
    "for site_id in site_ids:#tqdm(site_ids_shapes.keys()):#site_ids):  # over sites\n",
    "    n_s += 1\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    print(f\"Processing Predictions #{n_s}: Site-{site_id} with {len(parsed_test_data[site_id])} traces\")\n",
    "    ############# GET MODELS ####################\n",
    "    if len(models24_xy) == 0:\n",
    "        features_xy = pickle.load(open(model_path + f\"/{site_id}_s1/features_list.pkl\", \"rb\"))\n",
    "        scaler_xy = (pickle.load(open(model_path + f\"/{site_id}_s1/scaler.pkl\", \"rb\")))\n",
    "        encoder_xy = (pickle.load(open(model_path + f\"/{site_id}_s1/f_binarizer.pkl\", \"rb\")))\n",
    "        model_xy = (tf.keras.models.load_model(model_path + f\"/{site_id}_s1\", custom_objects={\"xy_loss_metric_mse\": xy_loss_metric_mse, \"xy_loss_metric\": xy_loss_metric}))\n",
    "        scaler_xy_2 = (pickle.load(open(model_path + f\"/{site_id}_s2/scaler.pkl\", \"rb\")))\n",
    "        model_xy_2 = (tf.keras.models.load_model(model_path + f\"/{site_id}_s2\", custom_objects={\"xy_loss_metric_mse\": xy_loss_metric_mse, \"xy_loss_metric\": xy_loss_metric}))\n",
    "    else:\n",
    "        features_xy = models24_xy[site_id][0]\n",
    "        scaler_xy =  models24_xy[site_id][1]\n",
    "        encoder_xy = models24_xy[site_id][2]\n",
    "        model_xy = models24_xy[site_id][3]\n",
    "        scaler_xy_2 =  models24_xy[site_id][4]\n",
    "        model_xy_2 = models24_xy[site_id][5]\n",
    "    ##############################################\n",
    "    predicted_data[site_id] = {}\n",
    "    for trace_id in parsed_test_data[site_id]:  # over traces\n",
    "        \n",
    "        trace_record = parsed_test_data[site_id][trace_id].copy(deep=True)\n",
    "        ######### GENERAL FEATURE MANIPULATION ################    \n",
    "        _time = trace_record.pop(\"time\").to_numpy()\n",
    "        \n",
    "        x_m = trace_record.pop(\"m\").to_numpy().reshape(-1,1)\n",
    "        x_r = trace_record.pop(\"r\").to_numpy().reshape(-1,1)\n",
    "        x_rx = trace_record.pop(\"rx\").to_numpy().reshape(-1,1)\n",
    "        x_ry = trace_record.pop(\"ry\").to_numpy().reshape(-1,1)\n",
    "        x_rx_cum = trace_record.pop(\"rx_cum\").to_numpy().reshape(-1,1)\n",
    "        x_ry_cum = trace_record.pop(\"ry_cum\").to_numpy().reshape(-1,1)\n",
    "        \n",
    "        trace_record_xy = trace_record[features_xy].copy(deep=True)\n",
    "        trace_record_xy = trace_record_xy.to_numpy()\n",
    "        \n",
    "        ##########################################################\n",
    "        #  PREDICT XY (with F100 feature)\n",
    "        ##########################################################\n",
    "        # combine features\n",
    "        _pred_f = floor100_siteid_traceid[site_id][trace_id]\n",
    "        trace_record_xy_scaled = scaler_xy.transform(np.concatenate((trace_record_xy, x_m, x_r, x_rx, x_ry, x_rx_cum, x_ry_cum, encoder_xy.transform(np.full_like(_time, _pred_f))), axis=1))\n",
    "        \n",
    "        folds_x, folds_y = [], []\n",
    "        # make predictions on new sequenced records\n",
    "        #for seq_len/fold in XXX:\n",
    "        \n",
    "        # stage #1\n",
    "        trace_record_xy_scaled_seq = make_seq(trace_record_xy_scaled, seq_mode, seq_len)\n",
    "        predictions_xy = model_xy.predict(trace_record_xy_scaled_seq)\n",
    "        \n",
    "        # stage #2\n",
    "        trace_record_xy_scaled_2 = scaler_xy_2.transform(np.concatenate((trace_record_xy_scaled[:, len(features_xy):], \n",
    "                                                                         make_seq_inv(predictions_xy[:, :, 0], seq_mode, seq_len),\n",
    "                                                                         make_seq_inv(predictions_xy[:, :, 1], seq_mode, seq_len)), axis=1))\n",
    "        trace_record_xy_scaled_2_seq = make_seq(trace_record_xy_scaled_2, seq_mode, seq_len)\n",
    "        predictions_xy = model_xy_2.predict(trace_record_xy_scaled_2_seq)\n",
    "\n",
    "        folds_x.append(make_seq_inv(predictions_xy[:, :, 0], seq_mode, seq_len))\n",
    "        folds_y.append(make_seq_inv(predictions_xy[:, :, 1], seq_mode, seq_len))\n",
    "        #print(predictions_xy.shape)\n",
    "\n",
    "        # remove outliers (and combine different folds/seq_len)\n",
    "        predictions_xy_x = median_filter(np.concatenate(folds_x, axis=1), (3,3))\n",
    "        predictions_xy_y = median_filter(np.concatenate(folds_y, axis=1), (3,3))\n",
    "                                                              \n",
    "        # combine into final DataFrame\n",
    "        predictions_xyf = pd.DataFrame(np.concatenate((predictions_xy_x, predictions_xy_y), axis=1), columns=[\"x\", \"y\"])\n",
    "        predictions_xyf[\"floor\"] = _pred_f\n",
    "        predictions_xyf[\"time\"] = _time[:predictions_xy_x.shape[0]]\n",
    "\n",
    "        predicted_data[site_id][trace_id] = predictions_xyf\n",
    "        \n",
    "    keras.backend.clear_session()\n",
    "        \n",
    "        #break  # only first trace\n",
    "    \n",
    "    #break  # only first site_id\n",
    "#plot_predictions_multi(model_name, predicted_data, sufix=f\"coarse_{seq_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06dfc841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28c5e7d41ba462eb7eda558c27741ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#predicted_data = pickle.load(open(f\"./submit/fit_data/{model_name}_predicted.pkl\", \"rb\"))\n",
    "plot_predictions_multi(model_name, predicted_data, sufix=f\"coarse_{seq_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd179226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d36289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./submit/fit_data/{model_name.replace('/','')}_predicted.pkl\", \"wb\") as f:\n",
    "    pickle.dump(predicted_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddb189a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cd2c595b234bd0a396337e679abd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_data = pickle.load(open(f\"./submit/fit_data/{model_name.replace('/','')}_predicted.pkl\", \"rb\"))\n",
    "make_submission(model_name.replace('/',''), predicted_data, sufix=f\"coarse_{seq_mode}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
