{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b63a89c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from post_multi import motion_multi, fs_multi, fs_multiF\n",
    "\n",
    "import matplotlib.path as mpltPath\n",
    "from scipy.spatial import distance\n",
    "from scipy.optimize import minimize\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from PIL import Image\n",
    "#%matplotlib inline\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from xyz10.io_f_mod import read_data_file\n",
    "from xyz10.visualize_f_mod import visualize_trajectory, save_figure_to_image\n",
    "from xyz10.compute_f_mod import compute_step_positions, compute_step_positions_mod, split_ts_seq\n",
    "from xyz10.compute_f_mod import compute_steps, compute_headings, compute_stride_length, compute_step_heading, compute_rel_positions\n",
    "from xyz10.compute_f_mod import correct_positions,  correct_positions_mod\n",
    "\n",
    "from xyz10.io_f_mod import read_data_file\n",
    "from xyz10.visualize_f_mod import visualize_trajectory, save_figure_to_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94171e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "paths = glob.glob('./img_out/predictions/blend_x7_0305/*/*snap2motion4s.png')\n",
    "_ = [os.remove(path) for path in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a5d12",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffd987",
   "metadata": {},
   "source": [
    "Function: Blend, Plot_Traces, Plot_Grid, Make_Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "723db886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FLOOR_NUM_to_ID = {'5a0546857ecc773753327266': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5c3c44b80379370013e0fd2b': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5d27075f03f801723c2e360f': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'},\n",
    "                 '5d27096c03f801723c31e5e0': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6'},\n",
    "                 '5d27097f03f801723c320d97': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5d27099f03f801723c32511d': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5d2709a003f801723c3251bf': {0: '1F', 1: '2F', 2: '3F', 3: '4F'},\n",
    "                 '5d2709b303f801723c327472': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F', 3: '4F'},\n",
    "                 '5d2709bb03f801723c32852c': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5d2709c303f801723c3299ee': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F', 3: '4F', 4: '5F', 5: '6F', 6: '7F', 7: '8F', 8: '9F'},\n",
    "                 '5d2709d403f801723c32bd39': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F'},\n",
    "                 '5d2709e003f801723c32d896': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5da138274db8ce0c98bbd3d2': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                 '5da1382d4db8ce0c98bbe92e': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5da138314db8ce0c98bbf3a0': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                 '5da138364db8ce0c98bc00f1': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                 '5da1383b4db8ce0c98bc11ab': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                 '5da138754db8ce0c98bca82f': {0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5da138764db8ce0c98bcaa46': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5da1389e4db8ce0c98bd0547': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5da138b74db8ce0c98bd4774': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5da958dd46f8266d0737457b': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'},\n",
    "                 '5dbc1d84c1eb61796cf7c010': {-1: 'B1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7', 7: 'F8'},\n",
    "                 '5dc8cea7659e181adb076a3f': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'}}\n",
    "\n",
    "def blend_predictions(blend_folder, reference_file):\n",
    "\n",
    "    blend_paths = glob.glob(blend_folder + \"*\")\n",
    "    blend_data = []\n",
    "    for b_path in blend_paths:\n",
    "        blend_data.append(pickle.load(open(b_path, \"rb\")))\n",
    "        #break\n",
    "    reference_data = pickle.load(open(blend_folder+reference_file, \"rb\"))\n",
    "\n",
    "    compound= {}\n",
    "    for site_id in reference_data.keys():\n",
    "\n",
    "        compound[site_id] = {}\n",
    "        for trace_id in reference_data[site_id].keys():\n",
    "\n",
    "            _timestamps = reference_data[site_id][trace_id].to_numpy()[:, 3]\n",
    "            _floor = int(np.median(reference_data[site_id][trace_id].to_numpy()[:, 2]))\n",
    "            _x = []\n",
    "            _y = []\n",
    "            for data in blend_data:\n",
    "\n",
    "                predicted_record = data[site_id][trace_id].to_numpy()\n",
    "                _x.append(interp1d(predicted_record[:, 3], predicted_record[:, 0], kind=\"linear\", copy=False, fill_value=\"extrapolate\")(_timestamps))\n",
    "                _y.append(interp1d(predicted_record[:, 3], predicted_record[:, 1], kind=\"linear\", copy=False, fill_value=\"extrapolate\")(_timestamps))\n",
    "\n",
    "            compound[site_id][trace_id] = pd.DataFrame({\"x\": np.median(_x, axis=0), \"y\": np.median(_y, axis=0), \"floor\": _floor, \"time\": _timestamps})\n",
    "\n",
    "            #break \n",
    "    return compound\n",
    "\n",
    "def make_submission_bkp(model_name, data, sufix=\"coarse\"):\n",
    "\n",
    "    sample_submit = pd.read_csv(\"./submit/sample_submission.csv\")\n",
    "    splits = sample_submit.site_path_timestamp.str.split(pat=\"_\", expand=True)\n",
    "    sub_data = sample_submit.copy(deep=True).join(splits)\n",
    "    sub_data.rename(columns={0:\"site\", 1:\"path\", 2:\"timestamp\"}, inplace=True)\n",
    "\n",
    "    for i in tqdm(list(sub_data.index)):\n",
    "        site_id = sub_data.site[i]\n",
    "        trace_id = sub_data.path[i]\n",
    "        timestamp = sub_data.timestamp[i]\n",
    "\n",
    "        predicted_record = data[site_id][trace_id].to_numpy()\n",
    "\n",
    "        func_x = interp1d(predicted_record[:, 3], predicted_record[:, 0], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "        func_y = interp1d(predicted_record[:, 3], predicted_record[:, 1], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "\n",
    "        sub_data.loc[i, \"x\"] = func_x(timestamp)\n",
    "        sub_data.loc[i, \"y\"] = func_y(timestamp)\n",
    "        sub_data.loc[i, \"floor\"] = int(np.median(predicted_record[:, 2]))\n",
    "        #break\n",
    "\n",
    "    _ = [sub_data.pop(col) for col in [\"site\", \"path\", \"timestamp\"]]\n",
    "\n",
    "    sub_data.to_csv(f\"./submit/{model_name}_{sufix}.csv\", index=False)\n",
    "    \n",
    "\n",
    "def make_submission(model_name, data, sufix=\"coarse\"):\n",
    "    \n",
    "    sample_submit = pd.read_csv(\"./submit/sample_submission.csv\")\n",
    "    splits = sample_submit.site_path_timestamp.str.split(pat=\"_\", expand=True)\n",
    "    sub_data = sample_submit.copy(deep=True).join(splits)\n",
    "    sub_data.rename(columns={0:\"site\", 1:\"path\", 2:\"timestamp\"}, inplace=True)\n",
    "\n",
    "    gr = sub_data.groupby(\"path\")\n",
    "    for trace_id in gr.groups:\n",
    "        timestamps = sub_data.loc[gr.groups[trace_id]].timestamp.to_list()\n",
    "\n",
    "        site_id = sub_data.loc[gr.groups[trace_id]].site.to_list()[0]\n",
    "        predicted_record = data[site_id][trace_id].to_numpy()\n",
    "\n",
    "        func_x = interp1d(predicted_record[:, 3], predicted_record[:, 0], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "        func_y = interp1d(predicted_record[:, 3], predicted_record[:, 1], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "\n",
    "        sub_data.loc[gr.groups[trace_id], \"x\"] = func_x(timestamps)\n",
    "        sub_data.loc[gr.groups[trace_id], \"y\"] = func_y(timestamps)\n",
    "        sub_data.loc[gr.groups[trace_id], \"floor\"] = predicted_record[0, 2]\n",
    "        #break\n",
    "\n",
    "    _ = [sub_data.pop(col) for col in [\"site\", \"path\", \"timestamp\"]]\n",
    "\n",
    "    sub_data.to_csv(f\"./submit/{model_name}_{sufix}.csv\", index=False)\n",
    "    \n",
    "def plot_predictions_multi(model_name, data, sufix=\"coarse\", delay_suffix=False):\n",
    "    \n",
    "    def swap_trace_floor(predicted_data):\n",
    "        swap = {}\n",
    "\n",
    "        for site_id in predicted_data.keys():\n",
    "\n",
    "            swap[site_id] = {}\n",
    "            for trace_id in predicted_data[site_id].keys():\n",
    "\n",
    "                floor_id = predicted_data[site_id][trace_id].floor[0]\n",
    "                if floor_id not in swap[site_id].keys():\n",
    "                    swap[site_id][floor_id] = {}\n",
    "                swap[site_id][floor_id][trace_id] = predicted_data[site_id][trace_id]\n",
    "\n",
    "        return swap\n",
    "\n",
    "    data = swap_trace_floor(data)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f\"./img_out/predictions/{model_name}/\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    n_s = 0\n",
    "    for site_id in tqdm(data.keys()):  # over sites \n",
    "        n_s += 1\n",
    "        #print(f\"Processing Trajectories #{n_s}: Site-{site_id} with {len(data[site_id])} traces\")\n",
    "\n",
    "        try:\n",
    "            os.makedirs(f\"./img_out/predictions/{model_name}/{site_id}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for floor_id in data[site_id]:  # over traces\n",
    "            site_path = \"./data_in/metadata/\" + site_id + \"/\"\n",
    "            \n",
    "            positions = []\n",
    "            legends = []\n",
    "            for trace_id in data[site_id][floor_id].keys():\n",
    "                positions.append(data[site_id][floor_id][trace_id].to_numpy()[:, :2])\n",
    "                \n",
    "                if delay_suffix:\n",
    "                    legends.append(f\"{trace_id}_s{int(0.001*data[site_id][floor_id][trace_id].to_numpy()[0, 4])}_e{int(0.001*data[site_id][floor_id][trace_id].to_numpy()[1, 4])}\")\n",
    "                else:\n",
    "                    legends.append(trace_id)\n",
    "\n",
    "            try:\n",
    "                floor = FLOOR_NUM_to_ID[site_id][floor_id]\n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                with open(info_path) as info_file:\n",
    "                    info_data = json.load(info_file)             \n",
    "\n",
    "                map_width = info_data[\"map_info\"][\"width\"]\n",
    "                map_height = info_data[\"map_info\"][\"height\"]\n",
    "\n",
    "                fig_steps = visualize_trajectory(trajectory=positions, is_multi = True,\n",
    "                                                 floor_plan_filename=map_path, mode=\"lines + markers\", title=f\"{site_id}_{floor}_{sufix}\", legends=legends, \n",
    "                                                 width_meter=map_width,  height_meter=map_height)\n",
    "                save_figure_to_image(fig_steps, f\"./img_out/predictions/{model_name}/{site_id}/{floor}_{sufix}.png\")\n",
    "            except:\n",
    "                print(f\"Exception: wrong floor-{floor} site-{site_id}\")\n",
    "\n",
    "        #break  # only first site_id\n",
    "        \n",
    "def plot_grid(model_name, data, sufix=\"coarse\", delay_suffix=False):\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f\"./img_out/{model_name}/\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    n_s = 0\n",
    "    for site_id in tqdm(data.keys()):  # over sites \n",
    "        n_s += 1\n",
    "        #print(f\"Processing Trajectories #{n_s}: Site-{site_id} with {len(data[site_id])} traces\")\n",
    "\n",
    "        try:\n",
    "            os.makedirs(f\"./img_out/{model_name}/{site_id}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for floor_id in data[site_id]:  # over traces\n",
    "            site_path = \"./data_in/metadata/\" + site_id + \"/\"\n",
    "            \n",
    "            positions = data[site_id][floor_id]\n",
    "            legends = []\n",
    "\n",
    "            try:\n",
    "                floor = FLOOR_NUM_to_ID[site_id][floor_id]\n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                with open(info_path) as info_file:\n",
    "                    info_data = json.load(info_file)             \n",
    "\n",
    "                map_width = info_data[\"map_info\"][\"width\"]\n",
    "                map_height = info_data[\"map_info\"][\"height\"]\n",
    "\n",
    "                fig_steps = visualize_trajectory(trajectory=positions, is_multi = False,\n",
    "                                                 floor_plan_filename=map_path, mode=\"markers\", title=f\"{site_id}_{floor}_{sufix}\", legends=legends, \n",
    "                                                 width_meter=map_width,  height_meter=map_height)\n",
    "                save_figure_to_image(fig_steps, f\"./img_out/{model_name}/{site_id}/{floor}_{sufix}.png\")\n",
    "            except:\n",
    "                print(f\"Exception: wrong floor-{floor} site-{site_id}\")\n",
    "                \n",
    "        #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e5af150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def snap2grid(predicted_data, grid_siteid_floorid, timestamps_traceid, snap_range=5):\n",
    "\n",
    "    def closest_point(path, point, snap_range):\n",
    "        #print(path)\n",
    "        distance = (path[:, 0] - point[0])**2 + (path[:, 1] - point[1])**2\n",
    "        \n",
    "        if distance.min() < snap_range**2:\n",
    "            idx = distance.argmin()\n",
    "            return [path[idx, 0], path[idx, 1]]\n",
    "        else:\n",
    "            return [point[0], point[1]]\n",
    "\n",
    "    snap2grid_data = {}\n",
    "\n",
    "    n_s= 0\n",
    "    for site_id in tqdm(predicted_data.keys()):  # over sites\n",
    "        n_s += 1\n",
    "\n",
    "        snap2grid_data[site_id] = {}\n",
    "        for trace_id in predicted_data[site_id].keys():  # over traces\n",
    "\n",
    "            trace = []  # list of points [x,y]\n",
    "            predicted_record = predicted_data[site_id][trace_id].to_numpy()\n",
    "            \n",
    "            floor_id = int(np.median(predicted_record[:, 2]))\n",
    "            grid = grid_siteid_floorid[site_id][floor_id]\n",
    "            \n",
    "            func_x = interp1d(predicted_record[:, 3], predicted_record[:, 0], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "            func_y = interp1d(predicted_record[:, 3], predicted_record[:, 1], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "\n",
    "            _x = func_x(timestamps_traceid[trace_id])\n",
    "            _y = func_y(timestamps_traceid[trace_id])\n",
    "\n",
    "            for i, _ in enumerate(_x):  # over points \n",
    "                point = [_x[i], _y[i]]\n",
    "                trace.append(closest_point(grid, point, snap_range))  # grid = closest step/waypoint point(slow) vs path_a0= closest contour point (fast) \n",
    "\n",
    "            snap2grid_data[site_id][trace_id] = pd.DataFrame(trace, columns=[\"x\", \"y\"])\n",
    "            snap2grid_data[site_id][trace_id][\"floor\"] = floor_id\n",
    "            snap2grid_data[site_id][trace_id][\"time\"] = timestamps_traceid[trace_id]\n",
    "            \n",
    "    return snap2grid_data\n",
    "\n",
    "def snap2grid_v2(predicted_data, grid_siteid_floorid, aux_grid_siteid_floorid, timestamps_traceid, snap_range=5, aux_snap_range=30):\n",
    "\n",
    "    def closest_point(path, point, snap_range):\n",
    "        #print(path)\n",
    "        distance = (path[:, 0] - point[0])**2 + (path[:, 1] - point[1])**2\n",
    "        \n",
    "        if distance.min() < snap_range**2:\n",
    "            idx = distance.argmin()\n",
    "            return [path[idx, 0], path[idx, 1]], True\n",
    "        else:\n",
    "            return [point[0], point[1]], False\n",
    "\n",
    "    snap2grid_data = {}\n",
    "\n",
    "    n_s= 0\n",
    "    for site_id in tqdm(predicted_data.keys()):  # over sites\n",
    "        n_s += 1\n",
    "\n",
    "        snap2grid_data[site_id] = {}\n",
    "        for trace_id in predicted_data[site_id].keys():  # over traces\n",
    "\n",
    "            trace = []  # list of points [x,y]\n",
    "            predicted_record = predicted_data[site_id][trace_id].to_numpy()\n",
    "            \n",
    "            floor_id = int(np.median(predicted_record[:, 2]))\n",
    "            grid = grid_siteid_floorid[site_id][floor_id]\n",
    "            \n",
    "            func_x = interp1d(predicted_record[:, 3], predicted_record[:, 0], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "            func_y = interp1d(predicted_record[:, 3], predicted_record[:, 1], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "\n",
    "            _x = func_x(timestamps_traceid[trace_id])\n",
    "            _y = func_y(timestamps_traceid[trace_id])\n",
    "\n",
    "            for i, _ in enumerate(_x):  # over points \n",
    "                point = [_x[i], _y[i]]\n",
    "                _closest_point, _isOk = closest_point(grid, point, snap_range)\n",
    "                if _isOk:\n",
    "                    trace.append(_closest_point)  # grid = closest step/waypoint point(slow) vs path_a0= closest contour point (fast)\n",
    "                else:\n",
    "                    _aus_close_point, _ = closest_point(aux_grid, point, aux_snap_range)\n",
    "                    trace.append(_aus_close_point)\n",
    "\n",
    "            snap2grid_data[site_id][trace_id] = pd.DataFrame(trace, columns=[\"x\", \"y\"])\n",
    "            snap2grid_data[site_id][trace_id][\"floor\"] = floor_id\n",
    "            snap2grid_data[site_id][trace_id][\"time\"] = timestamps_traceid[trace_id]\n",
    "            \n",
    "    return snap2grid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a708790",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c979a",
   "metadata": {},
   "source": [
    "Snap2Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd2485b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DELAY_OFFSET = 500 # msecs\n",
    "V_LIMIT = 0.0015  # m/msec\n",
    "INVALID_RANGE = 0.5 # invalidity distance in fraction of trajectory extent\n",
    "\n",
    "def optimize_trajectory_motion(motion_x, motion_y, motion_t,\n",
    "                               coarse_x, coarse_y, coarse_t,\n",
    "                               leaked_record, step_range, angle_range, angle0_range,\n",
    "                               verbose=False):  # +-step_range in fraction of step length; +-angle_range in fraction of Pi\n",
    "\n",
    "    num_elements = motion_t.shape[0]\n",
    "    trajectory_extent = ((coarse_x.max() - coarse_x.min()) ** 2 + (coarse_x.max() - coarse_x.min()) ** 2) ** 0.5\n",
    "\n",
    "    time_deltas = np.diff(motion_t)  # size \"n-1\"\n",
    "    motion_steps_lengths_input = ((motion_x ** 2 + motion_y ** 2) ** 0.5)[1:]  # size \"n-1\"\n",
    "    motion_angles_input = np.arctan2(motion_y[1:], motion_x[1:])  # size \"n-1\"\n",
    "    # print(num_elements)\n",
    "\n",
    "    ################  INITIAL/BOUNDARIES VALUES   ##################################\n",
    "    step_lengths_ini = np.amin(np.vstack([time_deltas * V_LIMIT, motion_steps_lengths_input]), axis=0)\n",
    "    angles_ini = motion_angles_input\n",
    "\n",
    "    step_lengths_min = np.amin(np.vstack([time_deltas * V_LIMIT * 0.99, motion_steps_lengths_input * (1 - step_range)]),\n",
    "                               axis=0)\n",
    "    step_lengths_max = np.amin(np.vstack([time_deltas * V_LIMIT * 1.00, motion_steps_lengths_input * (1 + step_range)]),\n",
    "                               axis=0)  # not more than maximum speed nor allowed variation of step length\n",
    "    angles_min = motion_angles_input - np.pi * angle_range\n",
    "    angles_max = motion_angles_input + np.pi * angle_range\n",
    "\n",
    "    angle0_ini = np.array([0.0])\n",
    "\n",
    "    # if non sequential=>negative-invalid delay/negative-invalid position point/delay is too long => take no limits, i.e. something large e.g. 1e6 = 1000s ~ 1500meters\n",
    "    # make rough estimate based on median of coarse_data in case of very long/invalid delays/invaliude start-end points(-1m value)\n",
    "\n",
    "    startpoint_invalid = False\n",
    "    if leaked_record[\"start_x\"] > 0 and leaked_record[\"start_delay\"] > 0 and (\n",
    "            V_LIMIT * leaked_record[\"start_delay\"]) < trajectory_extent * INVALID_RANGE:\n",
    "        _start_delay = leaked_record[\"start_delay\"] - DELAY_OFFSET if leaked_record[\n",
    "                                                                          \"start_delay\"] > DELAY_OFFSET else 100\n",
    "    else:\n",
    "        startpoint_invalid = True\n",
    "        _start_delay = 1e6\n",
    "\n",
    "    endpoint_invalid = False\n",
    "    if leaked_record[\"end_x\"] > 0 and leaked_record[\"end_delay\"] > 0 and (\n",
    "            V_LIMIT * leaked_record[\"end_delay\"]) < trajectory_extent * INVALID_RANGE:\n",
    "        _end_delay = leaked_record[\"end_delay\"] - DELAY_OFFSET if leaked_record[\"end_delay\"] > DELAY_OFFSET else 100\n",
    "\n",
    "    else:\n",
    "        endpoint_invalid = True\n",
    "        _end_delay = 1e6\n",
    "\n",
    "    initial_start = np.array([coarse_x[0], coarse_y[0]])\n",
    "    initial_end = np.array([coarse_x[-1], coarse_y[-1]])\n",
    "\n",
    "    x_start_min = leaked_record[\"start_x\"] - _start_delay * V_LIMIT\n",
    "    x_start_max = leaked_record[\"start_x\"] + _start_delay * V_LIMIT\n",
    "    x_end_min = leaked_record[\"end_x\"] - _end_delay * V_LIMIT\n",
    "    x_end_max = leaked_record[\"end_x\"] + _end_delay * V_LIMIT\n",
    "    y_start_min = leaked_record[\"start_y\"] - _start_delay * V_LIMIT\n",
    "    y_start_max = leaked_record[\"start_y\"] + _start_delay * V_LIMIT\n",
    "    y_end_min = leaked_record[\"end_y\"] - _end_delay * V_LIMIT\n",
    "    y_end_max = leaked_record[\"end_y\"] + _end_delay * V_LIMIT\n",
    "\n",
    "    bounds_angle0 = [(-angle0_range * np.pi, angle0_range * np.pi)]\n",
    "    bounds_start = [(0, None), (0, None)]\n",
    "    # bounds_start = [(x_start_min, x_start_max), (y_start_min, y_start_max)]  # 2x pairs\n",
    "    bounds_end = [(0, None), (0, None)]\n",
    "    # bounds_end = [(x_end_min, x_end_max), (y_end_min, y_end_max)] # 2x pairs\n",
    "    bounds_steps = [(min_el, max_el) for min_el, max_el in zip(step_lengths_min, step_lengths_max)]  # (n-1)x pairs\n",
    "    bounds_angles = [(min_el, max_el) for min_el, max_el in zip(angles_min, angles_max)]  # (n-1)x pairs\n",
    "\n",
    "    if endpoint_invalid:\n",
    "        initial_guess = np.concatenate([step_lengths_ini, angles_ini, angle0_ini, initial_start])  # 1+(2*n+2)x\n",
    "        bounds = bounds_steps + bounds_angles + bounds_angle0 + bounds_start  # 1+(2*n+2)x pairs\n",
    "    else:\n",
    "        initial_guess = np.concatenate(\n",
    "            [step_lengths_ini, angles_ini, angle0_ini, initial_start, initial_end])  # 1+(2*n+2)x\n",
    "        bounds = bounds_steps + bounds_angles + bounds_angle0 + bounds_start + bounds_end  # 1+(2*n+2)x pairs\n",
    "\n",
    "    if verbose:\n",
    "        print(\"----------------------------------------------------------\")\n",
    "        print(\"invalid distance\", trajectory_extent * INVALID_RANGE)\n",
    "        print(f\"invalid start/end: {startpoint_invalid}/{endpoint_invalid}\")\n",
    "        print(\n",
    "            f'leaked_start/end:  {leaked_record[\"start_x\"]}-{leaked_record[\"start_y\"]}/{leaked_record[\"end_x\"]}-{leaked_record[\"end_y\"]}')\n",
    "        print(f\"_start/end_delay: {_start_delay}/{_end_delay} \")\n",
    "        print(f\"initial_start/end:  {initial_start}/{initial_end}\")\n",
    "        print(f\"bounds_start/end: {bounds_start}/{bounds_end}\")\n",
    "        print(\"----------------------------------------------------------\")\n",
    "    ###############################################################################\n",
    "    d_ind_a = 1  # indices shift due to alpha0\n",
    "\n",
    "    def loss(params):\n",
    "        # print(params.shape)\n",
    "        _d_xs = np.cumsum(params[: num_elements - 1] * np.cos(\n",
    "            params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "        _d_ys = np.cumsum(params[: num_elements - 1] * np.sin(\n",
    "            params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "        _xs = np.hstack([params[2 * num_elements - 2 + d_ind_a + 0],\n",
    "                         params[2 * num_elements - 2 + d_ind_a + 0] + _d_xs])\n",
    "        _ys = np.hstack([params[2 * num_elements - 2 + d_ind_a + 1],\n",
    "                         params[2 * num_elements - 2 + d_ind_a + 1] + _d_ys])\n",
    "\n",
    "        if not endpoint_invalid:\n",
    "            _xs = _xs - (_xs[-1] - params[-2]) / 1\n",
    "            _ys = _ys - (_ys[-1] - params[-1]) / 1\n",
    "\n",
    "        _loss = np.sum(((coarse_x - _xs) ** 2 + (coarse_y - _ys) ** 2) ** 0.5) / num_elements  # MAE\n",
    "        return _loss\n",
    "\n",
    "    results = minimize(fun=loss,\n",
    "                       x0=initial_guess,\n",
    "                       bounds=bounds,\n",
    "                       options={'maxcor': 50, 'ftol': 1e-09, 'gtol': 1e-09, 'maxfun': 40000, 'maxiter': 40000,\n",
    "                                'maxls': 50},\n",
    "                       method=\"L-BFGS-B\")\n",
    "    params = results.x\n",
    "\n",
    "    _d_xs = np.cumsum(params[: num_elements - 1] * np.cos(\n",
    "        params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "    _d_ys = np.cumsum(params[: num_elements - 1] * np.sin(\n",
    "        params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "    _xs = np.hstack([params[2 * num_elements - 2 + d_ind_a + 0],\n",
    "                     params[2 * num_elements - 2 + d_ind_a + 0] + _d_xs])\n",
    "    _ys = np.hstack([params[2 * num_elements - 2 + d_ind_a + 1],\n",
    "                     params[2 * num_elements - 2 + d_ind_a + 1] + _d_ys])\n",
    "\n",
    "    if not endpoint_invalid:\n",
    "        _xs = _xs - (_xs[-1] - params[-2]) / 1\n",
    "        _ys = _ys - (_ys[-1] - params[-1]) / 1\n",
    "\n",
    "    return _xs, _ys, results.fun\n",
    "\n",
    "############################################################################################################\n",
    "def snap2motion(coarse_data, motion_data, leaked_data, step_range, angle_range, angle0_range):\n",
    "\n",
    "    snapped_data = {}\n",
    "    site_ids =  [\"5da1389e4db8ce0c98bd0547\", \"5d27075f03f801723c2e360f\", \"5d2709b303f801723c327472\", \"5d27097f03f801723c320d97\", \"5da138b74db8ce0c98bd4774\", \"5d2709d403f801723c32bd39\"]\n",
    "    #site_ids = coarse_data.keys()\n",
    "\n",
    "    for site_id in tqdm(site_ids):\n",
    "        \n",
    "        #print(site_id)\n",
    "        \n",
    "        snapped_data[site_id] = {}\n",
    "        for trace_id in tqdm(coarse_data[site_id]):  # over traces\n",
    "            \n",
    "            \n",
    "            verbose_ids = [\"9cc4412ff73ec37e30be9d9f\", \"472be94f5be907c04c932114\", \"4ff5b9152353b3dc458992fa\", \"4affd104e0ec7a7806edfa78\"]\n",
    "            verbose = trace_id in verbose_ids\n",
    "            #verbose = True\n",
    "            if verbose:\n",
    "                print(trace_id)\n",
    "            \n",
    "            coarse_record = coarse_data[site_id][trace_id][[\"time\", \"x\", \"y\"]].copy(deep=True)\n",
    "            motion_record = motion_data[site_id][trace_id][[\"time\", \"rx\", \"ry\"]].copy(deep=True)\n",
    "            leaked_record = leaked_data[site_id][trace_id]\n",
    "              \n",
    "            _x, _y, _fun = optimize_trajectory_motion(motion_record.rx.to_numpy(),\n",
    "                                                      motion_record.ry.to_numpy(),\n",
    "                                                      motion_record.time.to_numpy(),  # sensor data (relative steps vs cumulative)\n",
    "                                                      coarse_record.x.to_numpy(),\n",
    "                                                      coarse_record.y.to_numpy(),\n",
    "                                                      coarse_record.time.to_numpy(),  # wifi data\n",
    "                                                      leaked_record,\n",
    "                                                      step_range, angle_range, angle0_range, verbose=verbose)\n",
    "            \n",
    "            snapped_data[site_id][trace_id] = pd.DataFrame(_x, columns=[\"x\"])\n",
    "            snapped_data[site_id][trace_id][\"y\"] = _y\n",
    "            snapped_data[site_id][trace_id][\"floor\"] = leaked_record[\"floor\"]\n",
    "            snapped_data[site_id][trace_id][\"time\"] = motion_record.time.to_numpy()\n",
    "            \n",
    "            #print(_fun)\n",
    "            #break\n",
    "        #break\n",
    "        \n",
    "    return snapped_data\n",
    "\n",
    "###############    MULTIPROCESSING VERSION     #################################################################\n",
    "def snap2motion_multi(coarse_data, motion_data, leaked_data, step_range, angle_range, angle0_range):\n",
    "\n",
    "    site_ids = [\"5d2709b303f801723c327472\"]#[\"5da1389e4db8ce0c98bd0547\", \"5d27075f03f801723c2e360f\", \"5d2709b303f801723c327472\", \"5d27097f03f801723c320d97\", \"5da138b74db8ce0c98bd4774\", \"5d2709d403f801723c32bd39\"]\n",
    "    site_ids = coarse_data.keys()\n",
    "    \n",
    "    input_data = [(site_id, coarse_data[site_id], motion_data[site_id], leaked_data[site_id], step_range, angle_range, angle0_range) for site_id in site_ids]\n",
    "    \n",
    "    pool = Pool(cpu_count()-3) # Create a multiprocessing Pool\n",
    "    snapped_data = pool.starmap(motion_multi, input_data)  # process input_data iterable with pool\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "        \n",
    "    res_d = snapped_data[0]\n",
    "    for i, _d in enumerate(snapped_data):\n",
    "        if i>0:\n",
    "            res_d.update(_d)\n",
    "        \n",
    "    return res_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18506420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"blend_x7_0305\"\n",
    "parsed_test_data = pickle.load(open(\"./data_out/full24/test-10k_mix-counts_t550.pkl\", \"rb\"))   # counts_t550 vs counts\n",
    "predicted_data = pickle.load(open(\"./submit/fit_data/blend_x7_0305.pkl\", \"rb\"))   #\n",
    "leaked_data = pickle.load(open(\"./data_out/leaked/leaked_siteid_traceid_all_delays.pkl\", \"rb\")) \n",
    "\n",
    "snapped2motion = snap2motion_multi(predicted_data, parsed_test_data, leaked_data, step_range=0.3, angle_range=0.0001, angle0_range=0.0001)  # v2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9583f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef0500323044c2fb8f7899542964a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions_multi(model_name, snapped2motion, sufix=f\"snap2motion2s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68006703",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./submit/fit_data/post/{model_name}_snapped2motion2s.pkl\", \"wb\") as f:\n",
    "    pickle.dump(snapped2motion, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fdedd",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644313c",
   "metadata": {},
   "source": [
    "Snap2FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0259fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_LIMIT = 0.0015  # m/msec\n",
    "INVALID_RANGE = 0.5 # invalidity distance in fraction of trajectory extent\n",
    "SCALE_XY0 = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9264d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_trajectory_fs_v1(motion_x, motion_y, motion_t,\n",
    "                              fs_points,\n",
    "                              leaked_record,\n",
    "                              snap_range, # +-snap/start_point shift range in fraction of x/y bounding box (max-min)\n",
    "                              step_range, angle_range, angle0_range):  # +-step_range in fraction of step length; +-angle_range in fraction of Pi\n",
    "    \n",
    "    num_elements = motion_t.shape[0]\n",
    "    trajectory_extent = ((motion_x.max()-motion_x.min())**2 + (motion_y.max()-motion_y.min())**2)**0.5\n",
    "    trajectory_extent_x = abs(motion_x.max()-motion_x.min())   \n",
    "    trajectory_extent_y = abs(motion_y.max()-motion_y.min())\n",
    "    \n",
    "    snap_distance = snap_range * trajectory_extent\n",
    "    snap_distance_x = snap_range * trajectory_extent_x\n",
    "    snap_distance_y = snap_range * trajectory_extent_y\n",
    "    \n",
    "    time_deltas = np.diff(motion_t)  # size \"n-1\"\n",
    "    motion_dx = np.diff(motion_x)\n",
    "    motion_dy = np.diff(motion_y)\n",
    "    \n",
    "    motion_steps_lengths_input = (motion_dx**2 + motion_dy**2)**0.5\n",
    "    motion_angles_input = np.arctan2(motion_dy, motion_dx) \n",
    "    \n",
    "    d_ind_a = 1  # index shift due to angle0 parameter\n",
    "    #print(num_elements)\n",
    "\n",
    "    ################  INITIAL/BOUNDARIES VALUES   ##################################      \n",
    "    step_lengths_ini = np.amin(np.vstack([time_deltas * V_LIMIT, motion_steps_lengths_input]), axis=0)\n",
    "    angles_ini = motion_angles_input\n",
    "    \n",
    "    step_lengths_min = np.amin(np.vstack([time_deltas * V_LIMIT * 0.99, motion_steps_lengths_input*(1-step_range)]), axis=0)\n",
    "    step_lengths_max = np.amin(np.vstack([time_deltas * V_LIMIT * 1.00, motion_steps_lengths_input*(1+step_range)]), axis=0)  # not more than maximum speed nor allowed variation of step length\n",
    "    angles_min = motion_angles_input - np.pi*angle_range\n",
    "    angles_max = motion_angles_input + np.pi*angle_range\n",
    "    \n",
    "    angle0_ini = np.array([0.0])\n",
    "  \n",
    "    initial_start = np.array([motion_x[0], motion_y[0]])\n",
    "\n",
    "    x_start_min = initial_start[0] - snap_distance_x\n",
    "    x_start_max = initial_start[0] + snap_distance_x\n",
    "    y_start_min = initial_start[1] - snap_distance_y\n",
    "    y_start_max = initial_start[1] + snap_distance_y\n",
    "    \n",
    "    bounds_angle0 = [(-angle0_range*np.pi, angle0_range*np.pi)]\n",
    "    bounds_start = [(0, None), (0, None)]#[(x_start_min, x_start_max), (y_start_min, y_start_max)]  # 2x pairs\n",
    "    bounds_steps = [(min_el, max_el) for min_el, max_el in zip(step_lengths_min, step_lengths_max)] # (n-1)x pairs\n",
    "    bounds_angles = [(min_el, max_el) for min_el, max_el in zip(angles_min, angles_max)] # (n-1)x pairs\n",
    "    \n",
    "    initial_guess =  np.concatenate([step_lengths_ini, angles_ini, angle0_ini, initial_start])# (2*n+2)x\n",
    "    bounds = bounds_steps + bounds_angles + bounds_angle0 + bounds_start # (2*n+2)x pairs\n",
    "    ###############################################################################\n",
    "    #########################  SNAPPING POINTS   ##################################\n",
    "    snap_points = []\n",
    "    \n",
    "    for i_step, _ in enumerate(motion_x):\n",
    "        _distances = ((fs_points[:, 0] - motion_x[i_step])**2 + (fs_points[:, 1] - motion_y[i_step])**2)**0.5\n",
    "        _snap_points = fs_points[_distances <= snap_distance]\n",
    "        \n",
    "        if len(_snap_points) > 0:\n",
    "            snap_points.append(_snap_points)\n",
    "                \n",
    "    snap_points = np.concatenate(snap_points)\n",
    "    snap_points = np.array(list(set(map(tuple, snap_points))))\n",
    "    \n",
    "    #print(\"snap_distance\", snap_distance)\n",
    "    #print(\"snap_points.shape\", snap_points.shape)\n",
    "    ##################################################################################\n",
    "    ######################### OPTIMIZE ###############################################\n",
    "    if len(snap_points) > 0:  # some points are within snapping range\n",
    "               \n",
    "        def loss(params):   \n",
    "            #print(params.shape)\n",
    "            _d_xs = np.cumsum(params[: num_elements-1] * np.cos(params[2*num_elements-2]+params[num_elements-1: 2*num_elements-2]))\n",
    "            _d_ys = np.cumsum(params[: num_elements-1] * np.sin(params[2*num_elements-2]+params[num_elements-1: 2*num_elements-2]))\n",
    "            _xs = np.hstack([params[2*num_elements-2 + d_ind_a + 0], \n",
    "                             params[2*num_elements-2 + d_ind_a + 0] + _d_xs])\n",
    "            _ys = np.hstack([params[2*num_elements-2 + d_ind_a + 1], \n",
    "                             params[2*num_elements-2 + d_ind_a + 1] + _d_ys])\n",
    "\n",
    "            distances2 = (snap_points[:, 0].reshape(-1,1) - _xs.reshape(1,-1))**2 + (snap_points[:, 1].reshape(-1,1) - _ys.reshape(1,-1))**2\n",
    "            _loss = np.sum(np.amin(distances2, axis=0)**0.5)  # near-field\n",
    "            #_loss = np.sum(np.amin(distances2**0.5, axis=1))  # MAE\n",
    "            #_loss = np.sum(np.amin(distances2, axis=1))  # MSE\n",
    "\n",
    "            return _loss/num_elements\n",
    "\n",
    "        results = minimize(fun=loss,\n",
    "                           x0=initial_guess,\n",
    "                           bounds=bounds,\n",
    "                           method=\"L-BFGS-B\")\n",
    "\n",
    "        params = results.x\n",
    "\n",
    "        _d_xs = np.cumsum(params[: num_elements-1] * np.cos(params[2*num_elements-2]+params[num_elements-1: 2*num_elements-2]))\n",
    "        _d_ys = np.cumsum(params[: num_elements-1] * np.sin(params[2*num_elements-2]+params[num_elements-1: 2*num_elements-2]))\n",
    "        _xs = np.hstack([params[2*num_elements-2 + d_ind_a + 0], \n",
    "                         params[2*num_elements-2 + d_ind_a + 0] + _d_xs])\n",
    "        _ys = np.hstack([params[2*num_elements-2 + d_ind_a + 1], \n",
    "                         params[2*num_elements-2 + d_ind_a + 1] + _d_ys])\n",
    "\n",
    "        #print(\"done fitting\")\n",
    "        return _xs, _ys, results.fun\n",
    "    else:\n",
    "        return motion_x, motion_y, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74f69e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_trajectory_fs_v2(motion_x, motion_y, motion_t,\n",
    "                              fs_points,\n",
    "                              leaked_record,\n",
    "                              snap_range,  # +-snap/start_point shift range in fraction of x/y bounding box (max-min)\n",
    "                              step_range, angle_range,\n",
    "                              angle0_range):  # +-step_range in fraction of step length; +-angle_range in fraction of Pi\n",
    "\n",
    "    # REVERSE START <-> END POINTS?\n",
    "    is_start_semivalid = leaked_record[\"start_x\"] > 0 and leaked_record[\"start_delay\"] > 0\n",
    "    is_end_semivalid = leaked_record[\"end_x\"] > 0 and leaked_record[\"end_delay\"] > 0\n",
    "\n",
    "    if is_start_semivalid and is_end_semivalid:\n",
    "        is_reversed = leaked_record[\"start_delay\"] > leaked_record[\"end_delay\"]\n",
    "    elif is_end_semivalid:\n",
    "        is_reversed = True\n",
    "    else:\n",
    "        is_reversed = False\n",
    "\n",
    "    leaked_delay = leaked_record[\"end_delay\"] if is_reversed else leaked_record[\"start_delay\"]\n",
    "    leaked_x = leaked_record[\"end_x\"] if is_reversed else leaked_record[\"start_x\"]\n",
    "    leaked_y = leaked_record[\"end_y\"] if is_reversed else leaked_record[\"start_y\"]\n",
    "\n",
    "    if is_reversed:\n",
    "        motion_x = motion_x[::-1]\n",
    "        motion_y = motion_y[::-1]\n",
    "        motion_t = motion_t[::-1]\n",
    "    ###################################################################\n",
    "    num_elements = motion_t.shape[0]\n",
    "    trajectory_extent = ((motion_x.max() - motion_x.min()) ** 2 + (motion_y.max() - motion_y.min()) ** 2) ** 0.5\n",
    "    trajectory_extent_x = abs(motion_x.max() - motion_x.min())\n",
    "    trajectory_extent_y = abs(motion_y.max() - motion_y.min())\n",
    "\n",
    "    snap_distance = snap_range * trajectory_extent\n",
    "    snap_distance_x = snap_range * trajectory_extent_x\n",
    "    snap_distance_y = snap_range * trajectory_extent_y\n",
    "\n",
    "    time_deltas = np.abs(np.diff(motion_t))  # size \"n-1\"\n",
    "    motion_dx = np.diff(motion_x)\n",
    "    motion_dy = np.diff(motion_y)\n",
    "\n",
    "    motion_steps_lengths_input = (motion_dx ** 2 + motion_dy ** 2) ** 0.5\n",
    "    motion_angles_input = np.arctan2(motion_dy, motion_dx)\n",
    "    # print(num_elements)\n",
    "    ################  INITIAL/BOUNDARIES VALUES   ##################################\n",
    "    step_lengths_ini = np.amin(np.vstack([time_deltas * V_LIMIT, motion_steps_lengths_input]), axis=0)\n",
    "    angles_ini = motion_angles_input\n",
    "\n",
    "    step_lengths_min = np.amin(np.vstack([time_deltas * V_LIMIT * 0.99, motion_steps_lengths_input * (1 - step_range)]),\n",
    "                               axis=0)\n",
    "    step_lengths_max = np.amin(np.vstack([time_deltas * V_LIMIT * 1.00, motion_steps_lengths_input * (1 + step_range)]),\n",
    "                               axis=0)  # not more than maximum speed nor allowed variation of step length\n",
    "    angles_min = motion_angles_input - np.pi * angle_range\n",
    "    angles_max = motion_angles_input + np.pi * angle_range\n",
    "\n",
    "    angle0_ini = np.array([0.0])\n",
    "\n",
    "    initial_start = np.array([motion_x[0], motion_y[0]])\n",
    "\n",
    "    x_start_min = initial_start[0] - snap_distance_x\n",
    "    x_start_max = initial_start[0] + snap_distance_x\n",
    "    y_start_min = initial_start[1] - snap_distance_y\n",
    "    y_start_max = initial_start[1] + snap_distance_y\n",
    "    ############# Limit snap range by leaked data (if narrower and leaked data valid)  #######\n",
    "    if leaked_x > 0 and leaked_delay > 0 and (V_LIMIT * leaked_delay) < trajectory_extent * INVALID_RANGE:\n",
    "        _x_start_min = max(leaked_x - V_LIMIT * leaked_delay, x_start_min)\n",
    "        _x_start_max = min(leaked_x + V_LIMIT * leaked_delay, x_start_max)\n",
    "        _y_start_min = max(leaked_y - V_LIMIT * leaked_delay, y_start_min)\n",
    "        _y_start_max = min(leaked_y + V_LIMIT * leaked_delay, y_start_max)\n",
    "\n",
    "        x_start_min, x_start_max = (_x_start_min, _x_start_max) if _x_start_min < _x_start_max else (\n",
    "        x_start_min, x_start_max)\n",
    "        y_start_min, y_start_max = (_y_start_min, _y_start_max) if _y_start_min < _y_start_max else (\n",
    "        y_start_min, y_start_max)\n",
    "\n",
    "        #initial_start = np.array([(x_start_min + x_start_max) / 2, (y_start_min + y_start_max) / 2])\n",
    "    ################ RESCALE XY0/INI-BOUNDS  ####################\n",
    "    initial_start *= SCALE_XY0\n",
    "    x_start_min *= SCALE_XY0\n",
    "    x_start_max *= SCALE_XY0\n",
    "    y_start_min *= SCALE_XY0\n",
    "    y_start_max *= SCALE_XY0\n",
    "    #################################################################\n",
    "    bounds_angle0 = [(-angle0_range * np.pi, angle0_range * np.pi)]\n",
    "    bounds_start = [(0, None), (0, None)]\n",
    "    #bounds_start = [(x_start_min, x_start_max), (y_start_min, y_start_max)]  # 2x pairs\n",
    "    bounds_steps = [(min_el, max_el) for min_el, max_el in zip(step_lengths_min, step_lengths_max)]  # (n-1)x pairs\n",
    "    bounds_angles = [(min_el, max_el) for min_el, max_el in zip(angles_min, angles_max)]  # (n-1)x pairs\n",
    "\n",
    "    initial_guess = np.concatenate([step_lengths_ini, angles_ini, angle0_ini, initial_start])  # (2*n+2)x\n",
    "    bounds = bounds_steps + bounds_angles + bounds_angle0 + bounds_start  # (2*n+2)x pairs\n",
    "    ###############################################################################\n",
    "    #########################  SNAPPING POINTS   ##################################\n",
    "    snap_points = []\n",
    "\n",
    "    for i_step, _ in enumerate(motion_x):\n",
    "        _distances = ((fs_points[:, 0] - motion_x[i_step]) ** 2 + (fs_points[:, 1] - motion_y[i_step]) ** 2) ** 0.5\n",
    "        _snap_points = fs_points[_distances <= snap_distance]\n",
    "\n",
    "        if len(_snap_points) > 0:\n",
    "            snap_points.append(_snap_points)\n",
    "\n",
    "    snap_points = np.concatenate(snap_points)\n",
    "    snap_points = np.array(list(set(map(tuple, snap_points))))\n",
    "\n",
    "    # print(\"snap_distance\", snap_distance)\n",
    "    # print(\"snap_points.shape\", snap_points.shape)\n",
    "    ##################################################################################\n",
    "    ######################### OPTIMIZE ###############################################\n",
    "    if len(snap_points) > 0:  # some points are within snapping range\n",
    "\n",
    "        def loss(params):\n",
    "            # print(params.shape)\n",
    "            _d_xs = np.cumsum(params[: num_elements - 1] * np.cos(\n",
    "                params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "            _d_ys = np.cumsum(params[: num_elements - 1] * np.sin(\n",
    "                params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "            _xs = np.hstack([params[2 * num_elements - 1] / SCALE_XY0,\n",
    "                             params[2 * num_elements - 1] / SCALE_XY0 + _d_xs])\n",
    "            _ys = np.hstack([params[2 * num_elements] / SCALE_XY0,\n",
    "                             params[2 * num_elements] / SCALE_XY0 + _d_ys])\n",
    "\n",
    "            distances2 = (snap_points[:, 0].reshape(-1, 1) - _xs.reshape(1, -1)) ** 2 + (\n",
    "                        snap_points[:, 1].reshape(-1, 1) - _ys.reshape(1, -1)) ** 2\n",
    "            _loss = np.sum(np.amin(distances2, axis=0))  # near-field\n",
    "\n",
    "            return _loss / num_elements\n",
    "\n",
    "        results = minimize(fun=loss,\n",
    "                           x0=initial_guess,\n",
    "                           bounds=bounds,\n",
    "                           options={'maxcor': 30, 'ftol': 1e-08, 'gtol': 1e-07, 'maxfun': 20000, 'maxiter': 20000,\n",
    "                                    'maxls': 30},\n",
    "                           method=\"L-BFGS-B\")\n",
    "\n",
    "        params = results.x\n",
    "\n",
    "        _d_xs = np.cumsum(params[: num_elements - 1] * np.cos(\n",
    "            params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "        _d_ys = np.cumsum(params[: num_elements - 1] * np.sin(\n",
    "            params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "        _xs = np.hstack([params[2 * num_elements - 1] / SCALE_XY0,\n",
    "                         params[2 * num_elements - 1] / SCALE_XY0 + _d_xs])\n",
    "        _ys = np.hstack([params[2 * num_elements] / SCALE_XY0,\n",
    "                         params[2 * num_elements] / SCALE_XY0 + _d_ys])\n",
    "\n",
    "        if is_reversed:\n",
    "            return _xs[::-1], _ys[::-1], results.fun, results.success, results.message\n",
    "        else:\n",
    "            return _xs, _ys, results.fun, results.success, results.message\n",
    "    else:\n",
    "        return motion_x, motion_y, -1, False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484ea7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_trajectory_fs_v3(motion_x, motion_y, motion_t,\n",
    "                              fs_points,\n",
    "                              leaked_record,\n",
    "                              snap_range, # +-snap/start_point shift range in fraction of x/y bounding box (max-min)\n",
    "                              step_range, angle_range, angle0_range):  # +-step_range in fraction of step length; +-angle_range in fraction of Pi\n",
    "    \n",
    "    # REVERSE START <-> END POINTS?\n",
    "    is_start_semivalid = leaked_record[\"start_x\"] > 0 and leaked_record[\"start_delay\"] > 0 \n",
    "    is_end_semivalid = leaked_record[\"end_x\"] > 0 and leaked_record[\"end_delay\"] > 0\n",
    "    \n",
    "    if is_start_semivalid and is_end_semivalid:\n",
    "        is_reversed = leaked_record[\"start_delay\"] > leaked_record[\"end_delay\"]\n",
    "    elif is_end_semivalid:\n",
    "        is_reversed = True\n",
    "    else:\n",
    "        is_reversed = False\n",
    "        \n",
    "    leaked_delay = leaked_record[\"end_delay\"] if is_reversed else leaked_record[\"start_delay\"]\n",
    "    leaked_x =  leaked_record[\"end_x\"] if is_reversed else leaked_record[\"start_x\"]\n",
    "    leaked_y =  leaked_record[\"end_y\"] if is_reversed else leaked_record[\"start_y\"]\n",
    "\n",
    "    if is_reversed:\n",
    "        motion_x = motion_x[::-1]\n",
    "        motion_y = motion_y[::-1]\n",
    "        motion_t = motion_t[::-1]\n",
    "    ###################################################################\n",
    "    num_elements = motion_t.shape[0]\n",
    "    trajectory_extent = ((motion_x.max()-motion_x.min())**2 + (motion_y.max()-motion_y.min())**2)**0.5\n",
    "    trajectory_extent_x = abs(motion_x.max()-motion_x.min())   \n",
    "    trajectory_extent_y = abs(motion_y.max()-motion_y.min())\n",
    "    \n",
    "    snap_distance = snap_range * trajectory_extent\n",
    "    snap_distance_x = snap_range * trajectory_extent_x\n",
    "    snap_distance_y = snap_range * trajectory_extent_y\n",
    "    \n",
    "    time_deltas = np.abs(np.diff(motion_t))  # size \"n-1\"\n",
    "    motion_dx = np.diff(motion_x)\n",
    "    motion_dy = np.diff(motion_y)\n",
    "\n",
    "    motion_steps_lengths_input = (motion_dx**2 + motion_dy**2)**0.5\n",
    "    motion_angles_input = np.arctan2(motion_dy, motion_dx) \n",
    "    ################  INITIAL/BOUNDARIES VALUES   ##################################      \n",
    "    angle0_ini = np.array([motion_angles_input[0]])\n",
    "    angle0_min = np.minimum(motion_angles_input[0]-angle0_range*np.pi, motion_angles_input[0]+angle0_range*np.pi)-0.0000001\n",
    "    angle0_max = np.maximum(motion_angles_input[0]-angle0_range*np.pi, motion_angles_input[0]+angle0_range*np.pi)+0.0000001\n",
    "    # DELTA OF ANGLE CHANGE (in abs units) (n-1) segments-1 => n-2\n",
    "    angles_ini = np.diff(motion_angles_input) \n",
    "    angles_min = np.minimum(angles_ini * (1-angle_range), angles_ini * (1+angle_range))\n",
    "    angles_max = np.maximum(angles_ini * (1-angle_range), angles_ini * (1+angle_range))\n",
    "    \n",
    "    step_lengths_ini = np.minimum(time_deltas * V_LIMIT, motion_steps_lengths_input)   \n",
    "    step_lengths_min = np.minimum(time_deltas * V_LIMIT * 0.99, motion_steps_lengths_input*(1-step_range))\n",
    "    step_lengths_max = np.minimum(time_deltas * V_LIMIT * 1.00, motion_steps_lengths_input*(1+step_range))  # not more than maximum speed nor allowed variation of step length\n",
    "    \n",
    "    initial_start = np.array([motion_x[0], motion_y[0]])\n",
    "    x_start_min = initial_start[0] - snap_distance_x\n",
    "    x_start_max = initial_start[0] + snap_distance_x\n",
    "    y_start_min = initial_start[1] - snap_distance_y\n",
    "    y_start_max = initial_start[1] + snap_distance_y\n",
    "    ############# Limit snap range by leaked data (if narrower and leaked data valid)  #######\n",
    "    if leaked_x > 0 and leaked_delay > 0 and (V_LIMIT * leaked_delay) < trajectory_extent * INVALID_RANGE:\n",
    "\n",
    "        _x_start_min = max(leaked_x - V_LIMIT * leaked_delay, x_start_min)\n",
    "        _x_start_max = min(leaked_x + V_LIMIT * leaked_delay, x_start_max)\n",
    "        _y_start_min = max(leaked_y - V_LIMIT * leaked_delay, y_start_min)\n",
    "        _y_start_max = min(leaked_y + V_LIMIT * leaked_delay, y_start_max)\n",
    "        \n",
    "        x_start_min, x_start_max = (_x_start_min, _x_start_max) if _x_start_min < _x_start_max else (x_start_min, x_start_max)\n",
    "        y_start_min, y_start_max = (_y_start_min, _y_start_max) if _y_start_min < _y_start_max else (y_start_min, y_start_max)\n",
    "        \n",
    "        #initial_start = np.array([(x_start_min+x_start_max)/2, (y_start_min+y_start_max)/2])\n",
    "    ################ RESCALE XY0/INI-BOUNDS  ####################\n",
    "    initial_start *= SCALE_XY0\n",
    "    x_start_min *= SCALE_XY0 \n",
    "    x_start_max *= SCALE_XY0\n",
    "    y_start_min *= SCALE_XY0\n",
    "    y_start_max *= SCALE_XY0\n",
    "    ################################################################# \n",
    "    bounds_angle0 = [(None, None)]#[(angle0_min, angle0_max)]\n",
    "    bounds_start = [(0, None), (0, None)]\n",
    "    #bounds_start = [(x_start_min, x_start_max), (y_start_min, y_start_max)]  # 2x pairs\n",
    "    bounds_angles = [(min_el, max_el) for min_el, max_el in zip(angles_min, angles_max)] # (n-2)x pairs\n",
    "    bounds_steps = [(min_el, max_el) for min_el, max_el in zip(step_lengths_min, step_lengths_max)] # (n-1)x pairs\n",
    "    \n",
    "    initial_guess =  np.concatenate([step_lengths_ini, angles_ini, angle0_ini, initial_start])# (2*n+2)x\n",
    "    bounds = bounds_steps + bounds_angles + bounds_angle0 + bounds_start # (2*n+2)x pairs\n",
    "    \n",
    "    #print(initial_guess)\n",
    "    #print(bounds)\n",
    "    \n",
    "    # :num_elements-1 => step_lengths (n-1)\n",
    "    # num_elements-1:2*num_elements-3 => angle_changes (n-2)\n",
    "    # 2*num_elements-3 => angle0 (1) \n",
    "    # 2*num_elements-2 => x0 (1)\n",
    "    # 2*num_elements-1 => y0 (1)\n",
    "        \n",
    "    ###############################################################################\n",
    "    #########################  SNAPPING POINTS   ##################################\n",
    "    snap_points = []\n",
    "    \n",
    "    for i_step, _ in enumerate(motion_x):\n",
    "        _distances = ((fs_points[:, 0] - motion_x[i_step])**2 + (fs_points[:, 1] - motion_y[i_step])**2)**0.5\n",
    "        _snap_points = fs_points[_distances <= snap_distance]\n",
    "        \n",
    "        if len(_snap_points) > 0:\n",
    "            snap_points.append(_snap_points)\n",
    "                \n",
    "    snap_points = np.concatenate(snap_points)\n",
    "    snap_points = np.array(list(set(map(tuple, snap_points))))\n",
    "    \n",
    "    #print(\"snap_distance\", snap_distance)\n",
    "    #print(\"snap_points.shape\", snap_points.shape)\n",
    "    ##################################################################################\n",
    "    ######################### OPTIMIZE ###############################################\n",
    "    if len(snap_points) > 0:  # some points are within snapping range\n",
    "               \n",
    "        def loss(params):   \n",
    "            #print(params.shape)\n",
    "            _d_as = np.cumsum(params[num_elements-1:2*num_elements-3])\n",
    "            _as = np.hstack([params[2*num_elements-3],\n",
    "                             params[2*num_elements-3] + _d_as])\n",
    "            \n",
    "            _d_xs = np.cumsum(params[:num_elements-1] * np.cos(_as))\n",
    "            _d_ys = np.cumsum(params[:num_elements-1] * np.sin(_as))\n",
    "\n",
    "            _xs = np.hstack([params[2*num_elements-2]/SCALE_XY0, \n",
    "                             params[2*num_elements-2]/SCALE_XY0 + _d_xs])\n",
    "            _ys = np.hstack([params[2*num_elements-1]/SCALE_XY0, \n",
    "                             params[2*num_elements-1]/SCALE_XY0 + _d_ys])\n",
    "            #print(\"_xs.shape\", _xs.shape)\n",
    "            distances2 = (snap_points[:, 0].reshape(-1,1) - _xs.reshape(1,-1))**2 + (snap_points[:, 1].reshape(-1,1) - _ys.reshape(1,-1))**2\n",
    "            _loss = np.sum(np.amin(distances2, axis=0))\n",
    "\n",
    "            return _loss/num_elements\n",
    "\n",
    "        results = minimize(fun=loss,\n",
    "                           x0=initial_guess,\n",
    "                           bounds=bounds,\n",
    "                           options={'maxcor': 30, 'ftol': 1e-08, 'gtol': 1e-07, 'maxfun': 20000, 'maxiter': 20000, 'maxls': 30},\n",
    "                           method=\"L-BFGS-B\")\n",
    "                              \n",
    "        params = results.x\n",
    "\n",
    "        _d_as = np.cumsum(params[num_elements-1:2*num_elements-3])\n",
    "        _as = np.hstack([params[2*num_elements-3],\n",
    "                         params[2*num_elements-3] + _d_as])\n",
    "\n",
    "        _d_xs = np.cumsum(params[:num_elements-1] * np.cos(_as))\n",
    "        _d_ys = np.cumsum(params[:num_elements-1] * np.sin(_as))\n",
    "\n",
    "        _xs = np.hstack([params[2*num_elements-2]/SCALE_XY0, \n",
    "                         params[2*num_elements-2]/SCALE_XY0 + _d_xs])\n",
    "        _ys = np.hstack([params[2*num_elements-1]/SCALE_XY0, \n",
    "                         params[2*num_elements-1]/SCALE_XY0 + _d_ys])\n",
    "\n",
    "        #print(\"done fitting\")\n",
    "        if is_reversed:\n",
    "            return _xs[::-1], _ys[::-1], results.fun, results.success, results.message\n",
    "        else:\n",
    "            return _xs, _ys, results.fun, results.success, results.message\n",
    "    else:\n",
    "        return motion_x, motion_y, -1, False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91bf313e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T_CUT = 200000  # max segement duration in mseconds\n",
    "\n",
    "def optimize_trajectory_fs_v4(motion_x_full, motion_y_full, motion_t_full,\n",
    "                              fs_points,\n",
    "                              leaked_record,\n",
    "                              snap_range,  # +-snap/start_point shift range in fraction of x/y bounding box (max-min)\n",
    "                              step_range, angle_range,\n",
    "                              angle0_range):  # +-step_range in fraction of step length; +-angle_range in fraction of Pi\n",
    "    \n",
    "    num_cuts = np.ceil((leaked_record[\"end_t\"] - leaked_record[\"start_t\"])/T_CUT).astype(int)\n",
    "    num_elements_full = motion_t_full.shape[0]\n",
    "    \n",
    "    sum_xs = []\n",
    "    sum_ys = []\n",
    "    #print(\"cuts:\", num_cuts)\n",
    "    for i_cut in range(num_cuts):\n",
    "        # prepare data for a given segement\n",
    "        ind_s = i_cut*(num_elements_full//num_cuts)\n",
    "        ind_e = (num_elements_full//num_cuts)*(i_cut+1 if i_cut < num_cuts-1 else i_cut+2)  # large end index if last segment\n",
    "        \n",
    "        motion_x = motion_x_full[ind_s:ind_e]\n",
    "        motion_y = motion_y_full[ind_s:ind_e] \n",
    "        motion_t = motion_t_full[ind_s:ind_e]\n",
    "        \n",
    "        #print(f\"segment/full: {motion_x.shape}/{motion_x_full.shape}\")\n",
    "        ###################################################################\n",
    "        num_elements = motion_t.shape[0]\n",
    "        trajectory_extent = ((motion_x.max() - motion_x.min()) ** 2 + (motion_y.max() - motion_y.min()) ** 2) ** 0.5\n",
    "        trajectory_extent_x = abs(motion_x.max() - motion_x.min())\n",
    "        trajectory_extent_y = abs(motion_y.max() - motion_y.min())\n",
    "\n",
    "        snap_distance = snap_range * trajectory_extent\n",
    "        snap_distance_x = snap_range * trajectory_extent_x\n",
    "        snap_distance_y = snap_range * trajectory_extent_y\n",
    "\n",
    "        time_deltas = np.abs(np.diff(motion_t))  # size \"n-1\"\n",
    "        motion_dx = np.diff(motion_x)\n",
    "        motion_dy = np.diff(motion_y)\n",
    "\n",
    "        motion_steps_lengths_input = (motion_dx ** 2 + motion_dy ** 2) ** 0.5\n",
    "        motion_angles_input = np.arctan2(motion_dy, motion_dx)\n",
    "        # print(num_elements)\n",
    "        ################  INITIAL/BOUNDARIES VALUES   ##################################\n",
    "        step_lengths_ini = np.amin(np.vstack([time_deltas * V_LIMIT, motion_steps_lengths_input]), axis=0)\n",
    "        angles_ini = motion_angles_input\n",
    "\n",
    "        step_lengths_min = np.amin(np.vstack([time_deltas * V_LIMIT * 0.99, motion_steps_lengths_input * (1 - step_range)]),\n",
    "                                   axis=0)\n",
    "        step_lengths_max = np.amin(np.vstack([time_deltas * V_LIMIT * 1.00, motion_steps_lengths_input * (1 + step_range)]),\n",
    "                                   axis=0)  # not more than maximum speed nor allowed variation of step length\n",
    "        angles_min = motion_angles_input - np.pi * angle_range\n",
    "        angles_max = motion_angles_input + np.pi * angle_range\n",
    "\n",
    "        angle0_ini = np.array([0.0])\n",
    "\n",
    "        initial_start = np.array([motion_x[0], motion_y[0]])\n",
    "\n",
    "        x_start_min = initial_start[0] - snap_distance_x\n",
    "        x_start_max = initial_start[0] + snap_distance_x\n",
    "        y_start_min = initial_start[1] - snap_distance_y\n",
    "        y_start_max = initial_start[1] + snap_distance_y\n",
    "\n",
    "        ################ RESCALE XY0/INI-BOUNDS  ####################\n",
    "        initial_start *= SCALE_XY0\n",
    "        x_start_min *= SCALE_XY0\n",
    "        x_start_max *= SCALE_XY0\n",
    "        y_start_min *= SCALE_XY0\n",
    "        y_start_max *= SCALE_XY0\n",
    "        #################################################################\n",
    "        bounds_angle0 = [(-angle0_range * np.pi, angle0_range * np.pi)]\n",
    "        bounds_start = [(0, None), (0, None)]\n",
    "        #bounds_start = [(x_start_min, x_start_max), (y_start_min, y_start_max)]  # 2x pairs\n",
    "        bounds_steps = [(min_el, max_el) for min_el, max_el in zip(step_lengths_min, step_lengths_max)]  # (n-1)x pairs\n",
    "        bounds_angles = [(min_el, max_el) for min_el, max_el in zip(angles_min, angles_max)]  # (n-1)x pairs\n",
    "\n",
    "        initial_guess = np.concatenate([step_lengths_ini, angles_ini, angle0_ini, initial_start])  # (2*n+2)x\n",
    "        bounds = bounds_steps + bounds_angles + bounds_angle0 + bounds_start  # (2*n+2)x pairs\n",
    "        ###############################################################################\n",
    "        #########################  SNAPPING POINTS   ##################################\n",
    "        snap_points = []\n",
    "\n",
    "        for i_step, _ in enumerate(motion_x):\n",
    "            _distances = ((fs_points[:, 0] - motion_x[i_step]) ** 2 + (fs_points[:, 1] - motion_y[i_step]) ** 2) ** 0.5\n",
    "            _snap_points = fs_points[_distances <= snap_distance]\n",
    "\n",
    "            if len(_snap_points) > 0:\n",
    "                snap_points.append(_snap_points)\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"snap_distance\", snap_distance)\n",
    "        # print(\"snap_points.shape\", snap_points.shape)\n",
    "        ##################################################################################\n",
    "        ######################### OPTIMIZE ###############################################\n",
    "        if len(snap_points) > 0:  # some points are within snapping range\n",
    "            \n",
    "            snap_points = np.concatenate(snap_points)\n",
    "            snap_points = np.array(list(set(map(tuple, snap_points))))\n",
    "\n",
    "            def loss(params):\n",
    "                # print(params.shape)\n",
    "                _d_xs = np.cumsum(params[: num_elements - 1] * np.cos(\n",
    "                    params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "                _d_ys = np.cumsum(params[: num_elements - 1] * np.sin(\n",
    "                    params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "                _xs = np.hstack([params[2 * num_elements - 1] / SCALE_XY0,\n",
    "                                 params[2 * num_elements - 1] / SCALE_XY0 + _d_xs])\n",
    "                _ys = np.hstack([params[2 * num_elements] / SCALE_XY0,\n",
    "                                 params[2 * num_elements] / SCALE_XY0 + _d_ys])\n",
    "\n",
    "                distances2 = (snap_points[:, 0].reshape(-1, 1) - _xs.reshape(1, -1)) ** 2 + (\n",
    "                            snap_points[:, 1].reshape(-1, 1) - _ys.reshape(1, -1)) ** 2\n",
    "                _loss = np.sum(np.amin(distances2, axis=0))  # near-field\n",
    "\n",
    "                return _loss / num_elements\n",
    "\n",
    "            results = minimize(fun=loss,\n",
    "                               x0=initial_guess,\n",
    "                               bounds=bounds,\n",
    "                               options={'maxcor': 30, 'ftol': 1e-08, 'gtol': 1e-07, 'maxfun': 20000, 'maxiter': 20000,\n",
    "                                        'maxls': 30},\n",
    "                               method=\"L-BFGS-B\")\n",
    "\n",
    "            params = results.x\n",
    "\n",
    "            _d_xs = np.cumsum(params[: num_elements - 1] * np.cos(\n",
    "                params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "            _d_ys = np.cumsum(params[: num_elements - 1] * np.sin(\n",
    "                params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "            _xs = np.hstack([params[2 * num_elements - 1] / SCALE_XY0,\n",
    "                             params[2 * num_elements - 1] / SCALE_XY0 + _d_xs])\n",
    "            _ys = np.hstack([params[2 * num_elements] / SCALE_XY0,\n",
    "                             params[2 * num_elements] / SCALE_XY0 + _d_ys])\n",
    "\n",
    "            sum_xs.append(_xs)\n",
    "            sum_ys.append(_ys)\n",
    "        else:\n",
    "            sum_xs.append(motion_x)\n",
    "            sum_ys.append(motion_y)\n",
    "\n",
    "    return np.concatenate(sum_xs), np.concatenate(sum_ys), -1, False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff909e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "def snap2fs(predicted_data, fs_data, leaked_data, snap_range, step_range, angle_range, angle0_range):\n",
    "\n",
    "    snapped_data = {}\n",
    "    site_ids = [\"5d2709d403f801723c32bd39\"]\n",
    "    # long traj=> [\"5d27096c03f801723c31e5e0\", \"5d2709b303f801723c327472\", \"5d2709c303f801723c3299ee\", \n",
    "                 # \"5d2709d403f801723c32bd39\", \"5da138274db8ce0c98bbd3d2\", \"5da1382d4db8ce0c98bbe92e\", \n",
    "                 # \"5da138764db8ce0c98bcaa46\", \"5da958dd46f8266d0737457b\", \"5dbc1d84c1eb61796cf7c010\"]\n",
    "    \n",
    "    #site_ids = [\"5d27096c03f801723c31e5e0\", \"5d2709b303f801723c327472\", \"5d2709c303f801723c3299ee\",\n",
    "    #            \"5d2709d403f801723c32bd39\", \"5da138274db8ce0c98bbd3d2\", \"5da1382d4db8ce0c98bbe92e\",\n",
    "     #           \"5da138764db8ce0c98bcaa46\", \"5da958dd46f8266d0737457b\", \"5dbc1d84c1eb61796cf7c010\"]\n",
    "    #site_ids = [\"5dc8cea7659e181adb076a3f\"]#, \"5da138754db8ce0c98bca82f\", \"5d2709e003f801723c32d896\", \"5da138364db8ce0c98bc00f1\"]\n",
    "    #site_ids = predicted_data.keys()\n",
    "    for site_id in tqdm(site_ids):#\n",
    "        \n",
    "        #print(site_id)\n",
    "        snapped_data[site_id] = {}\n",
    "        for trace_id in tqdm(predicted_data[site_id]):  # over traces\n",
    "            #print(trace_id)\n",
    "            predicted_record = predicted_data[site_id][trace_id][[\"time\", \"x\", \"y\"]].copy(deep=True)\n",
    "            _floor = predicted_data[site_id][trace_id][\"floor\"][0]\n",
    "            fs_record = fs_data[site_id][_floor]\n",
    "            leaked_record = leaked_data[site_id][trace_id]\n",
    "            \n",
    "            _x, _y, _fun, _isOk, _exit_msg = optimize_trajectory_fs_v4(predicted_record.x.to_numpy(),\n",
    "                                                                       predicted_record.y.to_numpy(),\n",
    "                                                                       predicted_record.time.to_numpy(),\n",
    "                                                                       fs_record,\n",
    "                                                                       leaked_record,\n",
    "                                                                       snap_range, step_range, angle_range, angle0_range)\n",
    "            #print(f\"{_exit_msg}: {_isOk}\")\n",
    "            #if not _isOk:\n",
    "             #   print(f\"Failed Site/TraceIds: {site_id}/{trace_id}\")\n",
    "            \n",
    "            snapped_data[site_id][trace_id] = pd.DataFrame(_x, columns=[\"x\"])\n",
    "            snapped_data[site_id][trace_id][\"y\"] = _y\n",
    "            snapped_data[site_id][trace_id][\"floor\"] = _floor\n",
    "            snapped_data[site_id][trace_id][\"time\"] = predicted_record.time.to_numpy()\n",
    "            \n",
    "            #print(_fun)\n",
    "            #break\n",
    "        break\n",
    "        \n",
    "    return snapped_data\n",
    "\n",
    "###############    MULTIPROCESSING VERSION     #################################################################\n",
    "def snap2fs_multi(predicted_data, fs_data, leaked_data, snap_range, step_range, angle_range, angle0_range):\n",
    "\n",
    "    #site_ids = [\"5da1389e4db8ce0c98bd0547\", \"5d27075f03f801723c2e360f\"]#, \"5d2709b303f801723c327472\", \"5d27097f03f801723c320d97\", \"5da138b74db8ce0c98bd4774\", \"5d2709d403f801723c32bd39\"]\n",
    "    \n",
    "    site_ids = [\"5d27096c03f801723c31e5e0\", \"5d2709b303f801723c327472\", \"5d2709c303f801723c3299ee\",\n",
    "                \"5d2709d403f801723c32bd39\", \"5da138274db8ce0c98bbd3d2\", \"5da1382d4db8ce0c98bbe92e\",\n",
    "                \"5da138764db8ce0c98bcaa46\", \"5da958dd46f8266d0737457b\", \"5dbc1d84c1eb61796cf7c010\"]\n",
    "    site_ids = predicted_data.keys()\n",
    "    \n",
    "    input_data = [(site_id, predicted_data[site_id], fs_data[site_id], leaked_data[site_id], snap_range, step_range, angle_range, angle0_range) for site_id in site_ids]\n",
    "    \n",
    "    pool = Pool(cpu_count()-3) # Create a multiprocessing Pool\n",
    "    snapped_data = pool.starmap(fs_multi, input_data)  # process input_data iterable with pool\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "        \n",
    "    res_d = snapped_data[0]\n",
    "    for i, _d in enumerate(snapped_data):\n",
    "        if i>0:\n",
    "            res_d.update(_d)\n",
    "        \n",
    "    return res_d\n",
    "\n",
    "def snap2fs_multiF(predicted_data, fs_data, leaked_data, snap_range, step_range, angle_range, angle0_range):\n",
    "\n",
    "    #site_ids = [\"5da1389e4db8ce0c98bd0547\", \"5d27075f03f801723c2e360f\"]#, \"5d2709b303f801723c327472\", \"5d27097f03f801723c320d97\", \"5da138b74db8ce0c98bd4774\", \"5d2709d403f801723c32bd39\"]\n",
    "    \n",
    "    site_ids = [\"5d27096c03f801723c31e5e0\", \"5d2709b303f801723c327472\", \"5d2709c303f801723c3299ee\",\n",
    "                \"5d2709d403f801723c32bd39\", \"5da138274db8ce0c98bbd3d2\", \"5da1382d4db8ce0c98bbe92e\",\n",
    "                \"5da138764db8ce0c98bcaa46\", \"5da958dd46f8266d0737457b\", \"5dbc1d84c1eb61796cf7c010\"]\n",
    "    site_ids = predicted_data.keys()\n",
    "    \n",
    "    input_data = [(site_id, predicted_data[site_id], fs_data[site_id], leaked_data[site_id], snap_range, step_range, angle_range, angle0_range) for site_id in site_ids]\n",
    "    \n",
    "    pool = Pool(cpu_count()-3) # Create a multiprocessing Pool\n",
    "    snapped_data = pool.starmap(fs_multiF, input_data)  # process input_data iterable with pool\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "        \n",
    "    res_d = snapped_data[0]\n",
    "    for i, _d in enumerate(snapped_data):\n",
    "        if i>0:\n",
    "            res_d.update(_d)\n",
    "        \n",
    "    return res_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f854724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6615083fa07142b29b3ec39ec93a6954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"blend_x7_0305\"\n",
    "\n",
    "predicted_data = pickle.load(open(f\"./submit/fit_data/post/{model_name}_snapped2motion2s.pkl\", \"rb\")) #\n",
    "fs_data = pickle.load(open(\"./data_out/freespace_1m_siteid_floorid.pkl\", \"rb\"))\n",
    "leaked_data = pickle.load(open(\"./data_out/leaked/leaked_siteid_traceid_all_delays.pkl\", \"rb\")) \n",
    "\n",
    "snapped2fs = snap2fs_multi(predicted_data, fs_data, leaked_data, snap_range=0.2, step_range=0.4, angle_range=0.2, angle0_range=0.2)\n",
    "plot_predictions_multi(model_name, snapped2fs, sufix=f\"snap2motion2s-fsV2nlB-02-04-02-02\")\n",
    "with open(f\"./submit/fit_data/post/{model_name}_snapped2motion2s-fsV2nlB-02-04-02-02.pkl\", \"wb\") as f:\n",
    "    pickle.dump(snapped2fs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf07b7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82838ec6a7cb41639fb111cef0438eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_data = pickle.load(open(f\"./submit/fit_data/post/{model_name}_snapped2motion4s.pkl\", \"rb\")) #\n",
    "snapped2fs = snap2fs_multiF(predicted_data, fs_data, leaked_data, snap_range=0.2, step_range=0.4, angle_range=0.2, angle0_range=0.2)\n",
    "plot_predictions_multi(model_name, snapped2fs, sufix=f\"snap2motion4s-fsV2nlF-02-04-02-02\")\n",
    "with open(f\"./submit/fit_data/post/{model_name}_snapped2motion4s-fsV2nlF-02-04-02-02.pkl\", \"wb\") as f:\n",
    "    pickle.dump(snapped2fs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0098b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14dca6e2adb44715be13fed0e271ea6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_data = pickle.load(open(f\"./submit/fit_data/post/{model_name}_snapped2motion3s.pkl\", \"rb\")) #\n",
    "snapped2fs = snap2fs_multiF(predicted_data, fs_data, leaked_data, snap_range=0.2, step_range=0.4, angle_range=0.2, angle0_range=0.2)\n",
    "plot_predictions_multi(model_name, snapped2fs, sufix=f\"snap2motion3s-fsV2nlF-02-04-02-02\")\n",
    "with open(f\"./submit/fit_data/post/{model_name}_snapped2motion3s-fsV2nlF-02-04-02-02.pkl\", \"wb\") as f:\n",
    "    pickle.dump(snapped2fs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ae4e127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd32c483dd341c8808e0def3208bf15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_data = pickle.load(open(f\"./submit/fit_data/post/{model_name}_snapped2motion2s.pkl\", \"rb\")) #\n",
    "snapped2fs = snap2fs_multiF(predicted_data, fs_data, leaked_data, snap_range=0.2, step_range=0.4, angle_range=0.2, angle0_range=0.2)\n",
    "plot_predictions_multi(model_name, snapped2fs, sufix=f\"snap2motion2s-fsV2nlF-02-04-02-02\")\n",
    "with open(f\"./submit/fit_data/post/{model_name}_snapped2motion2s-fsV2nlF-02-04-02-02.pkl\", \"wb\") as f:\n",
    "    pickle.dump(snapped2fs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf77b8e",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f02c6d",
   "metadata": {},
   "source": [
    "Final snapping to grid/waypoints -> plotting -> submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e688f871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02a8e32f14a4c9696a25d5e463130a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"blend_x7_0305\"\n",
    "fork_name = \"2motion4-fsV2nlF-02-04-02-01\"\n",
    "\n",
    "predicted_data_post = pickle.load(open(f\"./submit/fit_data/post/{model_name}_snapped{fork_name}.pkl\", \"rb\")) #\n",
    "grid_data = pickle.load(open(\"./data_out/waypoints_siteid_floorid.pkl\", \"rb\"))#pickle.load(open(\"./data_out/freespace_1m_siteid_floorid.pkl\", \"rb\"))  # pickle.load(open(\"./data_out/waypoints_siteid_floorid.pkl\", \"rb\"))\n",
    "timestamps = pickle.load(open(f\"./data_out/submission_timestamps_traceid.pkl\", \"rb\"))\n",
    "aux_grid_data = pickle.load(open(\"./data_out/freespace_2m_siteid_floorid.pkl\", \"rb\"))\n",
    "\n",
    "snapped2grid = snap2grid(predicted_data_post, grid_data, aux_grid_data, timestamps, snap_range=4, aux_snap_range=50)\n",
    "\n",
    "make_submission(model_name, snapped2grid, sufix=f\"snap{fork_name}-gridWPs4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea7d94ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c22dc992a024dd5b09062295eb14944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_predictions_multi(model_name, snapped2grid, sufix=f\"snap2motion-gridWPs5\")\n",
    "plot_predictions_multi(model_name, snapped2grid, sufix=f\"snap2motion-fsV2p05-02-03-01-01-gridWPs5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da66936f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f88c87ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71af9d4913ac4390be937f120d8232f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"blend_x7_0305\"\n",
    "data_post = pickle.load(open(f\"./submit/fit_data/post/blend_x7_0305_snapped2motion4-fsV2nlF-02-04-02-01.pkl\", \"rb\")) #\n",
    "plot_predictions_multi(model_name, data_post, sufix=f\"snap2motion4-fsV2nlF-02-04-02-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "298e277e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0e395b95924e9890afb95fb8df6d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"blend_x7_0305\"\n",
    "data_post = pickle.load(open(f\"./submit/fit_data/post/blend_x7_0305_snapped2motion4-fsV2nlB-02-04-02-01.pkl\", \"rb\")) #\n",
    "plot_predictions_multi(model_name, data_post, sufix=f\"snap2motion4-fsV2nlB-02-04-02-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25177e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
