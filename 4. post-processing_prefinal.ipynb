{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca14151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from post_multi import motion_multi, fs_multi\n",
    "\n",
    "import matplotlib.path as mpltPath\n",
    "from scipy.spatial import distance\n",
    "from scipy.optimize import minimize\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from PIL import Image\n",
    "#%matplotlib inline\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from xyz10.io_f_mod import read_data_file\n",
    "from xyz10.visualize_f_mod import visualize_trajectory, save_figure_to_image\n",
    "from xyz10.compute_f_mod import compute_step_positions, split_ts_seq, correct_positions\n",
    "from xyz10.compute_f_mod import compute_steps, compute_headings, compute_stride_length, compute_step_heading, compute_rel_positions\n",
    "from xyz10.compute_f_mod import correct_positions_mod, correct_positions_mod2, compute_step_positions_mod, compute_step_positions_mod2, split_ts_seq_mod\n",
    "\n",
    "from xyz10.io_f_mod import read_data_file\n",
    "from xyz10.visualize_f_mod import visualize_trajectory, save_figure_to_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5562575",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22486f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "paths = glob.glob('./img_out/predictions/blend_postX9_1505/*/*_t*.png')\n",
    "_ = [os.remove(path) for path in paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa7ab3",
   "metadata": {},
   "source": [
    "Function: Blend, Plot_Traces, Snap2Grid, Make_Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "53241eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FLOOR_NUM_to_ID = {'5a0546857ecc773753327266': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5c3c44b80379370013e0fd2b': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5d27075f03f801723c2e360f': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'},\n",
    "                 '5d27096c03f801723c31e5e0': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6'},\n",
    "                 '5d27097f03f801723c320d97': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5d27099f03f801723c32511d': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5d2709a003f801723c3251bf': {0: '1F', 1: '2F', 2: '3F', 3: '4F'},\n",
    "                 '5d2709b303f801723c327472': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F', 3: '4F'},\n",
    "                 '5d2709bb03f801723c32852c': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5d2709c303f801723c3299ee': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F', 3: '4F', 4: '5F', 5: '6F', 6: '7F', 7: '8F', 8: '9F'},\n",
    "                 '5d2709d403f801723c32bd39': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F'},\n",
    "                 '5d2709e003f801723c32d896': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5da138274db8ce0c98bbd3d2': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                 '5da1382d4db8ce0c98bbe92e': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5da138314db8ce0c98bbf3a0': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                 '5da138364db8ce0c98bc00f1': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                 '5da1383b4db8ce0c98bc11ab': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                 '5da138754db8ce0c98bca82f': {0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5da138764db8ce0c98bcaa46': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5da1389e4db8ce0c98bd0547': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                 '5da138b74db8ce0c98bd4774': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                 '5da958dd46f8266d0737457b': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'},\n",
    "                 '5dbc1d84c1eb61796cf7c010': {-1: 'B1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7', 7: 'F8'},\n",
    "                 '5dc8cea7659e181adb076a3f': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'}}\n",
    "\n",
    "def blend_predictions(blend_folder, reference_file):\n",
    "\n",
    "    blend_paths = glob.glob(blend_folder + \"*\")\n",
    "    blend_data = []\n",
    "    for b_path in blend_paths:\n",
    "        blend_data.append(pickle.load(open(b_path, \"rb\")))\n",
    "        #break\n",
    "    reference_data = pickle.load(open(blend_folder+reference_file, \"rb\"))\n",
    "\n",
    "    compound= {}\n",
    "    for site_id in reference_data.keys():\n",
    "\n",
    "        compound[site_id] = {}\n",
    "        for trace_id in reference_data[site_id].keys():\n",
    "\n",
    "            _timestamps = reference_data[site_id][trace_id].to_numpy()[:, 3]\n",
    "            _floor = int(np.median(reference_data[site_id][trace_id].to_numpy()[:, 2]))\n",
    "            _x = []\n",
    "            _y = []\n",
    "            for data in blend_data:\n",
    "\n",
    "                predicted_record = data[site_id][trace_id].to_numpy()\n",
    "                _x.append(predicted_record[:, 0])\n",
    "                _y.append(predicted_record[:, 1])\n",
    "            \n",
    "            compound[site_id][trace_id] = pd.DataFrame({\"x\": np.median(_x, axis=0), \"y\": np.median(_y, axis=0), \"floor\": _floor, \"time\": _timestamps})\n",
    "\n",
    "            #break \n",
    "    return compound\n",
    "\n",
    "def blend_predictions_mean(blend_folder, reference_file):\n",
    "\n",
    "    blend_paths = glob.glob(blend_folder + \"*\")\n",
    "    blend_data = []\n",
    "    for b_path in blend_paths:\n",
    "        blend_data.append(pickle.load(open(b_path, \"rb\")))\n",
    "        #break\n",
    "    reference_data = pickle.load(open(blend_folder+reference_file, \"rb\"))\n",
    "\n",
    "    compound= {}\n",
    "    for site_id in reference_data.keys():\n",
    "\n",
    "        compound[site_id] = {}\n",
    "        for trace_id in reference_data[site_id].keys():\n",
    "\n",
    "            _timestamps = reference_data[site_id][trace_id].to_numpy()[:, 3]\n",
    "            _floor = int(np.median(reference_data[site_id][trace_id].to_numpy()[:, 2]))\n",
    "            _x = []\n",
    "            _y = []\n",
    "            for data in blend_data:\n",
    "\n",
    "                predicted_record = data[site_id][trace_id].to_numpy()\n",
    "                _x.append(predicted_record[:, 0])\n",
    "                _y.append(predicted_record[:, 1])\n",
    "            \n",
    "            compound[site_id][trace_id] = pd.DataFrame({\"x\": np.mean(_x, axis=0), \"y\": np.mean(_y, axis=0), \"floor\": _floor, \"time\": _timestamps})\n",
    "\n",
    "            #break \n",
    "    return compound\n",
    "\n",
    "def blend_predictions_inter(blend_folder, reference_file):\n",
    "\n",
    "    blend_paths = glob.glob(blend_folder + \"*\")\n",
    "    blend_data = []\n",
    "    for b_path in blend_paths:\n",
    "        blend_data.append(pickle.load(open(b_path, \"rb\")))\n",
    "        #break\n",
    "    reference_data = pickle.load(open(blend_folder+reference_file, \"rb\"))\n",
    "\n",
    "    compound= {}\n",
    "    for site_id in reference_data.keys():\n",
    "\n",
    "        compound[site_id] = {}\n",
    "        for trace_id in reference_data[site_id].keys():\n",
    "\n",
    "            _timestamps = reference_data[site_id][trace_id].to_numpy()[:, 3]\n",
    "            _floor = int(np.median(reference_data[site_id][trace_id].to_numpy()[:, 2]))\n",
    "            _x = []\n",
    "            _y = []\n",
    "            for data in blend_data:\n",
    "\n",
    "                predicted_record = data[site_id][trace_id].to_numpy()\n",
    "                \n",
    "                func_x = interp1d(predicted_record[:, 3], predicted_record[:, 0], kind=\"linear\", copy=False, fill_value=\"extrapolate\", assume_sorted=True)(_timestamps)\n",
    "                func_y = interp1d(predicted_record[:, 3], predicted_record[:, 1], kind=\"linear\", copy=False, fill_value=\"extrapolate\", assume_sorted=True)(_timestamps)\n",
    "                \n",
    "                _x.append(func_x)\n",
    "                _y.append(func_y)\n",
    "            \n",
    "            compound[site_id][trace_id] = pd.DataFrame({\"x\": np.median(_x, axis=0), \"y\": np.median(_y, axis=0), \"floor\": _floor, \"time\": _timestamps})\n",
    "\n",
    "            #break \n",
    "    return compound\n",
    "\n",
    "def make_submission_bkp(model_name, data, sufix=\"coarse\"):\n",
    "\n",
    "    sample_submit = pd.read_csv(\"./submit/sample_submission.csv\")\n",
    "    splits = sample_submit.site_path_timestamp.str.split(pat=\"_\", expand=True)\n",
    "    sub_data = sample_submit.copy(deep=True).join(splits)\n",
    "    sub_data.rename(columns={0:\"site\", 1:\"path\", 2:\"timestamp\"}, inplace=True)\n",
    "\n",
    "    for i in tqdm(list(sub_data.index)):\n",
    "        site_id = sub_data.site[i]\n",
    "        trace_id = sub_data.path[i]\n",
    "        timestamp = sub_data.timestamp[i]\n",
    "\n",
    "        predicted_record = data[site_id][trace_id].to_numpy()\n",
    "\n",
    "        func_x = interp1d(predicted_record[:, 3], predicted_record[:, 0], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "        func_y = interp1d(predicted_record[:, 3], predicted_record[:, 1], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "\n",
    "        sub_data.loc[i, \"x\"] = func_x(timestamp)\n",
    "        sub_data.loc[i, \"y\"] = func_y(timestamp)\n",
    "        sub_data.loc[i, \"floor\"] = int(np.median(predicted_record[:, 2]))\n",
    "        #break\n",
    "\n",
    "    _ = [sub_data.pop(col) for col in [\"site\", \"path\", \"timestamp\"]]\n",
    "\n",
    "    sub_data.to_csv(f\"./submit/{model_name}_{sufix}.csv\", index=False)\n",
    "    \n",
    "\n",
    "def make_submission(model_name, data, sufix=\"coarse\"):\n",
    "    \n",
    "    sample_submit = pd.read_csv(\"./submit/sample_submission.csv\")\n",
    "    splits = sample_submit.site_path_timestamp.str.split(pat=\"_\", expand=True)\n",
    "    sub_data = sample_submit.copy(deep=True).join(splits)\n",
    "    sub_data.rename(columns={0:\"site\", 1:\"path\", 2:\"timestamp\"}, inplace=True)\n",
    "\n",
    "    gr = sub_data.groupby(\"path\")\n",
    "    for trace_id in gr.groups:\n",
    "        timestamps = sub_data.loc[gr.groups[trace_id]].timestamp.to_list()\n",
    "\n",
    "        site_id = sub_data.loc[gr.groups[trace_id]].site.to_list()[0]\n",
    "        predicted_record = data[site_id][trace_id].to_numpy()\n",
    "\n",
    "        func_x = interp1d(predicted_record[:, 3], predicted_record[:, 0], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "        func_y = interp1d(predicted_record[:, 3], predicted_record[:, 1], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "\n",
    "        sub_data.loc[gr.groups[trace_id], \"x\"] = func_x(timestamps)\n",
    "        sub_data.loc[gr.groups[trace_id], \"y\"] = func_y(timestamps)\n",
    "        sub_data.loc[gr.groups[trace_id], \"floor\"] = predicted_record[0, 2]\n",
    "        #break\n",
    "\n",
    "    _ = [sub_data.pop(col) for col in [\"site\", \"path\", \"timestamp\"]]\n",
    "\n",
    "    sub_data.to_csv(f\"./submit/{model_name}_{sufix}.csv\", index=False)\n",
    "    \n",
    "def plot_predictions_multi(model_name, data, sufix=\"coarse\", delay_suffix=False):\n",
    "    \n",
    "    def swap_trace_floor(predicted_data):\n",
    "        swap = {}\n",
    "\n",
    "        for site_id in predicted_data.keys():\n",
    "\n",
    "            swap[site_id] = {}\n",
    "            for trace_id in predicted_data[site_id].keys():\n",
    "\n",
    "                floor_id = predicted_data[site_id][trace_id].floor[0]\n",
    "                if floor_id not in swap[site_id].keys():\n",
    "                    swap[site_id][floor_id] = {}\n",
    "                swap[site_id][floor_id][trace_id] = predicted_data[site_id][trace_id]\n",
    "\n",
    "        return swap\n",
    "\n",
    "    data = swap_trace_floor(data)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f\"./img_out/predictions/{model_name}/\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    n_s = 0\n",
    "    for site_id in tqdm(data.keys()):  # over sites \n",
    "        n_s += 1\n",
    "        #print(f\"Processing Trajectories #{n_s}: Site-{site_id} with {len(data[site_id])} traces\")\n",
    "\n",
    "        try:\n",
    "            os.makedirs(f\"./img_out/predictions/{model_name}/{site_id}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for floor_id in data[site_id]:  # over traces\n",
    "            site_path = \"./data_in/metadata/\" + site_id + \"/\"\n",
    "            \n",
    "            positions = []\n",
    "            legends = []\n",
    "            for trace_id in data[site_id][floor_id].keys():\n",
    "                positions.append(data[site_id][floor_id][trace_id].to_numpy()[:, :2])\n",
    "                \n",
    "                if delay_suffix:\n",
    "                    legends.append(f\"{trace_id}_s{int(0.001*data[site_id][floor_id][trace_id].to_numpy()[0, 4])}_e{int(0.001*data[site_id][floor_id][trace_id].to_numpy()[1, 4])}\")\n",
    "                else:\n",
    "                    legends.append(trace_id)\n",
    "\n",
    "            try:\n",
    "                floor = FLOOR_NUM_to_ID[site_id][floor_id]\n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                with open(info_path) as info_file:\n",
    "                    info_data = json.load(info_file)             \n",
    "\n",
    "                map_width = info_data[\"map_info\"][\"width\"]\n",
    "                map_height = info_data[\"map_info\"][\"height\"]\n",
    "\n",
    "                fig_steps = visualize_trajectory(trajectory=positions, is_multi = True,\n",
    "                                                 floor_plan_filename=map_path, mode=\"lines + markers\", title=f\"{site_id}_{floor}_{sufix}\", legends=legends, \n",
    "                                                 width_meter=map_width,  height_meter=map_height)\n",
    "                save_figure_to_image(fig_steps, f\"./img_out/predictions/{model_name}/{site_id}/{floor}_{sufix}.png\")\n",
    "            except:\n",
    "                print(f\"Exception: wrong floor-{floor} site-{site_id}\")\n",
    "\n",
    "        #break  # only first site_id\n",
    "        \n",
    "def snap2grid(predicted_data, grid_siteid_floorid, aux_grid_siteid_floorid, timestamps_traceid, snap_range=5, aux_snap_range=30):\n",
    "\n",
    "    def closest_point(path, point, snap_range):\n",
    "        #print(path)\n",
    "        distance = (path[:, 0] - point[0])**2 + (path[:, 1] - point[1])**2\n",
    "        \n",
    "        if distance.min() < snap_range**2:\n",
    "            idx = distance.argmin()\n",
    "            return [path[idx, 0], path[idx, 1]], True\n",
    "        else:\n",
    "            return [point[0], point[1]], False\n",
    "\n",
    "    snap2grid_data = {}\n",
    "\n",
    "    n_s= 0\n",
    "    for site_id in tqdm(predicted_data.keys()):  # over sites\n",
    "        n_s += 1\n",
    "\n",
    "        snap2grid_data[site_id] = {}\n",
    "        for trace_id in predicted_data[site_id].keys():  # over traces\n",
    "\n",
    "            trace = []  # list of points [x,y]\n",
    "            predicted_record = predicted_data[site_id][trace_id].to_numpy()\n",
    "            \n",
    "            floor_id = int(np.median(predicted_record[:, 2]))\n",
    "            grid = grid_siteid_floorid[site_id][floor_id]\n",
    "            aux_grid = aux_grid_siteid_floorid[site_id][floor_id]\n",
    "            \n",
    "            func_x = interp1d(predicted_record[:, 3], predicted_record[:, 0], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "            func_y = interp1d(predicted_record[:, 3], predicted_record[:, 1], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "\n",
    "            _x = func_x(timestamps_traceid[trace_id])\n",
    "            _y = func_y(timestamps_traceid[trace_id])\n",
    "\n",
    "            for i, _ in enumerate(_x):  # over points \n",
    "                point = [_x[i], _y[i]]\n",
    "                _closest_point, _isOk = closest_point(grid, point, snap_range)\n",
    "                if _isOk:\n",
    "                    trace.append(_closest_point)  # grid = closest step/waypoint point(slow) vs path_a0= closest contour point (fast)\n",
    "                else:\n",
    "                    _aus_close_point, _ = closest_point(aux_grid, point, aux_snap_range)\n",
    "                    trace.append(_aus_close_point)\n",
    "                \n",
    "\n",
    "            snap2grid_data[site_id][trace_id] = pd.DataFrame(trace, columns=[\"x\", \"y\"])\n",
    "            snap2grid_data[site_id][trace_id][\"floor\"] = floor_id\n",
    "            snap2grid_data[site_id][trace_id][\"time\"] = timestamps_traceid[trace_id]\n",
    "            \n",
    "    return snap2grid_data\n",
    "\n",
    "def snap2grid_full(predicted_data, grid_siteid_floorid, snap_range=5):\n",
    "    \n",
    "    \n",
    "    def sma(ar, length=1):\n",
    "\n",
    "        if not isinstance(ar, np.ndarray): ar = ar.to_numpy()\n",
    "        _length = min(length if length > 0 else 1, ar.size)  # length check\n",
    "        if _length == 1:\n",
    "            return ar\n",
    "        else:\n",
    "            _sum = np.cumsum(ar)\n",
    "            _sum[_length:] = _sum[_length:] - _sum[:-_length]\n",
    "            _sma = sma(ar[:_length - 1], _length-1)\n",
    "            return np.concatenate((_sma, _sum[_length - 1:] / _length))\n",
    "\n",
    "    def closest_point(path, point, snap_range):\n",
    "        #print(path)\n",
    "        distance = (path[:, 0] - point[0])**2 + (path[:, 1] - point[1])**2\n",
    "        \n",
    "        if distance.min() < snap_range**2:\n",
    "            idx = distance.argmin()\n",
    "            return [path[idx, 0], path[idx, 1]], True\n",
    "        else:\n",
    "            return [point[0], point[1]], False\n",
    "\n",
    "    snap2grid_data = {}\n",
    "\n",
    "    n_s= 0\n",
    "    for site_id in tqdm(predicted_data.keys()):  # over sites\n",
    "        n_s += 1\n",
    "\n",
    "        snap2grid_data[site_id] = {}\n",
    "        for trace_id in predicted_data[site_id].keys():  # over traces\n",
    "\n",
    "            trace = []  # list of points [x,y]\n",
    "            predicted_record = predicted_data[site_id][trace_id].to_numpy()\n",
    "            \n",
    "            floor_id = int(np.median(predicted_record[:, 2]))\n",
    "            grid = grid_siteid_floorid[site_id][floor_id]\n",
    "\n",
    "            for i, _ in enumerate(predicted_record):  # over points \n",
    "                point = [predicted_record[i, 0], predicted_record[i, 1]]\n",
    "                _closest_point, _isOk = closest_point(grid, point, snap_range)\n",
    "                if _isOk:\n",
    "                    trace.append(_closest_point)  # grid = closest step/waypoint point(slow) vs path_a0= closest contour point (fast)\n",
    "                else:\n",
    "                    trace.append(point)\n",
    "            trace = np.array(trace)\n",
    "            trace[:, 0] = sma(trace[:, 0], 4)\n",
    "            trace[:, 1] = sma(trace[:, 1], 4)\n",
    "\n",
    "            snap2grid_data[site_id][trace_id] = pd.DataFrame(trace, columns=[\"x\", \"y\"])\n",
    "            snap2grid_data[site_id][trace_id][\"floor\"] = floor_id\n",
    "            snap2grid_data[site_id][trace_id][\"time\"] = predicted_record[:, 3]\n",
    "            \n",
    "    return snap2grid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43788f1",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aca9bb",
   "metadata": {},
   "source": [
    "1) Combine/Plot/Submit blended raw models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c5234f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blend_suffix = \"_postX6_1705\"\n",
    "model_name = f\"blend{blend_suffix}\"\n",
    "\n",
    "blend_data_median = blend_predictions(\"./submit/fit_data/post/blend_x6_1705/\", \"blend_x7_0305_snapped2motion4s-fsV2nlB-02-04-02-02.pkl\")\n",
    "blend_data_mean = blend_predictions_mean(\"./submit/fit_data/post/blend_x6_1705/\", \"blend_x7_0305_snapped2motion4s-fsV2nlB-02-04-02-02.pkl\")\n",
    "\n",
    "with open(f\"./submit/fit_data/post/{model_name}_median.pkl\", \"wb\") as f:\n",
    "    pickle.dump(blend_data_median, f)\n",
    "    \n",
    "with open(f\"./submit/fit_data/post/{model_name}_mean.pkl\", \"wb\") as f:\n",
    "    pickle.dump(blend_data_mean, f)\n",
    "\n",
    "plot_predictions_multi(f\"{model_name}\", blend_data_median, sufix=f\"median\")\n",
    "plot_predictions_multi(f\"{model_name}\", blend_data_mean, sufix=f\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6dd13932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869478a89e324590930cbfd9ce681952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d08b6ebe1774c7189da6f484d1875f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_data_post_median = pickle.load(open(f\"./submit/fit_data/post/{model_name}_median.pkl\", \"rb\")) #\n",
    "predicted_data_post_mean = pickle.load(open(f\"./submit/fit_data/post/{model_name}_mean.pkl\", \"rb\")) #\n",
    "\n",
    "grid_data = pickle.load(open(\"./data_out/waypoints_siteid_floorid.pkl\", \"rb\"))\n",
    "aux_grid_data = pickle.load(open(\"./data_out/freespace_2m_siteid_floorid.pkl\", \"rb\"))\n",
    "timestamps = pickle.load(open(f\"./data_out/submission_timestamps_traceid.pkl\", \"rb\"))\n",
    "\n",
    "snapped2grid_median = snap2grid(predicted_data_post_median, grid_data, aux_grid_data, timestamps, snap_range=3, aux_snap_range=50)\n",
    "snapped2grid_mean = snap2grid(predicted_data_post_mean, grid_data, aux_grid_data, timestamps, snap_range=3, aux_snap_range=50)\n",
    "make_submission(f\"{model_name}_median\", snapped2grid_median, sufix=f\"gridWPs3-gridFS2m50\")\n",
    "make_submission(f\"{model_name}_mean\", snapped2grid_mean, sufix=f\"gridWPs3-gridFS2m50\")\n",
    "plot_predictions_multi(f\"{model_name}\", snapped2grid_median, sufix=f\"median_gridWPs3-gridFS2m50\")\n",
    "plot_predictions_multi(f\"{model_name}\", snapped2grid_mean, sufix=f\"mean_gridWPs3-gridFS2m50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc102dd",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ceec7d",
   "metadata": {},
   "source": [
    "2) Snap2Motion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac5389",
   "metadata": {},
   "source": [
    "- Snap to grid 05m vs No snap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fc76bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbeb891488684f569687bc59a8669875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_data_post = pickle.load(open(f\"./submit/fit_data/post/blend_postX9_1505.pkl\", \"rb\")) #\n",
    "grid_data = pickle.load(open(\"./data_out/freespace_05m_siteid_floorid.pkl\", \"rb\"))\n",
    "snapped2grid = snap2grid_full(predicted_data_post, grid_data, snap_range=400)\n",
    "with open(f\"./submit/fit_data/post/blend_postX9_1505G.pkl\", \"wb\") as f:\n",
    "    pickle.dump(snapped2grid, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08f4c7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed709347a0c8453eab5945b3b8182572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions_multi(f\"blend_postX9_1505\", snapped2grid, sufix=f\"coarseG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c44bec",
   "metadata": {},
   "source": [
    "- Generate waypoints (5-20s segments) + start/end points -> Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc47641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pickle.load(open(f\"./submit/fit_data/post/blend_postX9_1505G.pkl\", \"rb\")) #\n",
    "timestamps = pickle.load(open(f\"./data_out/submission_timestamps_traceid.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7e8a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cuts = [8000, 10000, 12000, 14000, 16000, 18000, 20000]\n",
    "\n",
    "data_tcut = {}\n",
    "\n",
    "for t_cut in t_cuts:\n",
    "\n",
    "    data_tcut[t_cut] = {}\n",
    "    for site_id in full_data:\n",
    "\n",
    "        data_tcut[t_cut][site_id] = {}\n",
    "        for trace_id in full_data[site_id]:\n",
    "\n",
    "            record = full_data[site_id][trace_id]\n",
    "            num_elements = len(record)\n",
    "            num_cuts = np.ceil((record.iloc[-1].time - record.iloc[0].time)/t_cut).astype(int)\n",
    "\n",
    "            inds_s = []\n",
    "            for i_cut in range(num_cuts-1):\n",
    "                inds_s.append((i_cut+1)*(num_elements//num_cuts))\n",
    "\n",
    "            inds_s = [0] + inds_s + [num_elements-1]\n",
    "            inds = sorted(set(inds_s))\n",
    "\n",
    "            data_tcut[t_cut][site_id][trace_id] = record.iloc[inds].reset_index(drop=True)\n",
    "\n",
    "            #print(inds_s, t_cut, num_cuts)\n",
    "            #display(record)\n",
    "            #display(full_data_tcut[site_id][trace_id][t_cut])\n",
    "\n",
    "                #break\n",
    "            #break\n",
    "        #break\n",
    "\n",
    "with open(f\"./submit/fit_data/post/blend_postX9_1505G_tcuts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_tcut, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d35e158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a06b43ca4de4189865b94b0d4f9d2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221ea29c8d98460fb33a29bbf88ec74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fca735150ae47b5a3accbf2e13f191d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acf817f7e7049bfb085191c2524b3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b0ac9619174c818554d03d4e9544f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_tcut = pickle.load(open(f\"./submit/fit_data/post/blend_postX9_1505G_tcuts.pkl\", \"rb\")) #\n",
    "for t_cut in data_tcut:\n",
    "    plot_predictions_multi(f\"blend_postX9_1505\", data_tcut[t_cut], sufix=f\"coarseG_t{t_cut//1000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a27773",
   "metadata": {},
   "source": [
    "- Calculate Steps (10-20-50-75-100s segments) -> Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec84111",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pickle.load(open(f\"./data_out/test_data.pkl\", \"rb\")) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361decdd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3af98d584dd43b79704e2bd7b9d283f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651aba71f1284c62b5da990837519fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3bc2aa1c5947db97ac448854775f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a14e92f2e394de3809fb72fed929bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ba22a2f1f24956991365130e3c4119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e63ccbcf2e54c0eaafb285a306b7863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e1692a753b477fae8d1486097da210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3450b8f67af348f7842d1ce21137cd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de41466857b247768611688cc38a838e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78ea0903e1744b7a9146f9bdba339c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1140556d4a024e24bee8f050960165dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a13502c81f44c5a321086979e0a859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a92856a01424123ac81c31882b92177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5e35348f5e40e89d57ed6e9d81a2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02376f69718e4870be0ade5982b26c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cb5f1def7f4650903f076fcf91b331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c793df28d54c3884493259e35826b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f487f413734942caaac9556be777ef32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1c45bb709f4807a16850618dee5474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7aa01d4b43d49a39f309e6a2bc63e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500cb7bae36e4a0e9a29d30567764456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6c62e1988241dd9b7d58e14ecf3ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736f46677ce04e07a0b0958df86cbadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9890d8e455e54999a289bb4d5f2f28d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdbeddb452f4b77965a73f5e04269f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rel_positions_all = {}\n",
    "\n",
    "for site_id in tqdm(test_data):  # over sites\n",
    "\n",
    "    rel_positions_all[site_id] = {}\n",
    "    for trace_id in test_data[site_id]:  # over traces\n",
    "\n",
    "        accel_record = test_data[site_id][trace_id].acce\n",
    "        rotate_record = test_data[site_id][trace_id].rotate\n",
    "\n",
    "        step_timestamps, step_indexs, step_acce_max_mins = compute_steps(accel_record[[\"time\", \"x_axis\", \"y_axis\", \"z_axis\"]].to_numpy())\n",
    "        headings = compute_headings(rotate_record[[\"time\", \"x_axis\", \"y_axis\", \"z_axis\"]].to_numpy())\n",
    "        stride_lengths = compute_stride_length(step_acce_max_mins)\n",
    "        step_headings = compute_step_heading(step_timestamps, headings)\n",
    "        rel_positions = compute_rel_positions(stride_lengths, step_headings)\n",
    "        \n",
    "        rel_positions_all[site_id][trace_id] = rel_positions\n",
    "        \n",
    "with open(f\"./data_out/rel_positions_siteid_traceid.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rel_positions_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "779bcc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_positions_all = pickle.load(open(f\"./data_out/rel_positions_siteid_traceid.pkl\", \"rb\")) #\n",
    "data_tcut = pickle.load(open(f\"./submit/fit_data/post/blend_postX9_1505G_tcuts.pkl\", \"rb\")) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbae3b71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab1a3e13618476ba02f11b60926d609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps_tcut = {}\n",
    "for t_cut in tqdm(data_tcut):\n",
    "\n",
    "    steps_tcut[t_cut] = {}\n",
    "    for site_id in data_tcut[t_cut]:  # over sites\n",
    "\n",
    "        steps_tcut[t_cut][site_id] = {}\n",
    "        for trace_id in data_tcut[t_cut][site_id]:  # over traces\n",
    "\n",
    "            waypoints = data_tcut[t_cut][site_id][trace_id]\n",
    "            rel_positions = rel_positions_all[site_id][trace_id]\n",
    "            #rel_positions_list = split_ts_seq_mod(rel_positions, waypoints[[\"time\", \"x\", \"y\"]].to_numpy()[:, 0])\n",
    "            step_positions = pd.DataFrame(correct_positions_mod2(rel_positions, waypoints[[\"time\", \"x\", \"y\"]].to_numpy()), columns=[\"time\", \"x\", \"y\"])\n",
    "            step_positions[\"floor\"] = waypoints[\"floor\"][0]\n",
    "            step_positions = step_positions[[\"x\",\"y\",\"floor\",\"time\"]]\n",
    "            \n",
    "            steps_tcut[t_cut][site_id][trace_id] = step_positions\n",
    "            \n",
    "            #break\n",
    "        #break\n",
    "    #break\n",
    "\n",
    "with open(f\"./submit/fit_data/post/blend_postX9_1505G_tcuts_steps.pkl\", \"wb\") as f:\n",
    "    pickle.dump(steps_tcut, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59edc47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dadbc6411a64cfe97b6e95dd2eb6d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fa967c89f54e2abad046f54cefb0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc8578d6732407d8ae35a836f86c1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2662dde817154aec8e5fd1f02f391996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a58352412a4ee699189205874f9252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps_tcut = pickle.load(open(f\"./submit/fit_data/post/blend_postX9_1505G_tcuts_steps.pkl\", \"rb\")) #\n",
    "for t_cut in steps_tcut:\n",
    "    plot_predictions_multi(f\"blend_postX9_1505\", steps_tcut[t_cut], sufix=f\"coarseG_t{t_cut//1000}_steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693897a9",
   "metadata": {},
   "source": [
    "- Select/Blend the best for snap to FS -> Snap2Grid -> Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e6af761",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_tcut = pickle.load(open(f\"./submit/fit_data/post/blend_postX9_1505G_tcuts_steps.pkl\", \"rb\")) #\n",
    "for t_cut in steps_tcut:\n",
    "    with open(f\"./submit/fit_data/post/blend_tcuts/{t_cut//1000}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(steps_tcut[t_cut], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "976d4565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blend_suffix = \"_tcutsX7_1605\"\n",
    "\n",
    "blend_data = blend_predictions_inter(\"./submit/fit_data/post/blend_tcuts/\", \"10.pkl\")\n",
    "\n",
    "with open(f\"./submit/fit_data/post/blend{blend_suffix}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(blend_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "706a6632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da8e1a34d4e40e7a78b7e2fb595f90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "blend_data = pickle.load(open(f\"./submit/fit_data/post/blend{blend_suffix}.pkl\", \"rb\"))\n",
    "plot_predictions_multi(f\"blend{blend_suffix}\", blend_data, sufix=f\"coarse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dfab175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07a5951546f45a390880547b238f781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6acba21b05246098f9872b431b2977a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"blend_tcutsX7_1605\"\n",
    "predicted_data_post = pickle.load(open(f\"./submit/fit_data/post/{model_name}.pkl\", \"rb\")) #\n",
    "\n",
    "grid_data = pickle.load(open(\"./data_out/waypoints_siteid_floorid.pkl\", \"rb\"))\n",
    "aux_grid_data = pickle.load(open(\"./data_out/freespace_2m_siteid_floorid.pkl\", \"rb\"))\n",
    "timestamps = pickle.load(open(f\"./data_out/submission_timestamps_traceid.pkl\", \"rb\"))\n",
    "\n",
    "snapped2grid = snap2grid(predicted_data_post, grid_data, aux_grid_data, timestamps, snap_range=3, aux_snap_range=50)\n",
    "plot_predictions_multi(f\"{model_name}\", snapped2grid, sufix=f\"gridWPs3-gridFS2m50\")\n",
    "make_submission(f\"{model_name}\", snapped2grid, sufix=f\"gridWPs3-gridFS2m50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363e765",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be60091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "190509eb",
   "metadata": {},
   "source": [
    "3. Snap2FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46eeb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_LIMIT = 0.0015  # m/msec\n",
    "INVALID_RANGE = 0.5 # invalidity distance in fraction of trajectory extent\n",
    "SCALE_XY0 = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa60125",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_trajectory_fs_v2(motion_x, motion_y, motion_t,\n",
    "                              fs_points,\n",
    "                              leaked_record,\n",
    "                              snap_range,  # +-snap/start_point shift range in fraction of x/y bounding box (max-min)\n",
    "                              step_range, angle_range,\n",
    "                              angle0_range):  # +-step_range in fraction of step length; +-angle_range in fraction of Pi\n",
    "\n",
    "    # REVERSE START <-> END POINTS?\n",
    "    is_start_semivalid = leaked_record[\"start_x\"] > 0 and leaked_record[\"start_delay\"] > 0\n",
    "    is_end_semivalid = leaked_record[\"end_x\"] > 0 and leaked_record[\"end_delay\"] > 0\n",
    "\n",
    "    if is_start_semivalid and is_end_semivalid:\n",
    "        is_reversed = leaked_record[\"start_delay\"] > leaked_record[\"end_delay\"]\n",
    "    elif is_end_semivalid:\n",
    "        is_reversed = True\n",
    "    else:\n",
    "        is_reversed = False\n",
    "\n",
    "    leaked_delay = leaked_record[\"end_delay\"] if is_reversed else leaked_record[\"start_delay\"]\n",
    "    leaked_x = leaked_record[\"end_x\"] if is_reversed else leaked_record[\"start_x\"]\n",
    "    leaked_y = leaked_record[\"end_y\"] if is_reversed else leaked_record[\"start_y\"]\n",
    "\n",
    "    if is_reversed:\n",
    "        motion_x = motion_x[::-1]\n",
    "        motion_y = motion_y[::-1]\n",
    "        motion_t = motion_t[::-1]\n",
    "    ###################################################################\n",
    "    num_elements = motion_t.shape[0]\n",
    "    trajectory_extent = ((motion_x.max() - motion_x.min()) ** 2 + (motion_y.max() - motion_y.min()) ** 2) ** 0.5\n",
    "    trajectory_extent_x = abs(motion_x.max() - motion_x.min())\n",
    "    trajectory_extent_y = abs(motion_y.max() - motion_y.min())\n",
    "\n",
    "    snap_distance = snap_range * trajectory_extent\n",
    "    snap_distance_x = snap_range * trajectory_extent_x\n",
    "    snap_distance_y = snap_range * trajectory_extent_y\n",
    "\n",
    "    time_deltas = np.abs(np.diff(motion_t))  # size \"n-1\"\n",
    "    motion_dx = np.diff(motion_x)\n",
    "    motion_dy = np.diff(motion_y)\n",
    "\n",
    "    motion_steps_lengths_input = (motion_dx ** 2 + motion_dy ** 2) ** 0.5\n",
    "    motion_angles_input = np.arctan2(motion_dy, motion_dx)\n",
    "    # print(num_elements)\n",
    "    ################  INITIAL/BOUNDARIES VALUES   ##################################\n",
    "    step_lengths_ini = np.amin(np.vstack([time_deltas * V_LIMIT, motion_steps_lengths_input]), axis=0)\n",
    "    angles_ini = motion_angles_input\n",
    "\n",
    "    step_lengths_min = np.amin(np.vstack([time_deltas * V_LIMIT * 0.99, motion_steps_lengths_input * (1 - step_range)]),\n",
    "                               axis=0)\n",
    "    step_lengths_max = np.amin(np.vstack([time_deltas * V_LIMIT * 1.00, motion_steps_lengths_input * (1 + step_range)]),\n",
    "                               axis=0)  # not more than maximum speed nor allowed variation of step length\n",
    "    angles_min = motion_angles_input - np.pi * angle_range\n",
    "    angles_max = motion_angles_input + np.pi * angle_range\n",
    "\n",
    "    angle0_ini = np.array([0.0])\n",
    "\n",
    "    initial_start = np.array([motion_x[0], motion_y[0]])\n",
    "\n",
    "    x_start_min = initial_start[0] - snap_distance_x\n",
    "    x_start_max = initial_start[0] + snap_distance_x\n",
    "    y_start_min = initial_start[1] - snap_distance_y\n",
    "    y_start_max = initial_start[1] + snap_distance_y\n",
    "    ############# Limit snap range by leaked data (if narrower and leaked data valid)  #######\n",
    "    if leaked_x > 0 and leaked_delay > 0 and (V_LIMIT * leaked_delay) < trajectory_extent * INVALID_RANGE:\n",
    "        _x_start_min = max(leaked_x - V_LIMIT * leaked_delay, x_start_min)\n",
    "        _x_start_max = min(leaked_x + V_LIMIT * leaked_delay, x_start_max)\n",
    "        _y_start_min = max(leaked_y - V_LIMIT * leaked_delay, y_start_min)\n",
    "        _y_start_max = min(leaked_y + V_LIMIT * leaked_delay, y_start_max)\n",
    "\n",
    "        x_start_min, x_start_max = (_x_start_min, _x_start_max) if _x_start_min < _x_start_max else (\n",
    "        x_start_min, x_start_max)\n",
    "        y_start_min, y_start_max = (_y_start_min, _y_start_max) if _y_start_min < _y_start_max else (\n",
    "        y_start_min, y_start_max)\n",
    "\n",
    "        #initial_start = np.array([(x_start_min + x_start_max) / 2, (y_start_min + y_start_max) / 2])\n",
    "    ################ RESCALE XY0/INI-BOUNDS  ####################\n",
    "    initial_start *= SCALE_XY0\n",
    "    x_start_min *= SCALE_XY0\n",
    "    x_start_max *= SCALE_XY0\n",
    "    y_start_min *= SCALE_XY0\n",
    "    y_start_max *= SCALE_XY0\n",
    "    #################################################################\n",
    "    bounds_angle0 = [(-angle0_range * np.pi, angle0_range * np.pi)]\n",
    "    bounds_start = [(0, None), (0, None)]\n",
    "    #bounds_start = [(x_start_min, x_start_max), (y_start_min, y_start_max)]  # 2x pairs\n",
    "    bounds_steps = [(min_el, max_el) for min_el, max_el in zip(step_lengths_min, step_lengths_max)]  # (n-1)x pairs\n",
    "    bounds_angles = [(min_el, max_el) for min_el, max_el in zip(angles_min, angles_max)]  # (n-1)x pairs\n",
    "\n",
    "    initial_guess = np.concatenate([step_lengths_ini, angles_ini, angle0_ini, initial_start])  # (2*n+2)x\n",
    "    bounds = bounds_steps + bounds_angles + bounds_angle0 + bounds_start  # (2*n+2)x pairs\n",
    "    ###############################################################################\n",
    "    #########################  SNAPPING POINTS   ##################################\n",
    "    snap_points = []\n",
    "\n",
    "    for i_step, _ in enumerate(motion_x):\n",
    "        _distances = ((fs_points[:, 0] - motion_x[i_step]) ** 2 + (fs_points[:, 1] - motion_y[i_step]) ** 2) ** 0.5\n",
    "        _snap_points = fs_points[_distances <= snap_distance]\n",
    "\n",
    "        if len(_snap_points) > 0:\n",
    "            snap_points.append(_snap_points)\n",
    "\n",
    "    snap_points = np.concatenate(snap_points)\n",
    "    snap_points = np.array(list(set(map(tuple, snap_points))))\n",
    "\n",
    "    # print(\"snap_distance\", snap_distance)\n",
    "    # print(\"snap_points.shape\", snap_points.shape)\n",
    "    ##################################################################################\n",
    "    ######################### OPTIMIZE ###############################################\n",
    "    if len(snap_points) > 0:  # some points are within snapping range\n",
    "\n",
    "        def loss(params):\n",
    "            # print(params.shape)\n",
    "            _d_xs = np.cumsum(params[: num_elements - 1] * np.cos(\n",
    "                params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "            _d_ys = np.cumsum(params[: num_elements - 1] * np.sin(\n",
    "                params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "            _xs = np.hstack([params[2 * num_elements - 1] / SCALE_XY0,\n",
    "                             params[2 * num_elements - 1] / SCALE_XY0 + _d_xs])\n",
    "            _ys = np.hstack([params[2 * num_elements] / SCALE_XY0,\n",
    "                             params[2 * num_elements] / SCALE_XY0 + _d_ys])\n",
    "\n",
    "            distances2 = (snap_points[:, 0].reshape(-1, 1) - _xs.reshape(1, -1)) ** 2 + (\n",
    "                        snap_points[:, 1].reshape(-1, 1) - _ys.reshape(1, -1)) ** 2\n",
    "            _loss = np.sum(np.amin(distances2, axis=0))  # near-field\n",
    "\n",
    "            return _loss / num_elements\n",
    "\n",
    "        results = minimize(fun=loss,\n",
    "                           x0=initial_guess,\n",
    "                           bounds=bounds,\n",
    "                           options={'maxcor': 30, 'ftol': 1e-08, 'gtol': 1e-07, 'maxfun': 20000, 'maxiter': 20000,\n",
    "                                    'maxls': 30},\n",
    "                           method=\"L-BFGS-B\")\n",
    "\n",
    "        params = results.x\n",
    "\n",
    "        _d_xs = np.cumsum(params[: num_elements - 1] * np.cos(\n",
    "            params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "        _d_ys = np.cumsum(params[: num_elements - 1] * np.sin(\n",
    "            params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "        _xs = np.hstack([params[2 * num_elements - 1] / SCALE_XY0,\n",
    "                         params[2 * num_elements - 1] / SCALE_XY0 + _d_xs])\n",
    "        _ys = np.hstack([params[2 * num_elements] / SCALE_XY0,\n",
    "                         params[2 * num_elements] / SCALE_XY0 + _d_ys])\n",
    "\n",
    "        if is_reversed:\n",
    "            return _xs[::-1], _ys[::-1], results.fun, results.success, results.message\n",
    "        else:\n",
    "            return _xs, _ys, results.fun, results.success, results.message\n",
    "    else:\n",
    "        return motion_x, motion_y, -1, False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66c93d7a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "T_CUT = 200000  # max segement duration in mseconds\n",
    "\n",
    "def optimize_trajectory_fs_v4(motion_x_full, motion_y_full, motion_t_full,\n",
    "                              fs_points,\n",
    "                              leaked_record,\n",
    "                              snap_range,  # +-snap/start_point shift range in fraction of x/y bounding box (max-min)\n",
    "                              step_range, angle_range,\n",
    "                              angle0_range):  # +-step_range in fraction of step length; +-angle_range in fraction of Pi\n",
    "    \n",
    "    num_cuts = np.ceil((leaked_record[\"end_t\"] - leaked_record[\"start_t\"])/T_CUT).astype(int)\n",
    "    num_elements_full = motion_t_full.shape[0]\n",
    "    \n",
    "    sum_xs = []\n",
    "    sum_ys = []\n",
    "    #print(\"cuts:\", num_cuts)\n",
    "    for i_cut in range(num_cuts):\n",
    "        # prepare data for a given segement\n",
    "        ind_s = i_cut*(num_elements_full//num_cuts)\n",
    "        ind_e = (num_elements_full//num_cuts)*(i_cut+1 if i_cut < num_cuts-1 else i_cut+2)  # large end index if last segment\n",
    "        \n",
    "        motion_x = motion_x_full[ind_s:ind_e]\n",
    "        motion_y = motion_y_full[ind_s:ind_e] \n",
    "        motion_t = motion_t_full[ind_s:ind_e]\n",
    "        \n",
    "        #print(f\"segment/full: {motion_x.shape}/{motion_x_full.shape}\")\n",
    "        ###################################################################\n",
    "        num_elements = motion_t.shape[0]\n",
    "        trajectory_extent = ((motion_x.max() - motion_x.min()) ** 2 + (motion_y.max() - motion_y.min()) ** 2) ** 0.5\n",
    "        trajectory_extent_x = abs(motion_x.max() - motion_x.min())\n",
    "        trajectory_extent_y = abs(motion_y.max() - motion_y.min())\n",
    "\n",
    "        snap_distance = snap_range * trajectory_extent\n",
    "        snap_distance_x = snap_range * trajectory_extent_x\n",
    "        snap_distance_y = snap_range * trajectory_extent_y\n",
    "\n",
    "        time_deltas = np.abs(np.diff(motion_t))  # size \"n-1\"\n",
    "        motion_dx = np.diff(motion_x)\n",
    "        motion_dy = np.diff(motion_y)\n",
    "\n",
    "        motion_steps_lengths_input = (motion_dx ** 2 + motion_dy ** 2) ** 0.5\n",
    "        motion_angles_input = np.arctan2(motion_dy, motion_dx)\n",
    "        # print(num_elements)\n",
    "        ################  INITIAL/BOUNDARIES VALUES   ##################################\n",
    "        step_lengths_ini = np.amin(np.vstack([time_deltas * V_LIMIT, motion_steps_lengths_input]), axis=0)\n",
    "        angles_ini = motion_angles_input\n",
    "\n",
    "        step_lengths_min = np.amin(np.vstack([time_deltas * V_LIMIT * 0.99, motion_steps_lengths_input * (1 - step_range)]),\n",
    "                                   axis=0)\n",
    "        step_lengths_max = np.amin(np.vstack([time_deltas * V_LIMIT * 1.00, motion_steps_lengths_input * (1 + step_range)]),\n",
    "                                   axis=0)  # not more than maximum speed nor allowed variation of step length\n",
    "        angles_min = motion_angles_input - np.pi * angle_range\n",
    "        angles_max = motion_angles_input + np.pi * angle_range\n",
    "\n",
    "        angle0_ini = np.array([0.0])\n",
    "\n",
    "        initial_start = np.array([motion_x[0], motion_y[0]])\n",
    "\n",
    "        x_start_min = initial_start[0] - snap_distance_x\n",
    "        x_start_max = initial_start[0] + snap_distance_x\n",
    "        y_start_min = initial_start[1] - snap_distance_y\n",
    "        y_start_max = initial_start[1] + snap_distance_y\n",
    "\n",
    "        ################ RESCALE XY0/INI-BOUNDS  ####################\n",
    "        initial_start *= SCALE_XY0\n",
    "        x_start_min *= SCALE_XY0\n",
    "        x_start_max *= SCALE_XY0\n",
    "        y_start_min *= SCALE_XY0\n",
    "        y_start_max *= SCALE_XY0\n",
    "        #################################################################\n",
    "        bounds_angle0 = [(-angle0_range * np.pi, angle0_range * np.pi)]\n",
    "        bounds_start = [(0, None), (0, None)]\n",
    "        #bounds_start = [(x_start_min, x_start_max), (y_start_min, y_start_max)]  # 2x pairs\n",
    "        bounds_steps = [(min_el, max_el) for min_el, max_el in zip(step_lengths_min, step_lengths_max)]  # (n-1)x pairs\n",
    "        bounds_angles = [(min_el, max_el) for min_el, max_el in zip(angles_min, angles_max)]  # (n-1)x pairs\n",
    "\n",
    "        initial_guess = np.concatenate([step_lengths_ini, angles_ini, angle0_ini, initial_start])  # (2*n+2)x\n",
    "        bounds = bounds_steps + bounds_angles + bounds_angle0 + bounds_start  # (2*n+2)x pairs\n",
    "        ###############################################################################\n",
    "        #########################  SNAPPING POINTS   ##################################\n",
    "        snap_points = []\n",
    "\n",
    "        for i_step, _ in enumerate(motion_x):\n",
    "            _distances = ((fs_points[:, 0] - motion_x[i_step]) ** 2 + (fs_points[:, 1] - motion_y[i_step]) ** 2) ** 0.5\n",
    "            _snap_points = fs_points[_distances <= snap_distance]\n",
    "\n",
    "            if len(_snap_points) > 0:\n",
    "                snap_points.append(_snap_points)\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"snap_distance\", snap_distance)\n",
    "        # print(\"snap_points.shape\", snap_points.shape)\n",
    "        ##################################################################################\n",
    "        ######################### OPTIMIZE ###############################################\n",
    "        if len(snap_points) > 0:  # some points are within snapping range\n",
    "            \n",
    "            snap_points = np.concatenate(snap_points)\n",
    "            snap_points = np.array(list(set(map(tuple, snap_points))))\n",
    "\n",
    "            def loss(params):\n",
    "                # print(params.shape)\n",
    "                _d_xs = np.cumsum(params[: num_elements - 1] * np.cos(\n",
    "                    params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "                _d_ys = np.cumsum(params[: num_elements - 1] * np.sin(\n",
    "                    params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "                _xs = np.hstack([params[2 * num_elements - 1] / SCALE_XY0,\n",
    "                                 params[2 * num_elements - 1] / SCALE_XY0 + _d_xs])\n",
    "                _ys = np.hstack([params[2 * num_elements] / SCALE_XY0,\n",
    "                                 params[2 * num_elements] / SCALE_XY0 + _d_ys])\n",
    "\n",
    "                distances2 = (snap_points[:, 0].reshape(-1, 1) - _xs.reshape(1, -1)) ** 2 + (\n",
    "                            snap_points[:, 1].reshape(-1, 1) - _ys.reshape(1, -1)) ** 2\n",
    "                _loss = np.sum(np.amin(distances2, axis=0))  # near-field\n",
    "\n",
    "                return _loss / num_elements\n",
    "\n",
    "            results = minimize(fun=loss,\n",
    "                               x0=initial_guess,\n",
    "                               bounds=bounds,\n",
    "                               options={'maxcor': 30, 'ftol': 1e-08, 'gtol': 1e-07, 'maxfun': 20000, 'maxiter': 20000,\n",
    "                                        'maxls': 30},\n",
    "                               method=\"L-BFGS-B\")\n",
    "\n",
    "            params = results.x\n",
    "\n",
    "            _d_xs = np.cumsum(params[: num_elements - 1] * np.cos(\n",
    "                params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "            _d_ys = np.cumsum(params[: num_elements - 1] * np.sin(\n",
    "                params[2 * num_elements - 2] + params[num_elements - 1: 2 * num_elements - 2]))\n",
    "            _xs = np.hstack([params[2 * num_elements - 1] / SCALE_XY0,\n",
    "                             params[2 * num_elements - 1] / SCALE_XY0 + _d_xs])\n",
    "            _ys = np.hstack([params[2 * num_elements] / SCALE_XY0,\n",
    "                             params[2 * num_elements] / SCALE_XY0 + _d_ys])\n",
    "\n",
    "            sum_xs.append(_xs)\n",
    "            sum_ys.append(_ys)\n",
    "        else:\n",
    "            sum_xs.append(motion_x)\n",
    "            sum_ys.append(motion_y)\n",
    "\n",
    "    return np.concatenate(sum_xs), np.concatenate(sum_ys), -1, False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "343c3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "def snap2fs(predicted_data, fs_data, leaked_data, snap_range, step_range, angle_range, angle0_range):\n",
    "\n",
    "    snapped_data = {}\n",
    "    site_ids = [\"5d2709d403f801723c32bd39\"]\n",
    "\n",
    "    for site_id in tqdm(site_ids):#\n",
    "        \n",
    "        #print(site_id)\n",
    "        snapped_data[site_id] = {}\n",
    "        for trace_id in tqdm(predicted_data[site_id]):  # over traces\n",
    "            #print(trace_id)\n",
    "            predicted_record = predicted_data[site_id][trace_id][[\"time\", \"x\", \"y\"]].copy(deep=True)\n",
    "            _floor = predicted_data[site_id][trace_id][\"floor\"][0]\n",
    "            fs_record = fs_data[site_id][_floor]\n",
    "            leaked_record = leaked_data[site_id][trace_id]\n",
    "            \n",
    "            _x, _y, _fun, _isOk, _exit_msg = optimize_trajectory_fs_v4(predicted_record.x.to_numpy(),\n",
    "                                                                       predicted_record.y.to_numpy(),\n",
    "                                                                       predicted_record.time.to_numpy(),\n",
    "                                                                       fs_record,\n",
    "                                                                       leaked_record,\n",
    "                                                                       snap_range, step_range, angle_range, angle0_range)\n",
    "            #print(f\"{_exit_msg}: {_isOk}\")\n",
    "            #if not _isOk:\n",
    "             #   print(f\"Failed Site/TraceIds: {site_id}/{trace_id}\")\n",
    "            \n",
    "            snapped_data[site_id][trace_id] = pd.DataFrame(_x, columns=[\"x\"])\n",
    "            snapped_data[site_id][trace_id][\"y\"] = _y\n",
    "            snapped_data[site_id][trace_id][\"floor\"] = _floor\n",
    "            snapped_data[site_id][trace_id][\"time\"] = predicted_record.time.to_numpy()\n",
    "            \n",
    "            #print(_fun)\n",
    "            #break\n",
    "        break\n",
    "        \n",
    "    return snapped_data\n",
    "\n",
    "###############    MULTIPROCESSING VERSION     #################################################################\n",
    "def snap2fs_multi(predicted_data, fs_data, leaked_data, snap_range, step_range, angle_range, angle0_range):\n",
    "\n",
    "    #site_ids = [\"5da1389e4db8ce0c98bd0547\", \"5d27075f03f801723c2e360f\"]#, \"5d2709b303f801723c327472\", \"5d27097f03f801723c320d97\", \"5da138b74db8ce0c98bd4774\", \"5d2709d403f801723c32bd39\"]\n",
    "    \n",
    "    site_ids = [\"5d27096c03f801723c31e5e0\", \"5d2709b303f801723c327472\", \"5d2709c303f801723c3299ee\",\n",
    "                \"5d2709d403f801723c32bd39\", \"5da138274db8ce0c98bbd3d2\", \"5da1382d4db8ce0c98bbe92e\",\n",
    "                \"5da138764db8ce0c98bcaa46\", \"5da958dd46f8266d0737457b\", \"5dbc1d84c1eb61796cf7c010\"]\n",
    "    site_ids = predicted_data.keys()\n",
    "    \n",
    "    input_data = [(site_id, predicted_data[site_id], fs_data[site_id], leaked_data[site_id], snap_range, step_range, angle_range, angle0_range) for site_id in site_ids]\n",
    "    \n",
    "    pool = Pool(cpu_count()-3) # Create a multiprocessing Pool\n",
    "    snapped_data = pool.starmap(fs_multi, input_data)  # process input_data iterable with pool\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "        \n",
    "    res_d = snapped_data[0]\n",
    "    for i, _d in enumerate(snapped_data):\n",
    "        if i>0:\n",
    "            res_d.update(_d)\n",
    "        \n",
    "    return res_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e354e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"blend_x7_0305\"\n",
    "predicted_data = pickle.load(open(f\"./submit/fit_data/post/{model_name}_snapped2motion4.pkl\", \"rb\")) #\n",
    "#predicted_data = pickle.load(open(f\"./submit/fit_data/post/{model_name}_snapped2motion-fsV2nl-02-03-01-01.pkl\", \"rb\")) \n",
    "fs_data = pickle.load(open(\"./data_out/freespace_1m_siteid_floorid.pkl\", \"rb\"))\n",
    "leaked_data = pickle.load(open(\"./data_out/leaked/leaked_siteid_traceid_all_delays.pkl\", \"rb\")) \n",
    "\n",
    "# +-step_range in fraction of step length; +-angle0/angle_range in fraction of Pi/fraction of angle-change\n",
    "#snapped2fs = snap2fs(predicted_data, fs_data, leaked_data, snap_range=0.2, step_range=0.4, angle_range=0.2, angle0_range=0.1) \n",
    "snapped2fs = snap2fs_multi(predicted_data, fs_data, leaked_data, snap_range=0.2, step_range=0.4, angle_range=0.2, angle0_range=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f881eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_multi(model_name, snapped2fs, sufix=f\"snap2motion4-fsV2nlF-02-04-02-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./submit/fit_data/post/{model_name}_snapped2motion4-fsV2nlF-02-04-02-01.pkl\", \"wb\") as f:\n",
    "    pickle.dump(snapped2fs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b22be2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02a8e32f14a4c9696a25d5e463130a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"blend_x7_0305\"\n",
    "predicted_data_post = pickle.load(open(f\"./submit/fit_data/post/{model_name}_snapped2motion4-fsV2nlp05-02-04-02-01.pkl\", \"rb\")) #\n",
    "\n",
    "grid_data = pickle.load(open(\"./data_out/waypoints_siteid_floorid.pkl\", \"rb\"))\n",
    "aux_grid_data = pickle.load(open(\"./data_out/freespace_2m_siteid_floorid.pkl\", \"rb\"))\n",
    "timestamps = pickle.load(open(f\"./data_out/submission_timestamps_traceid.pkl\", \"rb\"))\n",
    "\n",
    "snapped2grid = snap2grid(predicted_data_post, grid_data, aux_grid_data, timestamps, snap_range=4, aux_snap_range=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cb56b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_submission(model_name, snapped2grid, sufix=f\"snap2motionV2-gridWPs5_0805\")\n",
    "make_submission(model_name, snapped2grid, sufix=f\"snap2motion4-fsV2nlp05-02-04-02-01-gridWPs4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3260fa",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87942037",
   "metadata": {},
   "source": [
    "4. Multitude of Snaps2Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae62f7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02a8e32f14a4c9696a25d5e463130a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"blend_x7_0305\"\n",
    "predicted_data_post = pickle.load(open(f\"./submit/fit_data/post/{model_name}_snapped2motion4-fsV2nlp05-02-04-02-01.pkl\", \"rb\")) #\n",
    "\n",
    "grid_data = pickle.load(open(\"./data_out/waypoints_siteid_floorid.pkl\", \"rb\"))\n",
    "aux_grid_data = pickle.load(open(\"./data_out/freespace_2m_siteid_floorid.pkl\", \"rb\"))\n",
    "timestamps = pickle.load(open(f\"./data_out/submission_timestamps_traceid.pkl\", \"rb\"))\n",
    "\n",
    "snapped2grid = snap2grid(predicted_data_post, grid_data, aux_grid_data, timestamps, snap_range=4, aux_snap_range=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_multi(model_name, snapped2fs, sufix=f\"snap2motion4-fsV2nlF-02-04-02-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb416faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(model_name, snapped2grid, sufix=f\"snap2motion4-fsV2nlp05-02-04-02-01-gridWPs4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bbb6fb",
   "metadata": {},
   "source": [
    "---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf842e6",
   "metadata": {},
   "source": [
    "5. Blend Snaps2Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038cfc5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blend_suffix = \"_x7_0305\"\n",
    "\n",
    "blend_data = blend_predictions(\"./submit/fit_data/post/blend/\", \"models24_v11_LSTM-DNN_d02_s20-15-10-5_1619835100_predicted_s20_852.pkl\")\n",
    "\n",
    "with open(f\"./submit/fit_data/blend{blend_suffix}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(blend_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef5ba5e8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89e49888a68442e95051cb469498cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "blend_data = pickle.load(open(f\"./submit/fit_data/blend{blend_suffix}.pkl\", \"rb\"))\n",
    "plot_predictions_multi(f\"blend{blend_suffix}\", blend_data, sufix=f\"coarse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2426bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(model_name, snapped2grid, sufix=f\"snap2motion4-fsV2nlp05-02-04-02-01-gridWPs4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
