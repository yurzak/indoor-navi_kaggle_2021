{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d3f528b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from PIL import Image\n",
    "#%matplotlib inline\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from xyz10.io_f_mod import read_data_file\n",
    "from xyz10.visualize_f_mod import visualize_trajectory, save_figure_to_image\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from scipy.ndimage import median_filter\n",
    "from scipy.signal import medfilt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbdfe87",
   "metadata": {},
   "source": [
    "Supporting Functions (PLOT PREDICTIONS / MAKE SUBMISSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78337b78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_submission(model_name, data, sufix=\"coarse\"):\n",
    "\n",
    "    sample_submit = pd.read_csv(\"./submit/sample_submission.csv\")\n",
    "    splits = sample_submit.site_path_timestamp.str.split(pat=\"_\", expand=True)\n",
    "    sub_data = sample_submit.copy(deep=True).join(splits)\n",
    "    sub_data.rename(columns={0:\"site\", 1:\"path\", 2:\"timestamp\"}, inplace=True)\n",
    "\n",
    "    for i in tqdm(list(sub_data.index)):\n",
    "        site_id = sub_data.site[i]\n",
    "        trace_id = sub_data.path[i]\n",
    "        timestamp = sub_data.timestamp[i]\n",
    "\n",
    "        predicted_record = data[site_id][trace_id].to_numpy()\n",
    "\n",
    "        func_x = interp1d(predicted_record[:, 3], predicted_record[:, 0], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "        func_y = interp1d(predicted_record[:, 3], predicted_record[:, 1], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "\n",
    "        sub_data.loc[i, \"x\"] = func_x(timestamp)\n",
    "        sub_data.loc[i, \"y\"] = func_y(timestamp)\n",
    "        sub_data.loc[i, \"floor\"] = int(np.median(predicted_record[:, 2]))\n",
    "        #break\n",
    "\n",
    "    _ = [sub_data.pop(col) for col in [\"site\", \"path\", \"timestamp\"]]\n",
    "\n",
    "    sub_data.to_csv(f\"./submit/{model_name}_{sufix}.csv\", index=False)\n",
    "\n",
    "def plot_predictions_multi(model_name, data, sufix=\"coarse\"):\n",
    "    \n",
    "    def swap_trace_floor(predicted_data):\n",
    "        swap = {}\n",
    "\n",
    "        for site_id in predicted_data.keys():\n",
    "\n",
    "            swap[site_id] = {}\n",
    "            for trace_id in predicted_data[site_id].keys():\n",
    "\n",
    "                floor_id = predicted_data[site_id][trace_id].floor[0]\n",
    "                if floor_id not in swap[site_id].keys():\n",
    "                    swap[site_id][floor_id] = {}\n",
    "                swap[site_id][floor_id][trace_id] = predicted_data[site_id][trace_id]\n",
    "\n",
    "        return swap\n",
    "\n",
    "    data = swap_trace_floor(data)\n",
    "    \n",
    "    floor_convert = {'5a0546857ecc773753327266': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5c3c44b80379370013e0fd2b': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5d27075f03f801723c2e360f': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'},\n",
    "                     '5d27096c03f801723c31e5e0': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6'},\n",
    "                     '5d27097f03f801723c320d97': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5d27099f03f801723c32511d': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5d2709a003f801723c3251bf': {0: '1F', 1: '2F', 2: '3F', 3: '4F'},\n",
    "                     '5d2709b303f801723c327472': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F', 3: '4F'},\n",
    "                     '5d2709bb03f801723c32852c': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5d2709c303f801723c3299ee': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F', 3: '4F', 4: '5F', 5: '6F', 6: '7F', 7: '8F', 8: '9F'},\n",
    "                     '5d2709d403f801723c32bd39': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F'},\n",
    "                     '5d2709e003f801723c32d896': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da138274db8ce0c98bbd3d2': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da1382d4db8ce0c98bbe92e': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da138314db8ce0c98bbf3a0': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da138364db8ce0c98bc00f1': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da1383b4db8ce0c98bc11ab': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da138754db8ce0c98bca82f': {0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5da138764db8ce0c98bcaa46': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da1389e4db8ce0c98bd0547': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5da138b74db8ce0c98bd4774': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da958dd46f8266d0737457b': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'},\n",
    "                     '5dbc1d84c1eb61796cf7c010': {-1: 'B1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7', 7: 'F8'},\n",
    "                     '5dc8cea7659e181adb076a3f': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'}}\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f\"./img_out/predictions/{model_name}/\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    n_s = 0\n",
    "    for site_id in tqdm(data.keys()):  # over sites \n",
    "        n_s += 1\n",
    "        #print(f\"Processing Trajectories #{n_s}: Site-{site_id} with {len(data[site_id])} traces\")\n",
    "\n",
    "        try:\n",
    "            os.makedirs(f\"./img_out/predictions/{model_name}/{site_id}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for floor_id in data[site_id]:  # over traces\n",
    "            site_path = \"./data_in/metadata/\" + site_id + \"/\"\n",
    "            \n",
    "            positions = []\n",
    "            legends = []\n",
    "            for trace_id in data[site_id][floor_id].keys():\n",
    "                positions.append(data[site_id][floor_id][trace_id].to_numpy()[:, :2])\n",
    "                legends.append(trace_id)\n",
    "\n",
    "            try:\n",
    "                floor = floor_convert[site_id][floor_id]\n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                with open(info_path) as info_file:\n",
    "                    info_data = json.load(info_file)             \n",
    "\n",
    "                map_width = info_data[\"map_info\"][\"width\"]\n",
    "                map_height = info_data[\"map_info\"][\"height\"]\n",
    "\n",
    "                fig_steps = visualize_trajectory(trajectory=positions, is_multi = True,\n",
    "                                                 floor_plan_filename=map_path, mode=\"lines + markers\", title=f\"{site_id}_{floor}_{sufix}\", legends=legends, \n",
    "                                                 width_meter=map_width,  height_meter=map_height)\n",
    "                save_figure_to_image(fig_steps, f\"./img_out/predictions/{model_name}/{site_id}/{floor}_{sufix}.png\")\n",
    "            except:\n",
    "                print(f\"Exception: wrong floor-{floor} site-{site_id}\")\n",
    "\n",
    "        #break  # only first site_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a4910",
   "metadata": {},
   "source": [
    "Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970be0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_test_data = pickle.load(open(\"./data_out/full24/test-10k_mix-counts.pkl\", \"rb\"))   # CHANGE\n",
    "floor100_siteid_traceid = pickle.load(open(\"./data_out/floor100_siteid_traceid.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e113119",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dbbaa7b661a42f7aa0b110e36c85db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Predictions #1: Site-5da1389e4db8ce0c98bd0547 with 13 traces\n",
      "WARNING:tensorflow:AutoGraph could not transform <function xy_loss_metric at 0x000001B81A7C73A0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function xy_loss_metric at 0x000001B81A7C73A0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function xy_loss_metric_mse at 0x000001B81A7C7160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function xy_loss_metric_mse at 0x000001B81A7C7160> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Processing Predictions #2: Site-5da138b74db8ce0c98bd4774 with 29 traces\n",
      "Processing Predictions #3: Site-5da138764db8ce0c98bcaa46 with 36 traces\n",
      "Processing Predictions #4: Site-5dbc1d84c1eb61796cf7c010 with 50 traces\n",
      "Processing Predictions #5: Site-5da1383b4db8ce0c98bc11ab with 26 traces\n",
      "Processing Predictions #6: Site-5d2709a003f801723c3251bf with 20 traces\n",
      "Processing Predictions #7: Site-5a0546857ecc773753327266 with 29 traces\n",
      "Processing Predictions #8: Site-5da138274db8ce0c98bbd3d2 with 8 traces\n",
      "Processing Predictions #9: Site-5d2709b303f801723c327472 with 31 traces\n",
      "Processing Predictions #10: Site-5da958dd46f8266d0737457b with 51 traces\n",
      "Processing Predictions #11: Site-5d2709bb03f801723c32852c with 34 traces\n",
      "Processing Predictions #12: Site-5d2709d403f801723c32bd39 with 51 traces\n",
      "Processing Predictions #13: Site-5dc8cea7659e181adb076a3f with 35 traces\n",
      "Processing Predictions #14: Site-5d27096c03f801723c31e5e0 with 60 traces\n",
      "Processing Predictions #15: Site-5d2709c303f801723c3299ee with 31 traces\n",
      "Processing Predictions #16: Site-5da138314db8ce0c98bbf3a0 with 17 traces\n",
      "Processing Predictions #17: Site-5da1382d4db8ce0c98bbe92e with 11 traces\n",
      "Processing Predictions #18: Site-5d27075f03f801723c2e360f with 5 traces\n",
      "Processing Predictions #19: Site-5da138754db8ce0c98bca82f with 23 traces\n",
      "Processing Predictions #20: Site-5d2709e003f801723c32d896 with 31 traces\n",
      "Processing Predictions #21: Site-5d27097f03f801723c320d97 with 17 traces\n",
      "Processing Predictions #22: Site-5da138364db8ce0c98bc00f1 with 10 traces\n",
      "Processing Predictions #23: Site-5c3c44b80379370013e0fd2b with 3 traces\n",
      "Processing Predictions #24: Site-5d27099f03f801723c32511d with 5 traces\n"
     ]
    }
   ],
   "source": [
    "model_name = \"models24_v6_FcatMR_fix_data95\"\n",
    "model_path = \"./saved_models/\" + model_name\n",
    "\n",
    "site_ids = [\"5da1389e4db8ce0c98bd0547\"] \n",
    "\n",
    "predicted_data = {}\n",
    "\n",
    "def xy_loss_metric(y_true, y_pred):\n",
    "    e_xy = tf.sqrt(tf.square(y_true[:, 0] - y_pred[:, 0]) +  tf.square(y_true[:, 1] - y_pred[:, 1])) \n",
    "    return tf.reduce_mean(e_xy, axis=-1)\n",
    "\n",
    "def xy_loss_metric_mse(y_true, y_pred):\n",
    "    e_xy = tf.square(y_true[:, 0] - y_pred[:, 0]) +  tf.square(y_true[:, 1] - y_pred[:, 1]) \n",
    "    return tf.sqrt(tf.reduce_mean(e_xy, axis=-1))\n",
    "\n",
    "n_s= 0\n",
    "for site_id in tqdm(parsed_test_data.keys()):#site_ids:#tqdm(parsed_test_data.keys()):  # over sites\n",
    "    n_s += 1\n",
    "    print(f\"Processing Predictions #{n_s}: Site-{site_id} with {len(parsed_test_data[site_id])} traces\")\n",
    "    ############# GET MODELS (suffix indicates target values)####################   \n",
    "    features_xy = pickle.load(open(model_path + f\"/{site_id}/features_list.pkl\", \"rb\"))\n",
    "    scaler_xy = pickle.load(open(model_path + f\"/{site_id}/scaler.pkl\", \"rb\"))\n",
    "    encoder_xy = pickle.load(open(model_path + f\"/{site_id}/f_binarizer.pkl\", \"rb\"))\n",
    "    model_xy = tf.keras.models.load_model(model_path + f\"/{site_id}\", custom_objects={\"xy_loss_metric_mse\": xy_loss_metric_mse, \"xy_loss_metric\": xy_loss_metric})\n",
    "    ##############################################\n",
    "    predicted_data[site_id] = {}\n",
    "    for trace_id in parsed_test_data[site_id]:  # over traces\n",
    "        \n",
    "        trace_record = parsed_test_data[site_id][trace_id].copy(deep=True)\n",
    "        ######### GENERAL FEATURE MANIPULATION ################    \n",
    "        _time = trace_record.pop(\"time\").to_numpy()\n",
    "        _magnetic = trace_record.pop(\"m\").to_numpy().reshape((-1, 1))\n",
    "        _rotate = trace_record.pop(\"r\").to_numpy().reshape((-1, 1))\n",
    "        trace_record_xy = trace_record[features_xy].copy(deep=True)\n",
    "        \n",
    "        rssi_limit = -94\n",
    "        delay_limit = 1000\n",
    "####################################################\n",
    "        trace_record_xy[trace_record_xy > delay_limit] = delay_limit\n",
    "        trace_record_xy[trace_record_xy < rssi_limit] = rssi_limit\n",
    "####################################################\n",
    "        trace_record_xy = trace_record_xy.to_numpy()\n",
    "        \n",
    "        ##########################################################\n",
    "        #  PREDICT XY (with F100 feature)\n",
    "        ##########################################################\n",
    "        _pred_f = floor100_siteid_traceid[site_id][trace_id]\n",
    "        \n",
    "        trace_record_xy_scaled = scaler_xy.transform(np.concatenate((trace_record_xy, _rotate, _magnetic, encoder_xy.transform(np.full_like(_time, _pred_f))), axis=1))\n",
    "        predictions_xy = model_xy.predict(trace_record_xy_scaled)\n",
    "        \n",
    "        predictions_xy_x = predictions_xy[:, 0].reshape((-1, 1))\n",
    "        predictions_xy_y = predictions_xy[:, 1].reshape((-1, 1))\n",
    "        # median filter to remove outliers  + average over folds\n",
    "        #predictions_xy_x = medfilt(predictions_xy_x)\n",
    "        #predictions_xy_y = medfilt(predictions_xy_y)\n",
    "        #predictions_xy_x = np.median(median_filter(predictions_xy_x, (3,3)), axis=0).reshape((-1, 1))\n",
    "        #predictions_xy_y = np.median(median_filter(predictions_xy_y, (3,3)), axis=0).reshape((-1, 1))\n",
    "                                                              \n",
    "        #  COMBINE into final DataFrame\n",
    "        predictions_xyf = pd.DataFrame(np.concatenate((predictions_xy_x, predictions_xy_y), axis=1), columns=[\"x\", \"y\"])\n",
    "        predictions_xyf[\"floor\"] = _pred_f\n",
    "        predictions_xyf[\"time\"] = _time\n",
    "\n",
    "        predicted_data[site_id][trace_id] = predictions_xyf\n",
    "        \n",
    "    keras.backend.clear_session()\n",
    "        \n",
    "        #break  # only first trace\n",
    "    \n",
    "    #break  # only first site_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed4c5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./submit/fit_data/{model_name}_predicted.pkl\", \"wb\") as f:\n",
    "    pickle.dump(predicted_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b434ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0cd337046447a7bd68667b2dc426fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#predicted_data = pickle.load(open(f\"./submit/fit_data/{model_name}_predicted.pkl\", \"rb\"))\n",
    "plot_predictions_multi(model_name, predicted_data, sufix=\"coarse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2b86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fff8da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0351f6f7e2d64d5d90aa3bee4880ac60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_data = pickle.load(open(f\"./submit/fit_data/{model_name}_predicted.pkl\", \"rb\"))\n",
    "make_submission(model_name, predicted_data, sufix=\"coarse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a0db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
