{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a75a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from PIL import Image\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "sns.set_style('darkgrid')\n",
    "#%matplotlib inline\n",
    "\n",
    "from xyz10.io_f_mod import read_data_file\n",
    "from xyz10.visualize_f_mod import visualize_trajectory, save_figure_to_image\n",
    "from scipy.ndimage import median_filter\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample, shuffle\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"  # \"0\" = GPU_on, \"-1\" = GPU_off\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "#%load_ext tensorboard  # extension for notebook\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e6796",
   "metadata": {},
   "source": [
    "Supporting Functions (PLOT PREDICTIONS / MAKE SUBMISSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db19791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_seq(data, mode, seq_len):  # returns sequenced data (3d-> sequence*timestep*features)\n",
    "    # data: 2d -> number records*features\n",
    "    # modes: \"moving\"/\"stacked\"/\"full\"\n",
    "    if mode == \"full\":\n",
    "         return data.reshape(1, data.shape[0], data.shape[1])\n",
    "    elif mode == \"stacked\":\n",
    "        seq_num = data.shape[0]//seq_len\n",
    "        _data = data[:seq_num*seq_len]\n",
    "        return _data.reshape(seq_num, seq_len, _data.shape[1])\n",
    "    elif mode == \"moving\":\n",
    "        seq_num = data.shape[0]-seq_len+1\n",
    "        \n",
    "        seq = []\n",
    "        for i_seq in range(seq_num):\n",
    "            seq.append(data[i_seq:i_seq+seq_len])\n",
    "        return np.stack(seq)\n",
    "    \n",
    "def make_seq_inv(data, mode, seq_len):  # returns zero sequence data (2d-> timestep*1(x or y))\n",
    "    # data: 3d -> number of sequences*number records-sequence length*(x or y)\n",
    "    # modes: \"moving\"/\"stacked\"/\"full\"\n",
    "    if mode == \"full\" or mode == \"stacked\":\n",
    "        return data.reshape(data.shape[0]*data.shape[1], 1)\n",
    "    elif mode == \"moving\":  # combine moving sequences via median/mean filter\n",
    "        seq0_len = data.shape[0] + seq_len - 1\n",
    "        _data = np.zeros((seq0_len, 1))\n",
    "        \n",
    "        for i_row in range(seq0_len):\n",
    "            row_el = []\n",
    "            j_start = max(0, i_row-seq_len+1)\n",
    "            j_end = min(i_row, seq0_len-seq_len)\n",
    "            \n",
    "            for i_seq in range(j_start, j_end+1):\n",
    "                row_el.append(data[i_seq, i_row-i_seq])\n",
    "                \n",
    "            _data[i_row] = np.median(row_el)\n",
    "\n",
    "        return _data  \n",
    "\n",
    "def make_submission(model_name, data, sufix=\"coarse\"):\n",
    "\n",
    "    sample_submit = pd.read_csv(\"./submit/sample_submission.csv\")\n",
    "    splits = sample_submit.site_path_timestamp.str.split(pat=\"_\", expand=True)\n",
    "    sub_data = sample_submit.copy(deep=True).join(splits)\n",
    "    sub_data.rename(columns={0:\"site\", 1:\"path\", 2:\"timestamp\"}, inplace=True)\n",
    "\n",
    "    for i in tqdm(list(sub_data.index)):\n",
    "        site_id = sub_data.site[i]\n",
    "        trace_id = sub_data.path[i]\n",
    "        timestamp = sub_data.timestamp[i]\n",
    "\n",
    "        predicted_record = data[site_id][trace_id].to_numpy()\n",
    "\n",
    "        func_x = interp1d(predicted_record[:, 3], predicted_record[:, 0], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "        func_y = interp1d(predicted_record[:, 3], predicted_record[:, 1], kind=\"linear\", copy=False, fill_value=\"extrapolate\")\n",
    "\n",
    "        sub_data.loc[i, \"x\"] = func_x(timestamp)\n",
    "        sub_data.loc[i, \"y\"] = func_y(timestamp)\n",
    "        sub_data.loc[i, \"floor\"] = int(np.median(predicted_record[:, 2]))\n",
    "        #break\n",
    "\n",
    "    _ = [sub_data.pop(col) for col in [\"site\", \"path\", \"timestamp\"]]\n",
    "\n",
    "    sub_data.to_csv(f\"./submit/{model_name}_{sufix}.csv\", index=False)\n",
    "\n",
    "def plot_predictions_multi(model_name, data, sufix=\"coarse\", delay_suffix=False):\n",
    "    \n",
    "    def swap_trace_floor(predicted_data):\n",
    "        swap = {}\n",
    "\n",
    "        for site_id in predicted_data.keys():\n",
    "\n",
    "            swap[site_id] = {}\n",
    "            for trace_id in predicted_data[site_id].keys():\n",
    "\n",
    "                floor_id = predicted_data[site_id][trace_id].floor[0]\n",
    "                if floor_id not in swap[site_id].keys():\n",
    "                    swap[site_id][floor_id] = {}\n",
    "                swap[site_id][floor_id][trace_id] = predicted_data[site_id][trace_id]\n",
    "\n",
    "        return swap\n",
    "\n",
    "    data = swap_trace_floor(data)\n",
    "    \n",
    "    floor_convert = {'5a0546857ecc773753327266': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5c3c44b80379370013e0fd2b': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5d27075f03f801723c2e360f': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'},\n",
    "                     '5d27096c03f801723c31e5e0': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6'},\n",
    "                     '5d27097f03f801723c320d97': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5d27099f03f801723c32511d': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5d2709a003f801723c3251bf': {0: '1F', 1: '2F', 2: '3F', 3: '4F'},\n",
    "                     '5d2709b303f801723c327472': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F', 3: '4F'},\n",
    "                     '5d2709bb03f801723c32852c': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5d2709c303f801723c3299ee': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F', 3: '4F', 4: '5F', 5: '6F', 6: '7F', 7: '8F', 8: '9F'},\n",
    "                     '5d2709d403f801723c32bd39': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F'},\n",
    "                     '5d2709e003f801723c32d896': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da138274db8ce0c98bbd3d2': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da1382d4db8ce0c98bbe92e': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da138314db8ce0c98bbf3a0': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da138364db8ce0c98bc00f1': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da1383b4db8ce0c98bc11ab': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da138754db8ce0c98bca82f': {0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5da138764db8ce0c98bcaa46': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da1389e4db8ce0c98bd0547': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5da138b74db8ce0c98bd4774': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da958dd46f8266d0737457b': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'},\n",
    "                     '5dbc1d84c1eb61796cf7c010': {-1: 'B1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7', 7: 'F8'},\n",
    "                     '5dc8cea7659e181adb076a3f': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'}}\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f\"./img_out/predictions/{model_name}/\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    n_s = 0\n",
    "    for site_id in tqdm(data.keys()):  # over sites \n",
    "        n_s += 1\n",
    "        #print(f\"Processing Trajectories #{n_s}: Site-{site_id} with {len(data[site_id])} traces\")\n",
    "\n",
    "        try:\n",
    "            os.makedirs(f\"./img_out/predictions/{model_name}/{site_id}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for floor_id in data[site_id]:  # over traces\n",
    "            site_path = \"./data_in/metadata/\" + site_id + \"/\"\n",
    "            \n",
    "            positions = []\n",
    "            legends = []\n",
    "            for trace_id in data[site_id][floor_id].keys():\n",
    "                positions.append(data[site_id][floor_id][trace_id].to_numpy()[:, :2])\n",
    "                \n",
    "                if delay_suffix:\n",
    "                    legends.append(f\"{trace_id}_s{int(0.001*data[site_id][floor_id][trace_id].to_numpy()[0, 4])}_e{int(0.001*data[site_id][floor_id][trace_id].to_numpy()[1, 4])}\")\n",
    "                else:\n",
    "                    legends.append(trace_id)\n",
    "\n",
    "            try:\n",
    "                floor = floor_convert[site_id][floor_id]\n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                with open(info_path) as info_file:\n",
    "                    info_data = json.load(info_file)             \n",
    "\n",
    "                map_width = info_data[\"map_info\"][\"width\"]\n",
    "                map_height = info_data[\"map_info\"][\"height\"]\n",
    "\n",
    "                fig_steps = visualize_trajectory(trajectory=positions, is_multi = True,\n",
    "                                                 floor_plan_filename=map_path, mode=\"lines + markers\", title=f\"{site_id}_{floor}_{sufix}\", legends=legends, \n",
    "                                                 width_meter=map_width,  height_meter=map_height)\n",
    "                save_figure_to_image(fig_steps, f\"./img_out/predictions/{model_name}/{site_id}/{floor}_{sufix}.png\")\n",
    "            except:\n",
    "                print(f\"Exception: wrong floor-{floor} site-{site_id}\")\n",
    "\n",
    "        #break  # only first site_id\n",
    "        \n",
    "def plot_predictions_steps(model_name, data, sufix=\"coarse\", delay_suffix=False):\n",
    "       \n",
    "    floor_convert = {'5a0546857ecc773753327266': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5c3c44b80379370013e0fd2b': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5d27075f03f801723c2e360f': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'},\n",
    "                     '5d27096c03f801723c31e5e0': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6'},\n",
    "                     '5d27097f03f801723c320d97': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5d27099f03f801723c32511d': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5d2709a003f801723c3251bf': {0: '1F', 1: '2F', 2: '3F', 3: '4F'},\n",
    "                     '5d2709b303f801723c327472': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F', 3: '4F'},\n",
    "                     '5d2709bb03f801723c32852c': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5d2709c303f801723c3299ee': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F', 3: '4F', 4: '5F', 5: '6F', 6: '7F', 7: '8F', 8: '9F'},\n",
    "                     '5d2709d403f801723c32bd39': {-1: 'B1', 0: '1F', 1: '2F', 2: '3F'},\n",
    "                     '5d2709e003f801723c32d896': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da138274db8ce0c98bbd3d2': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da1382d4db8ce0c98bbe92e': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da138314db8ce0c98bbf3a0': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da138364db8ce0c98bc00f1': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da1383b4db8ce0c98bc11ab': {0: 'F1', 1: 'F2', 2: 'F3'},\n",
    "                     '5da138754db8ce0c98bca82f': {0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5da138764db8ce0c98bcaa46': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da1389e4db8ce0c98bd0547': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4'},\n",
    "                     '5da138b74db8ce0c98bd4774': {-2: 'B2', -1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5'},\n",
    "                     '5da958dd46f8266d0737457b': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'},\n",
    "                     '5dbc1d84c1eb61796cf7c010': {-1: 'B1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7', 7: 'F8'},\n",
    "                     '5dc8cea7659e181adb076a3f': {-1: 'B1', 0: 'F1', 1: 'F2', 2: 'F3', 3: 'F4', 4: 'F5', 5: 'F6', 6: 'F7'}}\n",
    "\n",
    "    try:\n",
    "        os.makedirs(f\"./img_out/predictions/{model_name}/\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    n_s = 0\n",
    "    for site_id in tqdm(data.keys()):  # over sites \n",
    "        n_s += 1\n",
    "        #print(f\"Processing Trajectories #{n_s}: Site-{site_id} with {len(data[site_id])} traces\")\n",
    "\n",
    "        try:\n",
    "            os.makedirs(f\"./img_out/predictions/{model_name}/{site_id}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for floor_id in data[site_id]:  # over traces\n",
    "            site_path = \"./data_in/metadata/\" + site_id + \"/\"\n",
    "            \n",
    "            positions = data[site_id][floor_id]\n",
    "            legends = []\n",
    "\n",
    "            try:\n",
    "                floor = floor_convert[site_id][floor_id]\n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                meta_path = site_path + floor\n",
    "                map_path = meta_path + \"/floor_image.png\"\n",
    "                info_path = meta_path + \"/floor_info.json\" \n",
    "\n",
    "                with open(info_path) as info_file:\n",
    "                    info_data = json.load(info_file)             \n",
    "\n",
    "                map_width = info_data[\"map_info\"][\"width\"]\n",
    "                map_height = info_data[\"map_info\"][\"height\"]\n",
    "\n",
    "                fig_steps = visualize_trajectory(trajectory=positions, is_multi = False,\n",
    "                                                 floor_plan_filename=map_path, mode=\"lines + markers\", title=f\"{site_id}_{floor}_{sufix}\", legends=legends, \n",
    "                                                 width_meter=map_width,  height_meter=map_height)\n",
    "                save_figure_to_image(fig_steps, f\"./img_out/predictions/{model_name}/{site_id}/{floor}_{sufix}.png\")\n",
    "            except:\n",
    "                print(f\"Exception: wrong floor-{floor} site-{site_id}\")\n",
    "                \n",
    "        #break\n",
    "\n",
    "def xy_loss_metric(y_true, y_pred):\n",
    "    e_xy = tf.sqrt(tf.square(y_true[:, 0] - y_pred[:, 0]) +  tf.square(y_true[:, 1] - y_pred[:, 1])) \n",
    "    return tf.reduce_mean(e_xy, axis=-1)\n",
    "\n",
    "def xy_loss_metric_mse(y_true, y_pred):\n",
    "    e_xy = tf.square(y_true[:, 0] - y_pred[:, 0]) +  tf.square(y_true[:, 1] - y_pred[:, 1]) \n",
    "    return tf.sqrt(tf.reduce_mean(e_xy, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9d7572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(site_id, fraction_bssid, seq_len=10, data_file=\"10k_mix-counts\"):\n",
    "    print(\"Loading Data...\")\n",
    "    path = f\"./data_out/full24/seq{seq_len}/\"  # full24/\n",
    "    file_name = f\"{site_id}_{data_file}.pkl\"\n",
    "    \n",
    "    f_list = features_list(site_id, fraction_bssid)\n",
    "\n",
    "    return f_list, pickle.load(open(path+file_name, \"rb\"))[f_list]\n",
    "\n",
    "def features_list(site_id, fraction_bssid, bssid_mode=\"count\"):\n",
    "    \n",
    "    train_bssid = pickle.load(open(\"./data_out/train_24IDs_standardF_bssid_ranks.pkl\", \"rb\"))\n",
    "    test_bssid = pickle.load(open(\"./data_out/test_bssid_ranks.pkl\", \"rb\"))\n",
    "\n",
    "    if fraction_bssid <= 1:\n",
    "        _train_bssid = train_bssid[bssid_mode][site_id].bssid.tolist()\n",
    "        _test_bssid = test_bssid[bssid_mode][site_id].bssid.tolist()\n",
    "        _mix = list(set(_train_bssid) & set(_test_bssid))\n",
    "        _list = _mix[:int(fraction_bssid*len(_mix))]\n",
    "    else:\n",
    "        _train_bssid = train_bssid[bssid_mode][site_id].bssid.tolist()\n",
    "        _test_bssid = test_bssid[bssid_mode][site_id].bssid.tolist()\n",
    "        _mix = list(set(_train_bssid) & set(_test_bssid))\n",
    "        _list = _mix[:int(fraction_bssid)]\n",
    "\n",
    "    _list_d = []\n",
    "    for record in _list:\n",
    "        _list_d.append(record)\n",
    "        _list_d.append(record+\"_D\")\n",
    "        \n",
    "    _list_d += [\"x\", \"y\", \"f\", \"m\", \"r\", \"rx\", \"ry\", \"rx_cum\", \"ry_cum\", \"trace\"]  # 2+7 = 9+11\n",
    "        \n",
    "    return _list_d\n",
    "\n",
    "def preprocess_data(featured_data, seq_len=10, train_fraction=0.8, random_state=123):\n",
    "    print(f\"Processing Data of shape {featured_data[1].shape}...\")\n",
    "\n",
    "    # shuffle sequences\n",
    "    data_shape = featured_data[1].shape\n",
    "    columns = featured_data[1].columns.tolist()\n",
    "    #display(featured_data[1].describe())\n",
    "\n",
    "    # define train/test traces\n",
    "    t_list = featured_data[1][\"trace\"].unique()\n",
    "    t_list_s = t_list[shuffle(list(range(len(t_list))))]\n",
    "    train_traces = list(t_list_s[:int((train_fraction)*len(t_list_s))])\n",
    "    test_traces = list(t_list_s[int((train_fraction)*len(t_list_s)):])\n",
    "\n",
    "    # define corresponding train/test indices\n",
    "    gr = featured_data[1].groupby(\"trace\")\n",
    "    for i, trace in enumerate(train_traces):\n",
    "        if i == 0:\n",
    "            inds_train = gr.groups[trace]\n",
    "        else:\n",
    "            inds_train = inds_train.append(gr.groups[trace])\n",
    "\n",
    "    for i, trace in enumerate(test_traces):\n",
    "        if i == 0:\n",
    "            inds_test = gr.groups[trace]\n",
    "        else:\n",
    "            inds_test = inds_test.append(gr.groups[trace])\n",
    "\n",
    "    _ = featured_data[1].pop(\"trace\")\n",
    "    columns.remove(\"trace\")\n",
    "    \n",
    "    train_data = featured_data[1].loc[inds_train, columns].copy(deep=True)\n",
    "    test_data = featured_data[1].loc[inds_test, columns].copy(deep=True)\n",
    "\n",
    "    train_data_shape = train_data.shape\n",
    "    train_data_s = train_data.to_numpy().reshape(len(train_data)//seq_len, seq_len, train_data.shape[1])\n",
    "    np.random.shuffle(train_data_s)\n",
    "    train_data_s = pd.DataFrame(train_data_s.reshape(train_data_shape), columns=columns)\n",
    "    #train_data_s[numeric_cols] = train_data_s[numeric_cols].apply(pd.to_numeric)\n",
    "\n",
    "    test_data_shape = test_data.shape\n",
    "    test_data_s = test_data.to_numpy().reshape(len(test_data)//seq_len, seq_len, test_data.shape[1])\n",
    "    np.random.shuffle(test_data_s)\n",
    "    test_data_s = pd.DataFrame(test_data_s.reshape(test_data_shape), columns=columns)\n",
    "    #test_data_s[numeric_cols] = test_data_s[numeric_cols].apply(pd.to_numeric)\n",
    "    \n",
    "    shuffled_data = pd.concat([train_data_s, test_data_s], axis=0)\n",
    "    \n",
    "    # split-combine features and targets    \n",
    "    y_x = shuffled_data.pop(\"x\")\n",
    "    y_y = shuffled_data.pop(\"y\")\n",
    "    \n",
    "    x_m = shuffled_data.pop(\"m\").to_numpy().reshape(-1,1)\n",
    "    x_r = shuffled_data.pop(\"r\").to_numpy().reshape(-1,1)\n",
    "    x_rx = shuffled_data.pop(\"rx\").to_numpy().reshape(-1,1)\n",
    "    x_ry = shuffled_data.pop(\"ry\").to_numpy().reshape(-1,1)\n",
    "    x_rx_cum = shuffled_data.pop(\"rx_cum\").to_numpy().reshape(-1,1)\n",
    "    x_ry_cum = shuffled_data.pop(\"ry_cum\").to_numpy().reshape(-1,1)\n",
    "    x_f = shuffled_data.pop(\"f\").to_numpy().astype(int)\n",
    "    \n",
    "    x = shuffled_data\n",
    "\n",
    "    encoder = LabelBinarizer()#OneHotEncoder(sparse=False)\n",
    "    x_f = encoder.fit_transform(x_f)\n",
    "    \n",
    "    #x = np.concatenate((x, x_f), axis=1)\n",
    "    x = np.concatenate((x, x_m, x_r, x_rx, x_ry, x_rx_cum, x_ry_cum, x_f), axis=1)  #x_m, x_r, x_rx, x_ry, x_rx_cum, x_ry_cum, \n",
    "    y = pd.concat([y_x, y_y], axis=1).to_numpy()\n",
    "    \n",
    "    # split into train/validation\n",
    "    train_x, val_x = x[:train_data_shape[0]], x[train_data_shape[0]:]\n",
    "    train_y, val_y = y[:train_data_shape[0]], y[train_data_shape[0]:]   \n",
    "            \n",
    "    # scale data\n",
    "    scaler = StandardScaler()  # RobustScaler()  /StandardScaler()/ MinMaxScaler\n",
    "    train_x = scaler.fit_transform(train_x)\n",
    "    val_x = scaler.transform(val_x)\n",
    "        \n",
    "    # final shaping\n",
    "    train_y = train_y.reshape(train_x.shape[0]//seq_len, seq_len, 2) \n",
    "    val_y = val_y.reshape(val_x.shape[0]//seq_len, seq_len, 2)\n",
    "    train_x = train_x.reshape(train_x.shape[0]//seq_len, seq_len, train_x.shape[1]) \n",
    "    val_x = val_x.reshape(val_x.shape[0]//seq_len, seq_len, val_x.shape[1])\n",
    "            \n",
    "    return featured_data[0][:-10], scaler, encoder, train_x, val_x, train_y, val_y  # features list is only bssid related\n",
    "        \n",
    "\n",
    "def save_models(models24, models=\"models24_bssid10k_SP_count_mix\"):\n",
    "    print(\"Saving Models...\")\n",
    "    \n",
    "    idx = int(time.time())\n",
    "    model_name = f\"{models}_{idx}/\"\n",
    "    model_path = \"./saved_models/\"+ model_name\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    for site_id in tqdm(models24.keys()):\n",
    "\n",
    "        models24[site_id][3].save(model_path + site_id)\n",
    "        with open(model_path + site_id + f\"/features_list.pkl\", \"wb\") as f:\n",
    "            pickle.dump(models24[site_id][0], f)\n",
    "        with open(model_path + site_id + f\"/scaler.pkl\", \"wb\") as f:\n",
    "            pickle.dump(models24[site_id][1], f)\n",
    "        with open(model_path + site_id + f\"/f_binarizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(models24[site_id][2], f)\n",
    "            \n",
    "    return model_name\n",
    "            \n",
    "def calculate_global_metrics(comparison):\n",
    "\n",
    "    train_mae = 0\n",
    "    val_mae = 0\n",
    "\n",
    "    for comparison_train, comparison_val in comparison:\n",
    "\n",
    "        train_mae += comparison_train.mean_abs_error[comparison_train.index[-1]]\n",
    "        val_mae += comparison_val.mean_abs_error[comparison_val.index[-1]]\n",
    "\n",
    "    comp_length = len(comparison)\n",
    "    \n",
    "    train_mae /= comp_length\n",
    "    val_mae /= comp_length\n",
    "\n",
    "    print(f\"Global Train/Validation MAE: {train_mae}/{val_mae}\")\n",
    "            \n",
    "def xy_loss_metric(y_true, y_pred):\n",
    "    e_xy = tf.sqrt(tf.square(y_true[:, 0] - y_pred[:, 0]) +  tf.square(y_true[:, 1] - y_pred[:, 1])) \n",
    "    return tf.reduce_mean(e_xy, axis=-1)\n",
    "\n",
    "def xy_loss_metric_mse(y_true, y_pred):\n",
    "    e_xy = tf.square(y_true[:, 0] - y_pred[:, 0]) +  tf.square(y_true[:, 1] - y_pred[:, 1]) \n",
    "    return tf.sqrt(tf.reduce_mean(e_xy, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4997c634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cont_LSTM(model, site_id, seq_len, train_x, val_x, train_y, val_y, w_f = 10, drop=0.1, learning_rate=0.005, epochs=200, batch=6):\n",
    "    print(f\"Continue-Fitting Model with {train_x.shape}/{val_x.shape} train/validation shapes => {round(100*train_x.shape[0]/(train_x.shape[0]+val_x.shape[0]), 1)}%...\")\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    epoch_iterations = 20\n",
    "    \n",
    "    record_shape = train_x.shape[1:]\n",
    "    features_count = train_x.shape[2]\n",
    "    targets_count = train_y.shape[2]\n",
    "    records_num = train_x.shape[0]\n",
    "        \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False)  # \"adam\"\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True)\n",
    "    \n",
    "    if batch==0:\n",
    "        batch = int(np.log2(records_num/epoch_iterations))\n",
    "    \n",
    "    MODEL_NAME = f\"S{site_id}_f{features_count}_seq{seq_len}_L2+2bi+2x{w_f}_LR{round(learning_rate*1000, 1)}_d{drop}x5_bch{batch}_{int(time.time())}\"\n",
    "    tensorboard = TensorBoard(log_dir=f\"log/Neurals/{MODEL_NAME}\", histogram_freq=1)\n",
    "                \n",
    "    fit = model.fit(train_x, train_y,\n",
    "                validation_data=(val_x,  val_y),\n",
    "                batch_size=2**batch,\n",
    "                epochs=epochs,\n",
    "                verbose=0,\n",
    "                callbacks=[tensorboard, reduce_lr]#, earlystop]\n",
    "               )\n",
    "    return model\n",
    "\n",
    "def LSTM(site_id, seq_len, train_x, val_x, train_y, val_y, w_f = 10, drop=0.1, learning_rate=0.005, epochs=200, batch=6): \n",
    "    print(f\"Fitting Model with {train_x.shape}/{val_x.shape} train/validation shapes => {round(100*train_x.shape[0]/(train_x.shape[0]+val_x.shape[0]), 1)}%...\")\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    epoch_iterations = 20\n",
    "    \n",
    "    record_shape = train_x.shape[1:]\n",
    "    features_count = train_x.shape[2]\n",
    "    targets_count = train_y.shape[2]\n",
    "    records_num = train_x.shape[0]\n",
    "        \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-7, amsgrad=False)  # \"adam\"\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5)\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True)\n",
    "        \n",
    "    if batch==0:\n",
    "        batch = int(np.log2(records_num/epoch_iterations))\n",
    "        \n",
    "    METRICS = [\n",
    "        #tf.keras.metrics.MeanAbsoluteError(name=\"mae\", dtype=None)\n",
    "        xy_loss_metric,\n",
    "        #tf.keras.metrics.RootMeanSquaredError(name=\"rmse\", dtype=None)\n",
    "    ]       \n",
    "       \n",
    "    nodes_max = 2048\n",
    "    nodes_min = 256\n",
    "    nodes_num = max(min(nodes_max, int(features_count*w_f if w_f <= 1 else w_f)), nodes_min)\n",
    "        \n",
    "    model_DNN = keras.Sequential([       \n",
    "        \n",
    "        layers.Dense(nodes_num, activation=\"relu\", input_shape=(None, features_count)),\n",
    "        layers.Dropout(drop),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Dense(nodes_num, activation=\"relu\"),  # 0.5\n",
    "        layers.Dropout(drop),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Bidirectional(layers.LSTM(nodes_num, return_sequences=True), merge_mode=\"concat\"),\n",
    "        layers.Dropout(drop),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Bidirectional(layers.LSTM(nodes_num, return_sequences=True), merge_mode=\"concat\"),\n",
    "        layers.Dropout(drop),\n",
    "        layers.BatchNormalization(),\n",
    "                \n",
    "        layers.Dense(nodes_num, activation=\"relu\"),  # 0.5\n",
    "        layers.Dropout(drop),\n",
    "        layers.BatchNormalization(),\n",
    "                \n",
    "        layers.Dense(nodes_num, activation=\"relu\"),  # 0.25\n",
    "        \n",
    "        layers.Dense(2),\n",
    "    ])\n",
    "    \n",
    "    model_LSTM = keras.Sequential([\n",
    "        \n",
    "        layers.Bidirectional(layers.LSTM(int(nodes_num/10), return_sequences=True), input_shape=(None, features_count), merge_mode=\"concat\"),\n",
    "        layers.Dropout(drop),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Dense(nodes_num, activation=\"relu\"),  # 0.5\n",
    "        layers.Dropout(drop),       \n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Dense(nodes_num, activation=\"relu\"),  # 0.5\n",
    "        layers.Dropout(drop),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        #merge_mode=\"concat\" \"sum\", \"mul\", \"concat\", \"ave\"\n",
    "        layers.Bidirectional(layers.LSTM(int(nodes_num/10), return_sequences=True), merge_mode=\"concat\"),\n",
    "        \n",
    "        layers.Dense(2),\n",
    "    ])\n",
    "    \n",
    "    MODEL_NAME = f\"S{site_id}_f{features_count}_seq{seq_len}_L2+2bi+2x{w_f}_LR{round(learning_rate*1000, 1)}_d{drop}x5_bch{batch}_{int(time.time())}\"\n",
    "    tensorboard = TensorBoard(log_dir=f\"log/Neurals/{MODEL_NAME}\", histogram_freq=1)\n",
    "    \n",
    "    model = model_LSTM\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  #loss=\"mae\",\n",
    "                  loss=\"mse\",\n",
    "                  #loss=xy_loss_metric_mse,\n",
    "                  metrics=METRICS)\n",
    "    \n",
    "    display(model.summary())  \n",
    "    \n",
    "    fit = model.fit(train_x, train_y,\n",
    "                    validation_data=(val_x,  val_y),\n",
    "                    batch_size=2**batch,\n",
    "                    epochs=epochs,\n",
    "                    verbose=0,\n",
    "                    callbacks=[tensorboard,  reduce_lr]#, earlystop]\n",
    "                   )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d99cd8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on #1: Site-5dc8cea7659e181adb076a3f\n",
      "Loading Data...\n",
      "Processing Data of shape (50840, 4938)...\n",
      "Fitting Model with (2477, 20, 4942)/(65, 20, 4942) train/validation shapes => 97.4%...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_22 (Bidirectio (None, None, 296)         6027744   \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, None, 296)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, None, 296)         1184      \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, None, 1482)        440154    \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, None, 1482)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, None, 1482)        5928      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, None, 1482)        2197806   \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, None, 1482)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, None, 1482)        5928      \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, None, 296)         1931104   \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, None, 2)           594       \n",
      "=================================================================\n",
      "Total params: 10,610,442\n",
      "Trainable params: 10,603,922\n",
      "Non-trainable params: 6,520\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0494s vs `on_train_batch_end` time: 4.8074s). Check your callbacks.\n",
      "Loading Data...\n",
      "Processing Data of shape (54490, 4938)...\n",
      "Continue-Fitting Model with (5281, 10, 4942)/(168, 10, 4942) train/validation shapes => 96.9%...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0705s vs `on_train_batch_end` time: 2.9375s). Check your callbacks.\n",
      "Loading Data...\n",
      "Processing Data of shape (56415, 4938)...\n",
      "Continue-Fitting Model with (10939, 5, 4942)/(344, 5, 4942) train/validation shapes => 97.0%...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0736s vs `on_train_batch_end` time: 4.2319s). Check your callbacks.\n",
      "Predictions (train/validation) #1: Site-5dc8cea7659e181adb076a3f\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real_X</th>\n",
       "      <th>real_Y</th>\n",
       "      <th>predict_X</th>\n",
       "      <th>predict_Y</th>\n",
       "      <th>abs_error_X</th>\n",
       "      <th>abs_error_Y</th>\n",
       "      <th>mean_abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1720.000000</td>\n",
       "      <td>1720.000000</td>\n",
       "      <td>1720.000000</td>\n",
       "      <td>1720.000000</td>\n",
       "      <td>1720.000000</td>\n",
       "      <td>1720.000000</td>\n",
       "      <td>1720.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>179.504059</td>\n",
       "      <td>111.069423</td>\n",
       "      <td>174.771759</td>\n",
       "      <td>113.354057</td>\n",
       "      <td>8.734953</td>\n",
       "      <td>9.331972</td>\n",
       "      <td>6.845131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>44.011383</td>\n",
       "      <td>35.956020</td>\n",
       "      <td>41.098591</td>\n",
       "      <td>26.654526</td>\n",
       "      <td>10.344872</td>\n",
       "      <td>10.445747</td>\n",
       "      <td>4.059943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>90.394128</td>\n",
       "      <td>23.687620</td>\n",
       "      <td>38.768436</td>\n",
       "      <td>45.095539</td>\n",
       "      <td>0.011586</td>\n",
       "      <td>0.007818</td>\n",
       "      <td>0.002374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>131.383728</td>\n",
       "      <td>100.979022</td>\n",
       "      <td>132.651325</td>\n",
       "      <td>99.808172</td>\n",
       "      <td>2.040530</td>\n",
       "      <td>2.367601</td>\n",
       "      <td>3.261399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>192.952146</td>\n",
       "      <td>117.587912</td>\n",
       "      <td>185.543495</td>\n",
       "      <td>117.452873</td>\n",
       "      <td>4.783585</td>\n",
       "      <td>5.006981</td>\n",
       "      <td>6.877046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>220.509350</td>\n",
       "      <td>137.259768</td>\n",
       "      <td>208.530800</td>\n",
       "      <td>132.795929</td>\n",
       "      <td>10.951865</td>\n",
       "      <td>11.328810</td>\n",
       "      <td>10.383263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>249.793490</td>\n",
       "      <td>170.257281</td>\n",
       "      <td>243.848373</td>\n",
       "      <td>157.641769</td>\n",
       "      <td>73.426294</td>\n",
       "      <td>40.637811</td>\n",
       "      <td>13.736671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            real_X       real_Y    predict_X    predict_Y  abs_error_X  \\\n",
       "count  1720.000000  1720.000000  1720.000000  1720.000000  1720.000000   \n",
       "mean    179.504059   111.069423   174.771759   113.354057     8.734953   \n",
       "std      44.011383    35.956020    41.098591    26.654526    10.344872   \n",
       "min      90.394128    23.687620    38.768436    45.095539     0.011586   \n",
       "25%     131.383728   100.979022   132.651325    99.808172     2.040530   \n",
       "50%     192.952146   117.587912   185.543495   117.452873     4.783585   \n",
       "75%     220.509350   137.259768   208.530800   132.795929    10.951865   \n",
       "max     249.793490   170.257281   243.848373   157.641769    73.426294   \n",
       "\n",
       "       abs_error_Y  mean_abs_error  \n",
       "count  1720.000000     1720.000000  \n",
       "mean      9.331972        6.845131  \n",
       "std      10.445747        4.059943  \n",
       "min       0.007818        0.002374  \n",
       "25%       2.367601        3.261399  \n",
       "50%       5.006981        6.877046  \n",
       "75%      11.328810       10.383263  \n",
       "max      40.637811       13.736671  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119625bbf81c48a09131a18d5e830a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Zhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\Zhi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./saved_models/models24_v11_LSTM-DNN_d02_RoP_s20-10_1619914768/5dc8cea7659e181adb076a3f\\assets\n",
      "Finished fitting\n",
      "Global Train/Validation MAE: 11.874172000991674/13.736671151995067\n"
     ]
    }
   ],
   "source": [
    "seq_len = 10\n",
    "\n",
    "models24_xy = {}\n",
    "train_val_comparison_xy = []\n",
    "num_s = 0\n",
    "\n",
    "site_ids_shapes = {\"5da958dd46f8266d0737457b\": (47796, 7003), \"5d2709c303f801723c3299ee\": (33368, 11667), \"5dbc1d84c1eb61796cf7c010\": (61727, 9043), \"5d27075f03f801723c2e360f\": (73141, 14063), \n",
    "                   \"5da138b74db8ce0c98bd4774\": (56668, 7075), \"5dc8cea7659e181adb076a3f\": (57849, 9733), \"5d27096c03f801723c31e5e0\": (19337, 9933), \"5d2709bb03f801723c32852c\": (44009, 4909), \n",
    "                   \"5a0546857ecc773753327266\": (26532, 6799), \"5c3c44b80379370013e0fd2b\": (29359, 6131), \"5d27097f03f801723c320d97\": (35121, 4985), \"5da1382d4db8ce0c98bbe92e\": (28975, 5729),\n",
    "                   \"5d2709b303f801723c327472\": (32449, 3831), \"5d2709d403f801723c32bd39\": (23545, 4283), \"5da138764db8ce0c98bcaa46\": (27771, 3781), \"5da1383b4db8ce0c98bc11ab\": (34396, 3055),\n",
    "                   \"5d2709e003f801723c32d896\": (29752, 2623), \"5da138754db8ce0c98bca82f\": (13795, 3259), \"5da1389e4db8ce0c98bd0547\": (17795, 2047), \"5da138314db8ce0c98bbf3a0\": (13122, 2429),\n",
    "                   \"5d2709a003f801723c3251bf\": (9345, 2509), \"5d27099f03f801723c32511d\": (9700, 1855), \"5da138364db8ce0c98bc00f1\": (5555, 1649), \"5da138274db8ce0c98bbd3d2\": (6338, 985)}\n",
    "\n",
    "site_ids = [\"5dc8cea7659e181adb076a3f\"]\n",
    "\n",
    "for site_id in site_ids:#site_ids_shapes.keys():#site_ids_shapes.keys(): #site_ids:#site_ids_num_bddsid.keys(): #site_ids:\n",
    "    models24_xy = {}\n",
    "    \n",
    "    num_s += 1\n",
    "    print(f\"Working on #{num_s}: Site-{site_id}\")\n",
    "    #========================================================================================\n",
    "    \n",
    "    for i_seq, seq_len in enumerate([20, 10, 5]):\n",
    "        \n",
    "        if i_seq == 0:  # initialize model/start fit\n",
    "            f_list, scaler_xy, encoder_xy, train_x, val_x, train_y, val_y = preprocess_data(load_data(site_id, 10000, seq_len=seq_len),\n",
    "                                                                                            seq_len=seq_len, train_fraction=0.97, random_state=123)\n",
    "            model_xy = LSTM(site_id=site_id,\n",
    "                            seq_len=seq_len,\n",
    "                            train_x=train_x, val_x=val_x,\n",
    "                            train_y=train_y, val_y=val_y,\n",
    "                            w_f=0.3, drop=0.1, learning_rate=round(100/site_ids_shapes[site_id][0], 4), epochs=50, batch=0)\n",
    "                            #w_f=0.3, drop=0.1, learning_rate=0.0056, epochs=100, batch=4)\n",
    "        else: #continue fitting\n",
    "            f_list, scaler_xy, encoder_xy, train_x, val_x, train_y, val_y = preprocess_data(load_data(site_id, 10000, seq_len=seq_len),\n",
    "                                                                                        seq_len=seq_len, train_fraction=0.97, random_state=123)\n",
    "            model_xy = cont_LSTM(model_xy,\n",
    "                                 site_id=site_id,\n",
    "                                 seq_len=seq_len,\n",
    "                                 train_x=train_x, val_x=val_x,\n",
    "                                 train_y=train_y, val_y=val_y,\n",
    "                                 #w_f=0.3, drop=0.1, learning_rate=round(100/site_ids_shapes[site_id][0], 4), epochs=10+seq_len*3, batch=0)\n",
    "                                 w_f=0.3, drop=0.1, learning_rate=round(100/site_ids_shapes[site_id][0], 4), epochs=50, batch=0)\n",
    "    \n",
    "    models24_xy[site_id] = [f_list, scaler_xy, encoder_xy, model_xy]\n",
    "    #========================================================================================\n",
    "    # sanity check section\n",
    "    predict_columns = [\"predict_X\", \"predict_Y\"]\n",
    "    real_columns = [\"real_X\", \"real_Y\"]\n",
    "    col_dic = {0: \"_X\", 1: \"_Y\"}\n",
    "\n",
    "    predictions_train_xy = pd.DataFrame(model_xy.predict(train_x).reshape(train_x.shape[0]*train_x.shape[1], 2), \n",
    "                                        columns=predict_columns)\n",
    "    comparison_train_xy = pd.concat([pd.DataFrame(train_y.reshape(train_x.shape[0]*train_x.shape[1], 2), \n",
    "                                                  columns=real_columns),\n",
    "                                     predictions_train_xy], axis=1)\n",
    "    predictions_val_xy = pd.DataFrame(model_xy.predict(val_x).reshape(val_x.shape[0]*val_x.shape[1], 2), \n",
    "                                      columns=predict_columns)\n",
    "    comparison_val_xy = pd.concat([pd.DataFrame(val_y.reshape(val_x.shape[0]*val_x.shape[1], 2), \n",
    "                                                columns=real_columns),\n",
    "                                   predictions_val_xy], axis=1)\n",
    "\n",
    "    for col_i, col in enumerate(predict_columns):\n",
    "        for train_valid in [comparison_train_xy, comparison_val_xy]:\n",
    "            train_valid[\"abs_error\"+col_dic[col_i]] = np.abs(train_valid[real_columns[col_i]] - train_valid[col])\n",
    "\n",
    "    for train_valid in [comparison_train_xy, comparison_val_xy]:\n",
    "        mean_error = np.sqrt(np.power(train_valid[\"abs_error_X\"], 2) + np.power(train_valid[\"abs_error_Y\"], 2))\n",
    "        train_valid[\"mean_abs_error\"] = mean_error.cumsum()/train_valid.shape[0]\n",
    "\n",
    "    print(f\"Predictions (train/validation) #{num_s}: Site-{site_id}\")\n",
    "    #display(comparison_train_xy[[\"mean_abs_error\"]].tail(1))\n",
    "    #display(comparison_val_xy[[\"mean_abs_error\"]].tail(1))\n",
    "\n",
    "    train_val_comparison_xy.append([comparison_train_xy, comparison_val_xy])\n",
    "    #========================================================================================\n",
    "    display(comparison_val_xy.describe())\n",
    "    #display(comparison_val_xy[(comparison_val_xy.abs_error_X > 20) | (comparison_val_xy.abs_error_Y > 20)])\n",
    "\n",
    "    #comparison_val_xy[[\"abs_error_X\", \"abs_error_Y\"]].plot.hist(cumulative=True, bins=100, logy=True, alpha=0.3, title=site_id)\n",
    "\n",
    "    model_name = save_models(models24_xy, f\"models24_v11_LSTM-DNN_d02_RoP_s20-10\")\n",
    "    keras.backend.clear_session()\n",
    "    #break\n",
    "print(\"Finished fitting\")\n",
    "calculate_global_metrics(train_val_comparison_xy)\n",
    "\n",
    "#print(\"Finished Models Saving\")\n",
    "#%tensorboard --logdir log/Neurals  # commandline to start tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc24d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab6f17e7",
   "metadata": {},
   "source": [
    "Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba44c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_test_data = pickle.load(open(\"./data_out/full24/test-10k_mix-counts_t550.pkl\", \"rb\"))   # counts_t550 vs counts\n",
    "floor100_siteid_traceid = pickle.load(open(\"./data_out/floor100_siteid_traceid.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb6162b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3488f49f692a4f228ba9c1c31051a674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Predictions #1: Site-5da138b74db8ce0c98bd4774 with 29 traces\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8a9449767a4f8c9f3ea2e92672fedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbacf68695bb4dabaee1018ae84332a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"models24_v11_LSTM-DNN_d02_s20-10_1619909982\"\n",
    "model_path = \"./saved_models/\" + model_name\n",
    "\n",
    "#models24_xy = {}\n",
    "seq_len = 10\n",
    "seq_mode = \"moving\" # [\"full\", \"moving\", \"stacked\"]\n",
    "\n",
    "#site_ids = parsed_test_data.keys()\n",
    "#site_ids = [\"5da958dd46f8266d0737457b\"]#, \"5d2709c303f801723c3299ee\", \"5dbc1d84c1eb61796cf7c010\", \"5d27075f03f801723c2e360f\", \"5da138b74db8ce0c98bd4774\"]#parsed_test_data.keys()#[\"5da1382d4db8ce0c98bbe92e\"]\n",
    "\n",
    "predicted_data = {}\n",
    "n_s = 0\n",
    "for site_id in tqdm(site_ids):#site_ids):  # over sites\n",
    "    n_s += 1\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    print(f\"Processing Predictions #{n_s}: Site-{site_id} with {len(parsed_test_data[site_id])} traces\")\n",
    "    ############# GET MODELS ####################\n",
    "    if len(models24_xy) == 0:\n",
    "        features_xy = pickle.load(open(model_path + f\"/{site_id}/features_list.pkl\", \"rb\"))\n",
    "        scaler_xy = (pickle.load(open(model_path + f\"/{site_id}/scaler.pkl\", \"rb\")))\n",
    "        encoder_xy = (pickle.load(open(model_path + f\"/{site_id}/f_binarizer.pkl\", \"rb\")))\n",
    "        model_xy = (tf.keras.models.load_model(model_path + f\"/{site_id}\", custom_objects={\"xy_loss_metric_mse\": xy_loss_metric_mse, \"xy_loss_metric\": xy_loss_metric}))\n",
    "    else:\n",
    "        features_xy = models24_xy[site_id][0]\n",
    "        scaler_xy =  models24_xy[site_id][1]\n",
    "        encoder_xy = models24_xy[site_id][2]\n",
    "        model_xy = models24_xy[site_id][3]\n",
    "    ##############################################\n",
    "    predicted_data[site_id] = {}\n",
    "    for trace_id in tqdm(parsed_test_data[site_id]):  # over traces\n",
    "        \n",
    "        trace_record = parsed_test_data[site_id][trace_id].copy(deep=True)\n",
    "        ######### GENERAL FEATURE MANIPULATION ################    \n",
    "        _time = trace_record.pop(\"time\").to_numpy()\n",
    "        \n",
    "        x_m = trace_record.pop(\"m\").to_numpy().reshape(-1,1)\n",
    "        x_r = trace_record.pop(\"r\").to_numpy().reshape(-1,1)\n",
    "        x_rx = trace_record.pop(\"rx\").to_numpy().reshape(-1,1)\n",
    "        x_ry = trace_record.pop(\"ry\").to_numpy().reshape(-1,1)\n",
    "        x_rx_cum = trace_record.pop(\"rx_cum\").to_numpy().reshape(-1,1)\n",
    "        x_ry_cum = trace_record.pop(\"ry_cum\").to_numpy().reshape(-1,1)\n",
    "        \n",
    "        trace_record_xy = trace_record[features_xy].copy(deep=True)\n",
    "        trace_record_xy = trace_record_xy.to_numpy()\n",
    "        \n",
    "        ##########################################################\n",
    "        #  PREDICT XY (with F100 feature)\n",
    "        ##########################################################\n",
    "        # combine features\n",
    "        _pred_f = floor100_siteid_traceid[site_id][trace_id]\n",
    "        trace_record_xy_scaled = scaler_xy.transform(np.concatenate((trace_record_xy, x_m, x_r, x_rx, x_ry, x_rx_cum, x_ry_cum, encoder_xy.transform(np.full_like(_time, _pred_f))), axis=1))  #x_m, x_r, x_rx, x_ry, x_rx_cum, x_ry_cum, \n",
    "        \n",
    "        folds_x, folds_y = [], []\n",
    "        # make predictions on new sequenced records\n",
    "        for seq_len in [20]:#[2, 5, 7, 10, 15, 20]:\n",
    "        #for seq_len in range(5,21)[2, 5, 7, 10, 15, 20]:\n",
    "        #for seq_len in range(5,21):\n",
    "            trace_record_xy_scaled_seq = make_seq(trace_record_xy_scaled, seq_mode, seq_len)\n",
    "            predictions_xy = model_xy.predict(trace_record_xy_scaled_seq)\n",
    "\n",
    "            folds_x.append(make_seq_inv(predictions_xy[:, :, 0], seq_mode, seq_len))\n",
    "            folds_y.append(make_seq_inv(predictions_xy[:, :, 1], seq_mode, seq_len))\n",
    "        #print(predictions_xy.shape)\n",
    "\n",
    "        # remove outliers (and combine different folds/seq_len)\n",
    "        if len(folds_x) > 1:\n",
    "            predictions_xy_x = np.median(median_filter(np.concatenate(folds_x, axis=1), (3,3)), axis=1).reshape(-1,1)\n",
    "            predictions_xy_y = np.median(median_filter(np.concatenate(folds_y, axis=1), (3,3)), axis=1).reshape(-1,1)\n",
    "        else:                           \n",
    "            predictions_xy_x = median_filter(np.concatenate(folds_x, axis=1), (3,3))\n",
    "            predictions_xy_y = median_filter(np.concatenate(folds_y, axis=1), (3,3))\n",
    "                                                              \n",
    "        # combine into final DataFrame\n",
    "        predictions_xyf = pd.DataFrame(np.concatenate((predictions_xy_x, predictions_xy_y), axis=1), columns=[\"x\", \"y\"])\n",
    "        predictions_xyf[\"floor\"] = _pred_f\n",
    "        predictions_xyf[\"time\"] = _time[:predictions_xy_x.shape[0]]\n",
    "\n",
    "        predicted_data[site_id][trace_id] = predictions_xyf\n",
    "        \n",
    "        #break  # only first trace\n",
    "    \n",
    "    #break  # only first site_id\n",
    "plot_predictions_multi(model_name, predicted_data, sufix=f\"coarse_s20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51f3fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./submit/fit_data/{model_name}_predicted_s20.pkl\", \"wb\") as f:\n",
    "    pickle.dump(predicted_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44f5db5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5a183e41954950a72c381a27c6b18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_data = pickle.load(open(f\"./submit/fit_data/{model_name}_predicted_s20.pkl\", \"rb\"))\n",
    "plot_predictions_multi(model_name, predicted_data, sufix=f\"coarse_s20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6746e9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364cf88f5b0b41a78a2fa914a61b08b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_data = pickle.load(open(f\"./submit/fit_data/{model_name}_predicted_s20.pkl\", \"rb\"))\n",
    "make_submission(model_name, predicted_data, sufix=f\"coarse_s20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe94189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
